{"config":{"lang":["en"],"separator":"[\\s\\-\\.\\_]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to 50.043","text":"<p>For course handout click here.</p>"},{"location":"notes/","title":"notes","text":""},{"location":"notes/l10_hdfs/","title":"L10 hdfs","text":"<p>-- header-includes:    - \\usepackage{tikz}   - \\usepackage{pgfplots} --</p>"},{"location":"notes/l10_hdfs/#50043-hadoop-distributed-file-system","title":"50.043 Hadoop Distributed File System","text":""},{"location":"notes/l10_hdfs/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this unit, you should be able to</p> <ol> <li>Explain the file system model of HDFS</li> <li>Explain the architecture of HDFS</li> <li>Explain the replication placement strategy</li> <li>Explain the operation of HDFS client</li> <li>Explain the use of erasure coding</li> </ol>"},{"location":"notes/l10_hdfs/#hadoop","title":"Hadoop","text":"<p>Hadoop is one of the widely-used frameworks for big data. It was started in 2005 by a group of Yahoo! scientists. The initial objective was to provide a scalable backend for the nutch crawler. </p> <p>Over time Hadoop has evolved into a fully fledged distributed data storage and processing framework. It has a few major components</p> <ul> <li>MapReduce - the data processing layer</li> <li>Hadoop Distributed File System - the data storage layer</li> <li>YARN - the resource management layer</li> </ul> <p>In this unit, our focus is on HDFS.</p>"},{"location":"notes/l10_hdfs/#hadoop-distributed-file-system","title":"Hadoop Distributed File System","text":"<p>Hadoop Distrubuted File System was developed based on the white paper of a closed source project the google file system. </p> <ul> <li>The Google file system, Sanjay Ghemawat, Howard Bradley Gobioff  and Shun-Tak Leung, ACM SIGOPS 2005<ul> <li>https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf</li> </ul> </li> </ul>"},{"location":"notes/l10_hdfs/#google-file-system","title":"Google File System","text":"<p>Google File System was created in the period of time where databases were expensive, but hard disks were cheap. They need to store a lot of non relational data (crawled data), i.e. data are in huge volume. These data needed to be processed by batches in huge sizes in a relative short amount of time, i.e. updating the search engine indices. </p> <p>The only alternative during that time was network file system (NFS), which deemed to be limited by its original use cases. </p> <ul> <li>a file must reside on one and only one machine.</li> <li>there is no reliability guarantee, data loss could be hard to recover</li> <li>it is not scalable, a computation over a huge batch of data cannot be distributed to multiple processors.</li> <li>there is no builtin replication mechanism, which leads to a high fail rate.</li> <li>file accessed over the network would incur huge amount of network I/O</li> </ul> <p>The google team then identified a list of features that required by the search engine indexing use case, and ordered them according to the priority</p> <ol> <li>support many clients</li> <li>support many disks</li> <li>support file in size of petabytes</li> <li>offer high fault tolerance</li> <li>allow file read/file write like normal file system</li> </ol> <p>It turned out no system can achieve all the above requirements. The team decided to drop the last one. </p> <p>By design, Google file system is a distributed file system that supports only sequential read and append operation. </p> <p>HDFS inherited the design of Google file system.</p>"},{"location":"notes/l10_hdfs/#hdfs-file-system-model","title":"HDFS File system model","text":"<p>Like many conventional file systems, HDFS follows a hierarchical name space, i.e. data are organized by tree like structure, where each branch is a folder (sub folder) and the leaf nodes are the file.</p> <p><pre><code>|- A\n|  |- B\n|     |- mydoc.txt\n|  |- B'\n|  |- B''\n|\n|- A'\n|- A''\n</code></pre> For instance in the file system above, there are three folders under the root directory, namely <code>A</code>, <code>A'</code> and <code>A''</code>.  Under <code>A</code>, there are three sub folders, <code>B</code>, <code>B'</code> and <code>B''</code>. Under <code>B</code>, we find a file <code>mydoc.txt</code></p> <p></p> <p>In the above diagram, we find the illustration of the data storage of the file <code>mydoc.txt</code>.  The first data block <code>Block 0</code> stores the first level of folders. Folder <code>A</code> is a reference pointing at the data block <code>Block 1</code>, which stores the list of subfolders within. Sub-folder <code>B</code> is a reference pointing at the data block <code>Block 2</code>. <code>mydoc.txt</code> is the first entry in <code>Block 2</code>, which is an array of pointers pointing at the actual file data. The data blocks <code>Block 0</code>, <code>Block 1</code> and <code>Block 2</code> are known as the meta data. File data block (in grey color) are the real data. </p> <p>There are several merits of such a file system model. </p> <ol> <li>It is a simple abstraction.</li> <li>It supports very large files, (could be larger than a single physical disk)</li> <li>Data can be distributed to multiple disks in multiple hosts.</li> </ol>"},{"location":"notes/l10_hdfs/#choice-of-block-size","title":"Choice of Block Size","text":"<p>Building upon the above model, the next design decision to be made is the size of the block. In normal file system, (i.e .OSes), the size of a block is around 4KB. For relational database system, the block size can range from 4 to 32KB. For HDFS the default block size is 128MB, which can be changed via system configuration.</p> <p>The advantage of a larger block size is to fetch more data in a single file IO operation, in the expense of larger unfilled disk space (due to data fragmentation). For HDFS, since files are written via append, data fragmentation is minimized. </p>"},{"location":"notes/l10_hdfs/#hdfs-architecture","title":"HDFS Architecture","text":"<p>The HDFS architecture follows a master-worker pattern. </p> <p></p> <p>The master node, which is also known as the name node, keeps track of a set of data nodes. </p> <p>The meta data of the files is stored in the name node. A secondary name node is provisioned for the backup purposes. </p> <p>The real data are stored and replicated among the data nodes. By default HDFS recommends there should be at least 3 copies of the same data block, i.e. replication factor = 3. Replication factors 5 or 7 are also recommended. </p> <p>A client, i.e. an application that tries to access a file in the HDFS, has to make the requests to the name node and waits for the responses. </p>"},{"location":"notes/l10_hdfs/#replication-and-replica-placement-policy","title":"Replication and Replica placement policy","text":"<p>Replication is to increase data survivability in case of hardware failure. But how and where shall we store / distribute the replicas of a data block? To maximize the survivability, it might be beneficial to take into account the actual location of the data nodes. </p> <p>In data centers, servers are physically mounted and connected in racks. Multiple servers can be placed in a rack which shares a power source and network switch (with redudancy). In case of power or network failure, the entire rack is affected.  Hence, it would be wise not to store replicas of a data block in servers located in the same rack. On the other hand intra rack data transfer will be more cost effective. </p> <p>In a HDFS cluster, when the rack information is present, the replicas of a data block are distributed as follows </p> <ul> <li>Max 1 replica per datanode</li> <li>Max 2 replicas per rack</li> <li>Number of racks for replication should be less than the replication factor</li> </ul> <p>For example, we consider the following setup with 3 racks, each rack contains 3 datanodes. 3 data blocks need to be stored and replicated with RF = 3.</p> <p></p> <p>The first copy of data block <code>A</code> is stored in data node <code>1</code>.  The 2<sup>nd</sup> copy must not be stored in any other data node in rack <code>1</code>. In this case the system randomly picks one rack out of <code>2</code> and <code>3</code>. It places the 2<sup>nd</sup> copy in data node <code>5</code> of rack <code>2</code> in this case. The 3<sup>rd</sup> replica can be placed in any rack. In this case put it in data node <code>6</code>.</p>"},{"location":"notes/l10_hdfs/#hdfs-client-operation","title":"HDFS client operation","text":""},{"location":"notes/l10_hdfs/#read","title":"Read","text":"<p>When a client attempts to read a file in the HDFS.</p> <ol> <li>The client requests for the data block locations of the file from the name node.</li> <li>The name node returns the list of data block locations to the client.</li> <li>Based on the data block locations, the client requests for the data blocks from the data node.</li> <li>The data nodes reply the client with the data being requested. </li> </ol> <pre><code>graph LR;\n  client --1. file path--&gt; NN[\"name node\"]\n  NN[\"name node\"] --2. block locations --&gt; client\n  client --3. block location --&gt; DN1\n  client --3. block location --&gt; DN2\n  client --3. block location --&gt; DN3\n  DN1[\"data node 1\"] --4. data --&gt; client\n  DN2[\"data node 2\"] --4. data --&gt; client\n  DN3[\"data node 3\"] --4. data --&gt; client</code></pre>"},{"location":"notes/l10_hdfs/#write","title":"Write","text":"<p>When a client attempts to write/append to a file in the HDFS.</p> <ol> <li>The client creates the meta data entry in the name node.</li> <li>The name node allocates and returns the first block locations to the client.</li> <li>Based on the first block locations the client initializes the data writing pipeline.</li> <li>The data will be sent in the daisy chain manner from the client to the data nodes.</li> <li>When the data is successfully transmitted, an acknowledgement will be sent to data source; When some data block (replica) write operation fails, the information is recorded by the name node and the operation will be re-executed on some data node.</li> <li>When the last data block is written, the client initiates a close request to the name node. </li> <li>The name node checks with the data nodes to ensure the minimum replica requirement is met. </li> <li>The data nodes reports the minimum replica status back to the name node.</li> <li>The name node sends the acknowledgement to the client notifying that the file write operation was successful.</li> </ol> <pre><code>graph LR \n  client --1. create--&gt; NN[\"name node\"]\n  NN[\"name node\"] --2. first block locations--&gt; client\n  client --3. organize pipeline; 4. send data--&gt; DN1[\"data node 1\"]\n  DN1 --5. acknowledge--&gt; client\n  DN1[\"data node 1\"] --3. organize pipeline; 4. send data--&gt; DN2[\"data node 2\"]\n  DN2 --5. acknowledge--&gt; DN1\n  DN2[\"data node 2\"] --3. organize pipeline; 4. send data--&gt; DN3[\"data node 3\"]\n  DN3 --5. acknowledge --&gt;DN2\n  client --6. close --&gt; NN[\"name node\"]\n  NN --7. check minimum replica --&gt; DN1\n  NN --7. check minimum replica --&gt; DN2\n  NN --7. check minimum replica --&gt; DN3\n  DN1 --8. report --&gt; NN\n  DN2 --8. report --&gt; NN\n  DN3 --8. report --&gt; NN</code></pre> <p>In case of the minimum replica requirement is not satisfied, the name node will arrange re-distribution the replica. </p>"},{"location":"notes/l10_hdfs/#alternative-to-data-replication","title":"Alternative To Data Replication","text":"<p>Replicating data and redistributing them is a simple and effective way to improve fault tolerance. However, one draw-back of data replication is the data overhead. A system with replication factor <code>n</code> will lead to <code>(n-1) * 100%</code> storage overhead and <code>(1/n)</code> storage efficiency. </p> <p>For example, when <code>n = 3</code>, we have <code>200%</code> storage overhead and <code>0.3333</code> storage efficiency.</p> <p>One of the popular alternatives to data replication is erasure coding. The main idea of erasure coding is not to replicate data, instead it generates a parity for each segment of actual data, with which the actual data could be restored in case of data loss. The idea behind was inspired by the Exclusive OR (XOR) operation for bits. </p>"},{"location":"notes/l10_hdfs/#properties-of-xor","title":"Properties of XOR","text":"<p>Recall that XOR operation \\(\\bigoplus\\) on bits</p> IN IN XOR 0 0 0 1 0 1 0 1 1 1 1 0 <p>having some nice properties</p> \\[ X \\bigoplus Y = Y \\bigoplus X \\] \\[ (X \\bigoplus Y) \\bigoplus Z = X \\bigoplus (Y \\bigoplus Z) \\] \\[ X \\bigoplus Y = Z \\Rightarrow X \\bigoplus Z = Y \\] <p>We can use the result of XOR to recover if one of the inputs is lost.</p> <p>We are trying to transfer this idea to increase the recoverability of the HDFS data. Let one of the input to be the actual data, the other input is a special key, the output is the data with the parity.</p> <p>One issue remained is that we are dealing with more than bit of actual data, and we want to keep the special key and the data parity relatively small.</p>"},{"location":"notes/l10_hdfs/#reed-solomon-algorithm","title":"Reed-solomon Algorithm","text":"<p>Addressing the issue leads us to the Reed-solomon algorithm.</p> <p>The algorithm requires a Generator Matrix \\(G^T\\) (the special key), which has the following requirement. </p> <ul> <li>It is \\(n \\times k\\) matrix</li> <li>all \\(k \\times k\\) sub matrices in \\(G^T\\) are non-singular (the inverse matrices exist).</li> </ul> <p>The actual data (from HDFS) are arranged into a \\(k \\times L\\) matrix. What \\(L\\) is not important for demonstration purpose, we assume may let \\(L = 1\\) for simplicity.</p> <p></p> <p>In the above diagram we multiply the generator matrix with the actual data, which produces a code word, consists of the data and the parity. </p> <p>Let's instantiate the diagram with concrete numbers. </p> \\[ \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\  0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 1  \\end{bmatrix}  \\times \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}  =  \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{bmatrix}  \\] <p>Thanks to the properties of the left operand \\(G^T\\), the first four rows of the code word is identical to the data. </p> <p>In the HDFS we will store the \\(G^T\\) in some safe place, i.e. name node with backup and the codeword is distributed among the data nodes </p> <p>Now let's say we lose the 2<sup>nd</sup> and 4<sup>th</sup> rows in the codeword and try to recover the data. </p> \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 2 \\end{bmatrix}  \\] <p>We remove the correspondent rows from the \\(G^T\\), </p> \\[ G^{T^{-1}}_{\\neg{(1,3)}} =  \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\  0 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 1  \\end{bmatrix}  \\] <p>Based on the matrix multiplication the following equation holds</p> \\[ \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\  0 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 1  \\end{bmatrix}  \\times \\begin{bmatrix} ? \\\\ ? \\\\ ? \\\\ ? \\end{bmatrix}  =  \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 2 \\end{bmatrix}  \\] <p>where the question marks are the actual to be reovered. </p> <p>Thanks to the fact that all \\(k \\times k\\) sub matrices in \\(G^T\\) are non-singular, the inverse of \\(G^{T^{-1}}_{\\neg{(1,3)}}\\) exists </p> \\[ \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ -1 &amp; -1 &amp; 0 &amp; 1  \\end{bmatrix} \\times \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\  0 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 1  \\end{bmatrix}  \\times \\begin{bmatrix} ? \\\\ ? \\\\ ? \\\\ ? \\end{bmatrix}  =  \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ -1 &amp; -1 &amp; 0 &amp; 1  \\end{bmatrix} \\times \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 2 \\end{bmatrix}  \\] <p>We cancel \\(G^{T^{-1}}_{\\neg{(1,3)}} \\times G^T_{\\neg{(1,3)}}\\) from the LHS $$ \\begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \\end{bmatrix}  =  \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; -1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ -1 &amp; -1 &amp; 0 &amp; 1  \\end{bmatrix} \\times \\begin{bmatrix} 1 \\ 1 \\ 1 \\ 2 \\end{bmatrix}  $$</p> <p>We recover the lost data.</p>"},{"location":"notes/l10_hdfs/#erasure-coding-storage-overhead-and-data-efficiency","title":"Erasure Coding storage overhead and data efficiency","text":"<p>Erase coding is more economical compared to data replication. \\(G^T\\) is fixed for all data, hence its overhead is neglectable.  We write \\(RS(k,m)\\) to denote that \\(G^T\\) is a matrix of \\((k+m) \\times k\\) and \\(k\\) is the number of rows of the actual data matrix and \\(m\\) is the number of parity rows. We have \\(m/k\\) storage overhead and \\(k / (k + m)\\) storage efficiency.</p> <p>One crucial observation is that we can always recover the codeword with \\((k +m) \\times 1\\) when the number of lost cells is less than or equal to \\(m\\).</p>"},{"location":"notes/l10_hdfs/#question","title":"Question","text":"<p>What happen when \\(L &gt; 1\\)? Hint: you may think about how matrix multiplication works.</p>"},{"location":"notes/l10_mapreduce/","title":"50.043 Map Reduce","text":""},{"location":"notes/l10_mapreduce/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this lesson, you are able to</p> <ul> <li>Differentiate Parallelism and Concurrency</li> <li>Differentiate Data Parallelism and Task Parallelism</li> <li>Explain map and reduce</li> <li>Explain Hadoop MapReduce</li> </ul>"},{"location":"notes/l10_mapreduce/#recall","title":"Recall","text":"<p>HDFS addresses the big data issues by </p> <ol> <li>distributing the data into multiple data nodes.</li> <li>storing meta data on the name node.</li> <li>allowing only sequential read and append.</li> </ol> <p>Based on the design decision made during the design of Google File system, data query via join and B-tree based indexing was not the top-most wanted use case.</p> <p>On the other hand, most of the data processing is to scan through the needed data files and perform transformation or aggregation (page rank algorithm).</p> <p>To shorten the processing time, we could leverage on the processors found in the data nodes. This leads us to parallel computing.</p>"},{"location":"notes/l10_mapreduce/#parallel-computing","title":"Parallel Computing","text":"<p>A parallel program is one that uses a multiplicity of computational hardware (e.g., several processor cores or several server nodes) to perform a computation more quickly. The aim is to arrive at the answer earlier, by delegating different parts of the computation to different processors that execute at the same time.</p> <p>We are often confused parallelism with concurrency. By contrast, concurrency is a program-structuring technique in which there are multiple threads of control. Conceptually, the threads of control execute \"at the same time\"; that is, the user sees their effects interleaved. Whether they actually execute at the same time or not is an implementation detail; a concurrent program can execute on a single processor through interleaved execution or  on multiple physical processors.</p> <p>We summarize the difference between parallelism and and concurrency in the following table.</p> Parallelism Concurrency Area of Focus Efficiency Structural and Modularity Number of Goals One One or more Program Semantic Deterministic Non-deterministic Single processor Exec Sequential Interleaving <p>We focus on the key differences. Though concurrency also achieve certain level of speed up, its focus is in handling multiple tasks with a fixed set of resources through scheduling. Parallelism focuses on getting one thing done and fast. The result of parallel computing must be deterministic whilst concurrency does not necessarily entail determinism.</p> <p>The following are some examples of parallelism </p> <ul> <li>A sodoku solver uses multiple CPU cores</li> <li>A parallelized database query that retrieves and aggregates records from a cluster of replica of database.</li> <li>A K-means analyses running over a Hadoop cluster</li> </ul> <p>and concurrency</p> <ul> <li>A web application that handles multiple clients HTTP requests  and interacting with the databases.</li> <li>A User Interface of the a mobile phone handles user's touch screen input    and exchanging data via the 4G network</li> </ul>"},{"location":"notes/l10_mapreduce/#means-of-parallelism","title":"Means of Parallelism","text":"<p>Parallelism can be achieved via special hardware or/and via software </p>"},{"location":"notes/l10_mapreduce/#hardware-parallelism","title":"Hardware Parallelism","text":""},{"location":"notes/l10_mapreduce/#single-processor","title":"Single Processor","text":"<p>Parallelism can be actualized via a single processor when the given instructions use less than provided bits, e.g. we can parallelize the execution of some 32 bit operations given that the processor operates on 64 bit addresses.</p> <p>Alterantively, the parallelism can be achieved by instruction pipelining, assume different sub components of the processor handles different types of process operators. For instance, </p> <p><pre><code>1: add r1 r2 r3 // r1 = r2 + r3\n2: mul r2 2     // r2 *= 2\n3: add r2 r4 r5 // r2 = r4 + r5\n</code></pre> assuming <code>add</code> and <code>mul</code> instructions are executed by different components of the same processor, after the instruction <code>1</code> is executed, instructions <code>2</code> and <code>3</code> can be executed in parallel.</p>"},{"location":"notes/l10_mapreduce/#gpu","title":"GPU","text":"<p>GPU can be leveraged to perform matrix operations in parallel by making use of the multiple cores builtin. The acceleration of computation often requires firwmware level and driver level support, e.g. Nvidia CUDA and AMD ROCM.</p>"},{"location":"notes/l10_mapreduce/#multiple-processors-with-shared-memory","title":"Multiple processors with shared memory","text":"<p>Modern personal computers and mobile devices are equipped with multi-core processors. These processors share the same pool of physical memory. Parallelism can be achieved via data parallelism, task parallelism, multi-threading etc.</p>"},{"location":"notes/l10_mapreduce/#multiple-processors-with-distributed-memory","title":"Multiple processors with distributed memory","text":"<p>As the number of processors increases, the processing power per dollar for multiple processors with shared memory is diminishing. Large scale systems are often built over multiple processors (multiple servers) with distributed memory. Each processor (set of processors) has its own dedicated memory. They are connected and communicating through high speed networks.</p>"},{"location":"notes/l10_mapreduce/#software-parallelism","title":"Software Parallelism","text":"<p>In the earlier year of study, we came acrossing the concept of multithreading. In this module, we focus on the other two commonly used (higher level) software parallelism concepts: task parallelism and data paralleism.</p>"},{"location":"notes/l10_mapreduce/#task-parallelism","title":"Task Parallelism","text":"<p>In Task parallelism, if some sub-computations (sub tasks) are mutually independent, (which can be identified via human checking or software analysis), they can be executed in parallel. For instance,</p> <pre><code>def cook_beef_bolognese(beef,spaghetti):\n  sauce = cook_sauce(beef) # task A\n  pasta = cook_pasta(spaghetti) # task B\n  return mix(sauce,pasta)\n</code></pre> <p>If we identify that sub task of cooking sauce is independent of the sub task of cooking pasta, we can execute the two sub tasks in parallel when the hardware resource is available.</p> <p>Note that in Task parallelism, the sub tasks are not necessary having the same set of instructions.</p>"},{"location":"notes/l10_mapreduce/#data-parallelism","title":"Data Parallelism","text":"<p>In data parallelism, we compute the result by running a common routine repeatedly over a set of input data. If there is no dependency among the iteration, we could parallize these computations too. </p> <p>For example,</p> <pre><code>def fib(n): ... \ndef main():\n    inputs = [ 10, 100, 200, ...]\n    results = []\n    for i in inputs: \n        results.append(fib(i))\n    return results\n</code></pre> <p>in the above code snippet, we repeatedly compute the fibonacci numbers given the index positions as the input. Note that these computations are independent. We could rewrite the above using the python `map`` function.</p> <pre><code>def fib(n): ... \ndef main():\n   inputs = [ 10, 100, 200, ...]\n   # task C \n   results = map(fib, inputs) \n   return results\n\ndef map(f,l):\n    if len(l) == 0:\n        return []\n    else:\n        hd = l[0]\n        tl = l[1:]\n        fst = f(hd)\n        rest = map(f,tl)\n        return [fst] + rest\n</code></pre> <p>The above variant is the computing the same results as the original version, except that we are using the map function instead of the for loop. The <code>map</code> function applies a higher order function <code>f</code>  to a list of items <code>l</code>. In case <code>l</code> is an empty list, it returns an empty list. otherwise, it applies <code>f</code> to the first item in <code>l</code> and recursively calls itself to handle the rest of the elements. (Note: since Python 3, the <code>map</code> is implemented in iterator style. However, we still use the above version for the ease of reasoning.) One motivation to use <code>map</code> instead of for-loop is to explicitly show that</p> <ol> <li>each call to <code>f(hd)</code> for each item in <code>l</code> is using the same instruction</li> <li>each call to <code>f(hd)</code> for each item in <code>l</code> is indpendent (c.f. task parallelism)</li> </ol> <p>Exploiting these results, the compiler could parallelize these calls, e.g. by scheduling <code>fib(10)</code> in CPU core 1, <code>fib(100)</code> in CPU core 2, and etc. </p> <p>Unfortunately, Python does not have data parallelism builtin for <code>map</code>. In some other languages such as Scala, the <code>map</code> function has builtin support of data parallelism. We will see some of the demo during the lecture when time permits.</p>"},{"location":"notes/l10_mapreduce/#data-parallelism-and-determinism","title":"Data Parallelism and determinism","text":""},{"location":"notes/l10_mapreduce/#defintion-determinisism","title":"Defintion: Determinisism","text":"<p>We say a programm \\(P\\) is deterministic iff  for all input \\(i_1\\) and \\(i_2\\) such that \\(i_1 \\equiv i_2\\) then \\(P(i_1) \\equiv P(i_2)\\).</p> <p>In the presence of parallelism and concurrency, it is tricky to ensure a program is determinsitic. </p> <p>Fortunately, for <code>map</code> data parallelism, all we need is a side condition to ensure deterministic semantics. </p>"},{"location":"notes/l10_mapreduce/#pure-function","title":"Pure function","text":"<p>Recall from some earlier module, we learned that a function <code>f</code> is pure if it does not modify nor is dependent on the external state when it is executed.</p> <p><code>map(f,l)</code> is guaranteed to be deterministic (regardless whether it is executed sequentially or in parallel.)</p>"},{"location":"notes/l10_mapreduce/#data-parallelism-with-reduce","title":"Data Parallelism with <code>Reduce</code>","text":"<p>Orthogonal to <code>map</code>, function <code>reduce(f, l, initial)</code> aggregates all the items in <code>l</code> with a binary operation <code>f</code>, using <code>initial</code> as the initial aggregated value.</p> <pre><code>def reduce(f,l,acc):\n    if len(l) == 0: \n        return acc\n    else:\n        hd = l[0]\n        tl = l[1:]\n        return reduce(f, tl, f(hd,acc))\n\ndef reduce(f,l):\n    return reduce(f,l[1:], l[0])\n</code></pre> <p>For example,</p> <pre><code>def main():\n    inputs = [10, 100, 200, 400]\n    result = reduce(lambda x,y:x+y,inputs,0)\n    return result\n</code></pre> <p>computes the sum of all numbers found in <code>inputs</code>, i.e. it is effectively computing </p> <pre><code>(0 + 10 + 100 + 200 + 400)\n</code></pre> <p>If we are given 2 CPU cores, we could evaluate <code>(0 + 10 + 100)</code> in Core 1, <code>(200 + 400)</code> in Core 2, then <code>(110 + 600)</code> in Core 1. </p>"},{"location":"notes/l10_mapreduce/#reduce-and-determinism","title":"<code>Reduce</code> and determinism","text":"<p>Given that a binary function <code>f</code> is pure, commutative and associative it is guaranteed that <code>reduce(f,l,a)</code> can be parallelized. The results will be the same as it is executed sequentially.</p> <p>Note: A binary function <code>f</code> is commutative iff <code>f(x,y) = f(y,x)</code>. A binary function <code>f</code> is associative iff <code>f(x,f(y,z)) = f(f(x,y),z)</code>.</p>"},{"location":"notes/l10_mapreduce/#mapreduce-framework","title":"MapReduce Framework","text":"<p>Though this looks prosing on paper, </p> <ul> <li>In practice, it won't scale well with each map or reduce task being teeny tiny. </li> <li>It is better to partition data into chunks so that each map or reduce task is reasonably large enough.</li> </ul> <p>This leads to the MapReduce Framework found in Google FS, Hadoop and many other big data platforms.</p>"},{"location":"notes/l10_mapreduce/#the-toy-mapreduce-framework","title":"The Toy MapReduce Framework","text":"<p>For the ease of understanding, we consider a scaled down and simplified implementation of the MapReduce Framework in Python.  Though there is no parallelism builtin, we know that when the same library can be easily ported to other languguages or environments with parallel <code>map</code> and <code>reduce</code> support. </p> <p>Besides <code>map</code> and <code>reduce</code> we need a few more combinators (functions)</p> <pre><code>def flatMap(f,l):\n    ll = map(f,l)\n    return reduce(lambda x,y:x+y, ll, [])\n</code></pre> <p><code>flatMap</code> is similar to map, except that each inner list is flattened. e.g.  <code>flatMap(lambda x: [x+1], [1,2,3])</code> yields <code>[2,3,4]</code>.</p> <pre><code>def lift_if(p,x):\n    if p(x):\n        return [x]\n    else:\n        return []\n\ndef filter(p,l):\n    return flatMap(lambda x:lift_if(p,x), l)\n</code></pre> <p><code>filter</code> returns a new list whose elements are from <code>l</code> and satisfying the test <code>p</code>. e.g. <code>filter(lambda x:x%2==0, [1,2,3,4,5,6])</code> yields <code>[2,4,6]</code>.</p> <pre><code>def merge(kvls1, kvls2):\n    if len(kvls1) == 0: return kvls2\n    elif len(kvls2) == 0: return kvls1\n    else:\n        ((k1,vl1), tl1) = (kvls1[0], kvls1[1:])\n        ((k2,vl2), tl2) = (kvls2[0], kvls2[1:])\n        if k1 == k2: return [(k1,vl1+vl2)]+merge(tl1,tl2)\n        elif k1 &lt; k2: return [(k1,vl1)]+merge(tl1,kvls2)\n        else: return [(k2,vl2)]+merge(kvls1, tl2)\n\ndef shuffle(kvs):\n    kvls = map(lambda kv: [(kv[0], [kv[1]])], kvs)\n    return reduce(merge, kvls, [])\n</code></pre> <p>We assume that there exists a total order among keys.  Given a list of key-value pairs, <code>shuffle</code> shuffles and merge values sharing the same key. e.g. <code>shuffle([(\"k1\",1),(\"k2\",1), (\"k1\",2), (\"k2\",3)])</code> yields <code>[('k1', [1, 2]), ('k2', [1, 3])]</code></p> <pre><code>def reduceByKey(f, kvs, acc):\n    s = shuffle(kvs)\n    return map(lambda p: (p[0], reduce(f,p[1],acc)), s)\n\ndef reduceByKey(f, kvs):\n    s = shuffle(kvs)\n    return map(lambda p: (p[0], reduce(f,p[1])), s)\n</code></pre> <p><code>reduceByKey</code> shuffles the list of key-value pairs, grouping them by keys, then applies the binary aggregation function <code>f</code> to values in  each group. e.g. <code>reduceByKey(lambda x,y:x+y, [(\"k1\",1),(\"k2\",1), (\"k1\",2), (\"k2\",3)],0)</code> yields <code>[('k1', 3), ('k2', 4)]</code></p> <p>Note that all these combinators are implemented using <code>map</code> and <code>reduce</code>. If <code>map</code> and <code>reduce</code> are parallelized, so are these combinators.</p> <p>Apart from <code>reduceByKey</code>, we would also like to include a variant </p> <p><pre><code>def reduceByKey2(agg, kvs):\n    return map(agg, shuffle(kvs))\n</code></pre> Both variants call shuffle to partition the input. The difference is that given a partition <code>(key,values)</code> obtained from the shuffled results, the function <code>agg</code> in <code>reducedByKey2</code> is applied to aggregate <code>values</code>, and returns the key of the partition and the aggregated result. Note that <code>agg</code> is given by the user/programmer, which might not be implemented using <code>reduce</code>, in contrast, the <code>f</code> in <code>reduceByKey</code> is applied to the <code>reduce(f,values)</code>.</p>"},{"location":"notes/l10_mapreduce/#example-word-count","title":"Example : Word Count","text":"<p>In this example, we would to write a program which opens a text file, reads all the words in the file and counts the number of occurences of words.</p>"},{"location":"notes/l10_mapreduce/#using-for-loop","title":"Using for loop","text":"<pre><code>infile = open(sys.argv[1], 'r')\ndict = {}\nfor line in infile:\n     words = line.strip().split()\n     for word in words:\n         if (dict.has_key(word)):\n             dict[word] +=1\n         else:\n             dict[word] = 1\nfor word,count in dict.items():\n    print(\"%s,%d\\n\" % (word,count))\n</code></pre>"},{"location":"notes/l10_mapreduce/#using-toy-mapreduce","title":"Using Toy MapReduce","text":"<pre><code>infile = open(sys.argv[1], 'r')\nlines = []\nfor line in infile: lines.append(line.strip())\n\ndef f(text):\n    wordand1s = []\n    for word in text.split(): wordand1s.append((word,1))\n    return wordand1s\n\ndef g(p):\n    word,icounts = p\n    return (word, sum(icounts))\n\nw1s = flatMap(f,lines)\n\nres = reduceByKey2(g, w1s)\n\nfor word,count in res: print(\"%s,%d\" % (word,count))\n</code></pre> <p>we abstract away the use of for loop and the dictionary by using <code>flatMap</code> and <code>reduceByKey2</code>.</p>"},{"location":"notes/l10_mapreduce/#using-mapreduce-in-hadoop","title":"Using MapReduce in Hadoop","text":"<p>We consider using the Python API of Hadoop (a.k.a. pydoop). Hadoop MapReduce provides the standard parallelized/distributed implementtation of the <code>flatMap</code> and <code>reduceByKey2</code>, we don't need to worry implement them.</p> <pre><code>def mapper(key, text, writer): \n    for word in text.split():\n        writer.emit(word, \"1\")\n\ndef reducer(word, icounts, writer):\n    writer.emit(word, sum(map(int, icounts)))\n</code></pre> <p>the function <code>mapper</code> is taking the role of <code>f</code> in the toy mapreduce version, and function <code>reducer</code> is taking the role of <code>g</code>.</p> <p>We can think of <code>writer.emit</code> is similar to the regular <code>return</code> and <code>yield</code> in Python's iterator depending on the context. </p> <p>Note that <code>mapper</code> also takes a key as input. Hadoop generalize to all data that potentially has a key for each entry. In case like the input ot the mapper is a plain text file, the key is the byte offset w.r.t to the start of the text file.</p> <p>Since we are using Pydoop, the integration with with the Hadoop MapReduce is just to run it as a script.  <pre><code>$ pydoop script wordcount.py /input /output\n</code></pre></p> <p>Note that we do not need to call <code>flatMap</code> or <code>reduceByKey2</code> explicitly.</p> <p>In the following chart illustrate of the steps taken place during wordcount example.</p> <p></p>"},{"location":"notes/l10_mapreduce/#hadoop-mapreduce-architecture","title":"Hadoop MapReduce Architecture","text":"<p>The following diagram show a basic structure of all the server and clients component of the Hadoop MapReduce.</p> <p></p> <p>In Hadoop v1, a JobTracker is spawned for coordination purposes; TaskTrackers are spawed for executing the actual jobs. We can think of the JobTrackers are run in the name node, and the Tasktrackers are processes in the data nodes.</p> <p>We leave the advanced architecture of Hadoop Version 2+ in the upcoming lesson, i.e. YARN. </p>"},{"location":"notes/l10_mapreduce/#hadoop-mapreduce-job-management","title":"Hadoop MapReduce Job Management","text":"<p>Hadoop MapReduce follows the locallity principal, i.e. computation must be moved to the data, i.e. compute at the data nodes. This is possible thanks to the determinism property discussed earlier. These common tasks, mappers and reducers are compiled and packaged as (JAR) and uploaded to the data node. </p> <p>The input data are (pre-) partitioned into splits, (by data nodes). Data are fed to the mappers by split.  </p> <p>The output of the mappers will be re-shuffled and re-partitioned and sent to the reducers. </p> <p>In some cases combiners are used. Combiners are like local mini-reducers, they reduce the immediate output coming straight from the mapper task so that the network traffic can be further reduced. </p>"},{"location":"notes/l11_spark/","title":"50.043 Spark","text":""},{"location":"notes/l11_spark/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this lesson, you are able to</p> <ul> <li>Differentiate Hadoop MapReduce and Spark</li> <li>Apply Spark based on application requirements</li> <li>Develop data engineering process using Spark RDD</li> <li>Reason about Spark Execution Model</li> <li>Develop machine learning application using Spark MLLib</li> <li>Explain Spark Architecture</li> <li>Develop Data processing application using Spark Dataframe</li> <li>Develop Machine Learning application using Spark ML package</li> <li>Explain Spark Streaming</li> </ul>"},{"location":"notes/l11_spark/#spark-vs-hadoop-mapreduce","title":"Spark VS Hadoop MapReduce","text":"<p>Hadoop MapReduce was created to batch process data once or twice, e.g.  web search index, processing crawled data, etc.</p> <p>When machine learning being applied to big data, the terabyte or zeta byte of data probably need to be processed iteratively.  For instance, if we need to run gradient descent thousands of times.</p> <p>On top of that data visualization for big data also impose further challenge. It requires data to be reprocessed based on the user inputs such as sorting and filtering constraints. </p> <p>Hadoop MapReduce is no longer suitable for these applications because of the following  1. each of mapper (and educer) task needs to transfer intermediate data to the disk back and forth.  2. its rigit computation model (i.e. one step of map followed by one step of reduce) makes most of the application look unnecessarily complex. 3. it does not utilize much of RAM. Many of the MapReduce applications are disk and network I/O bound rather than RAM and CPU bound. 4. it is hard to redistribute the workload</p> <p>Apache Spark is a project which started off as an academic reseach idea and became an enterprise level success. It addressed the above-mentioned limitations of the Hadoop MapReduce by introducing the following features</p> <ul> <li>Resilient distributed datasets, which act as the primary data structure for distributed map reduce operation</li> <li>It unions all the available RAM from the cluster (all data nodes) to form a large pool of virtual RAM. RDD and its derivative such as dataframe and dataset, are in memory parallel distributed data structure to be used in the virtual RAM pool.</li> <li>It has a MapReduce like programming interface (closer to our toy MapReduce   library compared to the Hadoop MapReduce)</li> <li>It offers fault tolerance, RDD, dataframe and datasets can always be re-computed in case of node failure   </li> <li>Machine Learning Libraries, Graph computation libraries.</li> </ul> <p>Spark supports many mainstream programming languagues such as Scala, Python, R, Java and SQL. In this module we consider the Python interface.</p>"},{"location":"notes/l11_spark/#spark-rdd-api","title":"Spark RDD API","text":"<p>The primary data structure of the Spark RDD API is the RDD. </p> <pre><code>data = [1, 2, 3, 4, 5]\ndistData = sc.parallelize(data)\n</code></pre> <p>In the above code snippet, we initialize a list of integers and turn it into an RDD <code>distData</code>. </p> <p>Alternatively, we can load the data from a file.</p> <pre><code>distData = sc.textFile(\"hdfs://127.0.0.1:9000/data.txt\")\n</code></pre> <p>Given the RDD, we can now perform data manipulation. There are two kinds of RDD APIs, namely Transformations and Actions.  Transformation APIs are lazy and action APIs are strict.</p>"},{"location":"notes/l11_spark/#lazy-vs-strict-evaluation","title":"Lazy vs Strict Evaluation","text":"<p>In lazy evaluation, the argument of a function is not fully computed until it is needed. We can mimic this in Python using generator and iterator.</p> <p><pre><code>r = range(2,-1,-1) # a range 2,1,0\nl = ( 4 // x for x in r) # a generator 2, 4, div_by_0\ndef takeOne(l):\n    res = next(iter(l), None)\n    if res is None:\n        return error(\" ... \")\n    else:\n        return res\ntakeOne(l) # yield 2\n</code></pre> When <code>takeOne(l)</code> is called, only the first element of <code>l</code> is computed, the rest are discarded. </p> <p>In strict evaluation, the argument of a function is alwaysfully computed.</p> <p><pre><code>def takeOneStrict(l):\n    res = next(iter(list(l)), None)\n    if res is None:\n        return error(\" ... \")\n    else:\n        return res\ntakeOneStrict(l) # yield an div by zero error.\n</code></pre> When <code>takeOneStrict(l)</code> is called, even though only the first element of <code>l</code> is required, the rest are computed eagerly, thanks to the <code>list()</code> function call materializing the generator.</p>"},{"location":"notes/l11_spark/#rdd-transformations","title":"RDD Transformations","text":"<p>All Transformations takes the current RDD as (part of) the input and return some a new RDD as output.</p> <p>Let <code>l</code>, <code>l1</code> and <code>l2</code> be RDDs, <code>f</code> be a function</p> RDD APIs Description Toy MapReduce equivalent <code>l.map(f)</code> <code>map(f,l)</code> <code>l.flatMap(f)</code> <code>flatMap(f,l)</code> <code>l.filter(f)</code> <code>filter(f,l)</code> <code>l.reduceByKey(f)</code> <code>reduceByKey(f,l)</code> <code>l.mapPartition(f)</code> similar to <code>map</code>, <code>f</code> takes an iterator and produces an iterator NA <code>l.distinct()</code> all distinct elems N.A. <code>l.sample(b,ratio,seed)</code> sample dataset. <code>b</code>: a boolean value to indicate w/wo replacement. <code>ratio</code>: a value range [0,1] N.A. <code>l.aggregateByKey(zv)(sop,cop)</code> <code>zv</code>: accumulated value. <code>sop</code>: intra-partition aggregation function. <code>cop</code>: inter-partition aggregation function similar to <code>reduceByKey(f,l,acc)</code>, except that we don't have 2 version of <code>f</code> <code>l1.union(l2)</code> union <code>l1</code> <code>l2</code> <code>l1 + l2</code> <code>l1.intersection(l2)</code> the intersection of elements from <code>l1</code> and <code>l2</code> N.A. <code>l1.groupByKey()</code> group elemnts by keys <code>shuffle(l1)</code> <code>l1.sortByKey()</code> sort by keys N.A. <code>l1.join(l2)</code> join <code>l1</code> <code>l2</code> by keys we've done it in lab <code>l1.cogroup(l2)</code> similar to <code>join</code>, it returns RDDs of <code>(key, ([v1,..], [v2,..]))</code>, <code>[v1,...]</code> are values from <code>l1</code>, <code>[v2,...]</code> are values from <code>l2</code> N.A. <p>Note that the RDDS APIs follow the builtin Scala library's convention, <code>map</code>, <code>filter</code> and etc are methods of the List class.</p>"},{"location":"notes/l11_spark/#rdd-actions","title":"RDD Actions","text":"<p>All Actions takes the current RDD as (part of) the input and return some value that is not an RDD. It forces computation to happen.</p> RDDs Desc Toy MR <code>l.reduce(f)</code> <code>reduce(f,l)</code> <code>l.collect()</code> converts rdd to a local array <code>l.count()</code> <code>len(l)</code> <code>l.first()</code> <code>l[0]</code> <code>l.take(n)</code> returns an array <code>l[:n]</code> <code>l.saveAsTextFile(path)</code> save rdd to text file N.A. <code>l.countByKey()</code> return hash map of key and count N.A. <code>l.foreach(f)</code> run a function for each element in the dataset with side-effects <code>for x in l: ...</code>"},{"location":"notes/l11_spark/#special-transformation","title":"Special Transformation","text":"<p>Some transformation/opereations such as <code>reduceByKey</code>, <code>join</code>, <code>groupByKey</code> and <code>sortByKey</code> will trigger a shuffle event, in which Spark redistribute the data across partititon, which means intermediate results from lazy operations will be materialized. </p>"},{"location":"notes/l11_spark/#wordcount-example-in-spark","title":"Wordcount example in Spark","text":"<p>The follow code snippet, we find the wordcount application implemented using PySpark.</p> <pre><code>import sys\nfrom pyspark import SparkContext, SparkConf\nconf = SparkConf().setAppName(\"Wordcount Application\")\nsc = SparkContext(conf=conf)\n\ntext_file = sc.textFile(\"hdfs://localhost:9000/input/\")\ncounts = text_file.flatMap(lambda line: line.split(\" \")) \\\n             .map(lambda word: (word, 1)) \\\n             .reduceByKey(lambda a, b: a + b)\ncounts.saveAsTextFile(\"hdfs://localhost:9000/output/\")\nsc.stop()\n</code></pre> <p>We can compare it with the version in toy MapReduce, and find many similarities </p> <pre><code>infile = open(sys.argv[1], 'r')\nlines = []\nfor line in infile: lines.append(line.strip())\n\nws = flatMap(lambda line:line.split(\" \"),lines)\nw1s = map(lambda w:(w,1), ws)\nres = reduceByKey(lambda x,y:x+y,w1s,0)\n\nwith open(sys.argv[2], 'w') as out:\n    for w,c in res:\n        out.write(w + \"\\t\" + str(c) + \"\\n\")\n</code></pre> <p>We will see more examples of using PySpark in the lecture and the labs.</p>"},{"location":"notes/l11_spark/#spark-architecture","title":"Spark Architecture","text":"<p>Next we consider the how Spark manages the execution model.</p> <p>In the following, we find the Spark Architecture (in simple standalone mode)</p> <p></p> <p>Like Hadoop, Spark follows a simple master and worker architecture. A machine is dedicated to manage the Spark cluster, which is known as the Spark Master node (c.f. namenode in a Hadoop Cluster). A set of machines are in charge of running the actual computation, namely, the Spark Worker nodes (c.f. data nodes in a Hadoop Cluster).</p> <p></p> <p>A driver is a program that runs on the Spark Master node, which manages the interaction between the application and the client. Upon a job submission from the client, a Spark driver schedules the jobs by analyzing the sub tasks dependency and allocate the sub tasks to the executors. A list of executors (runnning on some worker nodes) receive tasks from the driver and reports the result upon completion.</p>"},{"location":"notes/l11_spark/#spark-task-scheduling","title":"Spark Task Scheduling","text":"<p>In this section, we take a look at how Spark divides a given job into tasks and schedules the tasks to the workers. </p> <p></p> <p>As illustrated by the above diagram Spark takes 4 stages to schedule the job. </p> <ol> <li>Given a job, Spark builds a directed acyclic graph (DAG) which represents the dependencies among the operations performed in the job.  </li> <li>Given the DAG, Spark splits the graph into multiple stages of tasks.</li> <li>Tasks are scheduled according to their stages, A later stage must not be started until the earlier stages are completed. </li> <li>Tasks scheduled to the workers then executed. </li> </ol>"},{"location":"notes/l11_spark/#a-simple-example","title":"A simple example","text":"<p>Let's consider a simple example </p> <p><pre><code># spark job 1\nr1 = sc.textFile(\"...\")\nr2 = r1.map(f) \n</code></pre> A spark driver takes the above program and construct the DAG, since it contain just a read followed by a map operations. It has the following DAG. </p> <pre><code>graph TD\n    r1 --map--&gt; r2 </code></pre> <p>It is clear that there is only one stage, (because there is only one operation, one set of inputs and one set of outputs). </p> <p></p> <p>The Task Scheduler allocate the tasks in parallel, as follows.</p> <p></p>"},{"location":"notes/l11_spark/#a-less-simple-example","title":"A less simple example","text":"<p>Let's consider another example </p> <pre><code># spark job 2\nr1 = sc.textFile(\"...\")\nr2 = r1.map(f)\nr3 = sc.textFile(\"...\")\nr4 = r3.map(g)\nr5 = r2.join(r4)\nr6 = r5.groupByKey()\nr7 = r6.map(h)\nr8 = r7.reduce(i)\n</code></pre> <p>The DAG is as follows</p> <pre><code>graph TD;\nr1 --map--&gt; r2 \nr3 --map--&gt; r4\nr2 --join--&gt; r5\nr4 --join--&gt; r5\nr5 --groupByKey--&gt; r6\nr6 --map--&gt; r7\nr7 --reduce--&gt; r8</code></pre> <p>There are mulitple way of dividing the above DAG into stages. For instance, we could naively turns each operation (each arrow) into a stage (we end up with many stages and poor parallelization) or we group the simple paths (straight line paths) into a stage.</p> <p>Spark takes a more intelligent approach by classifying different types of dependencies. </p> <ul> <li> <p>Narrow dependencies - Input partition used by at most 1 output partition</p> </li> <li> <p>Wide dependencies - Input partition used by more than one output partitions</p> </li> </ul> <p></p> <p>Thus for <code>Spark Job 2</code>,  we further annotate the DAG with dependency types.</p> <pre><code>graph TD;\nr1 --map/narrow--&gt; r2 \nr3 --map/narrow--&gt; r4\nr2 --join/narrow--&gt; r5\nr4 --join/wide--&gt; r5\nr5 --groupByKey/wide--&gt; r6\nr6 --map/narrow--&gt; r7\nr7 --reduce--&gt; r8</code></pre> <p>All the map operations are narrow. The join of <code>r2</code> and <code>r4</code> should be wide. However since the result must be ordered either by <code>r2</code>'s partition order or <code>r4</code>'s. Only one side of the operation is wide. In this case, we assume <code>r5</code>'s partition follows <code>r2</code>'s, hence <code>r2</code> to <code>r5</code> is narrow. <code>groupByKey</code> operations are wide. </p> <p>Next we can decide how many stages we need by </p> <ol> <li>allowing narrow dependencies preceding the same wide dependency to the grouped under the same stage, because they do not incur any network I/O. <ul> <li>We apply task parallelism here., </li> </ul> </li> <li>wide dependency initiates a new stage, as we need to wait for all the data operands to be fully computed before the operation being executed. </li> </ol> <pre><code>graph TD;\nr1 --map/narrow/1--&gt; r2 \nr3 --map/narrow/1--&gt; r4\nr2 --join/narrow/2--&gt; r5\nr4 --join/wide/2--&gt; r5\nr5 --groupByKey/wide/3--&gt; r6\nr6 --map/narrow/3--&gt; r7\nr7 --reduce/3--&gt; r8</code></pre> <p>In the above, we stage the first two map operations. The join operation is assigned to the 2<sup>nd</sup> stage. The <code>groupByKey</code> initiates the 3<sup>rd</sup> stage which includes the following map and reduce. </p>"},{"location":"notes/l11_spark/#spark-performance-tuning","title":"Spark performance tuning","text":"<p>Knowing how Spark schedules a job into stages of sub tasks. We can optimize the job by rewriting it into an equivalent one such that </p> <ul> <li>Pre-partition or re-partition of the data</li> <li>Cache the common intermediate data</li> </ul>"},{"location":"notes/l11_spark/#pre-partition-or-re-partition-of-the-data","title":"Pre-partition or re-partition of the data","text":"<p>Recall that a spark job is represented a DAG of stages</p> <p></p> <p>If we know the join operation and pre- (or re-) arrange the data according to the key to partition mapping, we could reduce the shuffling. </p> <p></p> <pre><code>d1 = [(1, \"A\"), (2, \"B\"), (1, \"C\"), (2, \"D\"), (1, \"E\"), (3, \"F\")]\nd2 = [(1, 0.1), (2, 0.2), (2, 3.1), (3, 0)]\nr1 = sc.parallelize(d1) \nr2 = sc.parallelize(d2)\n</code></pre> <p>let assume that <code>r1</code> is by default partitioned randomly into two partitions and so is <code>r2</code>.</p> <pre><code>r1.glom().collect() # collect rdd into list by retaining the partition\nr2.glom().collect()\n# r1 being partitioned \n[[(1, 'A'), (2, 'B'), (1, 'C')],   # p1\n  [(2, 'D'), (1, 'E'), (3, 'F')]]  # p2\n# r2 being partitioned\n[[(1, 0.1), (2, 0.2)],  # p3\n  [(2, 3.1), (3, 0)]])  # p4\n</code></pre> <p>next we would like join <code>r1</code> with <code>r2</code> by key, i.e. the first component of the pairs</p> <pre><code>r3 = r1.join(r2)\nr3.glom().collect()\n</code></pre> <p>the result will be stored in three partitions</p> <pre><code>[[(1, ('A', 0.1)), (1, ('C', 0.1)), (1, ('E', 0.1))], # p1\n [(2, ('B', 0.2)), (2, ('B', 3.1)), (2, ('D', 0.2)), (2, ('D', 3.1))], # p2\n [(3, ('F', 0))]] # p3\n</code></pre> <p>Note that the actual order of the collected result might not be same as above, Spark tries to reuse partitions being created whenever possible. (For breivity we omit an empty partition <code>p4</code>.)</p> <p>As observed, there are tuple transferred from <code>p1</code> to <code>p2</code>, e.g. <code>(2, 'B')</code>  and from <code>p2</code> to <code>p1</code>, e.g. <code>(1, 'E')</code>. These transfers can be eliminited  if we manually control the partitioning of the initial RDDs, </p> <pre><code>r4 = sc.parallelize(d1).partitionBy(3, lambda key: key)\nr5 = sc.parallelize(d2).partitionBy(3, lambda key: key)\nr4.glom().collect()\nr5.glom().collect()\n</code></pre> <pre><code># r4 is partitioned \n[[(3, 'F')], #p1 \n [(1, 'A'), (1, 'C'), (1, 'E')], #p2 \n [(2, 'B'), (2, 'D')]] #p3\n# r5 is partitioned \n[[(3, 0)],  #p4\n[(1, 0.1)], #p5\n[(2, 0.2), (2, 3.1)]] #p6\n</code></pre> <p>when we perform the join</p> <pre><code>r6 = r4.join(r5)\nr6.glom().collect()\n</code></pre> <p>we have </p> <p><pre><code>[\n[(3, ('F', 0))], #p1\n[(1, ('A', 0.1)), (1, ('C', 0.1)), (1, ('E', 0.1))], #p2\n[(2, ('B', 0.2)), (2, ('B', 3.1)), (2, ('D', 0.2)), (2, ('D', 3.1))] #p3\n]\n</code></pre> The number of tuples being transferred across partition is now minimized. </p> <ul> <li>Sample code can be found here. <pre><code>https://colab.research.google.com/drive/1XO1hqcRCn9JKu0tkRb0ANQylCnBod3B-?usp=sharing\n</code></pre></li> </ul>"},{"location":"notes/l11_spark/#cache-the-intermediate-data","title":"Cache the intermediate data","text":"<p>Recall that in Spark transformation are lazy until a shuffling is required. Laziness is a double-edged sword. </p> <p></p> <p>In the above, the orange partition (data) is being used by 3 different \"sinks\". If all the operations above the last levels are transformations (i.e. lazy.) Having 3 sink operations will require most of the intermediate partitions (all boxes except for the last) </p> <p>For example consider the following</p> <pre><code>data = [(1, 100), (1, 90), (1, 80), (2, 80), (2, 30), (2, 50)]\nr1 = sc.parallelize(data)\nr2 = r1.map( lambda x:(x[0], x[1] * 0.01))\nr3 = r2.groupByKey()\nr4 = r3.map( lambda x:(x[0], std(x[1])))\nr5 = r3.map( lambda x:(x[0], min(x[1])))\nr4.collect()\nr5.collect()\n</code></pre> <p>There are two downstream operation of <code>r3</code>, namely <code>r4</code> computing the standard deviation by key, and <code>r5</code> computing the minimum by key. </p> <p>Due to the fact that <code>r3</code> is lazy. The <code>r4.collect()</code> action (sink) triggers the computation of <code>r1</code> to <code>r3</code> and <code>r4</code>. The <code>r5.collect()</code> triggers the recomputation of <code>r1</code> to <code>r3</code> and the computation of <code>r5</code>.</p> <p>If we materialize <code>r3</code> and cache it, we would avoid the recomputation of <code>r1</code> to <code>r3</code>. </p> <pre><code>data = [(1, 100), (1, 90), (1, 80), (2, 80), (2, 30), (2, 50)]\nr1 = sc.parallelize(data)\nr2 = r1.map( lambda x:(x[0], x[1] * 0.01))\nr3 = r2.groupByKey().cache()\nr4 = r3.map( lambda x:(x[0], std(x[1])))\nr5 = r3.map( lambda x:(x[0], min(x[1])))\nr4.collect()\nr5.collect()\n</code></pre> <ul> <li>Sample code <pre><code>https://colab.research.google.com/drive/1QqUSa5Kkjpw3Avw2DlWX9ZMrZcsnDz5x?usp=sharing\n</code></pre></li> </ul>"},{"location":"notes/l11_spark/#other-optimization-tricks","title":"Other optimization tricks","text":"<p>Besides the above mentioned two approaches, a trick that we've used in RDBMS for optimization is also applicable to Spark. i.e. apply filter as early as possible so as to reduce the size of intermediate output. </p> <p>Another Spark specific optimization trick is to rewriting <code>groupByKey().map(...)</code> by <code>reduceByKey(...)</code>. However in some cases, a rewrite solution might not exist.</p>"},{"location":"notes/l11_spark/#spark-failure-recovery","title":"Spark Failure Recovery","text":"<p>Recall that for any MapReduce implementation to produce deterministic results. The computation must be pure. </p> <p>It turns out that purity property makes failure recovery much easier.  For each Spark job, a lineage of the sub tasks is computed. In the event of failure, the Spark driver will refer to the lineage and recompute the affected sub-tasks. Thanks to the purity property, partially completed and incomplete computation can always be computed without the need of restoring the original state.</p> <p>For instance consider the following job</p> <p></p> <p>Where all the square boxes denote the partitions of some RDDs, suppose partition <code>C1</code> is faulty.</p> <p></p> <p>Based on the diagram (lineage), we know that we can recompute <code>C1</code> elsewhere by using <code>B1</code> and <code>B2</code>.</p>"},{"location":"notes/l11_spark/#spark-dataframe","title":"Spark DataFrame","text":"<p>Besides Spark RDD, Spark offers DataFrame as a higher level API interface to the programmers and data engineers. The ussage is influenced and inspired by Python's Pandas.</p> <p>In a nutshell, a dataframe can be seenas a schema plus a set of RDDs. </p> <p></p> <p>Since Dataframe was designed for machine learning applications, it adopts the column-based data structure instead of row based. </p>"},{"location":"notes/l11_spark/#question-why-columnar","title":"Question: Why Columnar?","text":"<p>Hint: How data are used in ML model?</p>"},{"location":"notes/l11_spark/#creating-spark-dataframe","title":"Creating Spark Dataframe","text":"<p>We can convert a Spark rdd into a dataframe.</p> <pre><code>data = [(1, 100), (1, 90), (1, 80), (2, 80), (2, 30), (2, 50)]\nr1 = sc.parallelize(data)\ndf1 = r1.toDF(\"id\", \"score\")\n</code></pre> <p>Alternatively, we can create DataFrames directly from a CSV file.</p> <p>Given file <code>hdfs://127.0.0.1:9000/foo.csv</code> <pre><code>foo,bar\n1,true\n2,false\n3,true\n4,false\n</code></pre> <pre><code>df = sparkSession.read\\\n     .option(\"header\", \"true\")\\\n     .option(\"inferSchema\", \"true\")\\\n     .csv(\"hdfs://127.0.0.1:9000/foo.csv\")\ndf.printSchema()\n</code></pre> shows <pre><code>root\n |-- foo: integer (nullable = true)\n |-- bar: boolean (nullable = true)\n</code></pre></p> <p>Note in the above, Spark loads a text file (CSV) from HDFS and infers the schema based on the first line and the values.</p>"},{"location":"notes/l11_spark/#dataframe-apis","title":"DataFrame APIs","text":"<p>Let's have tour of the Spark DataFrame APIs by going through some examples.</p> <p>Here is the data we use in the examples</p> <pre><code>data = [(\"100001\", \"Ace\", \"50043\", 90), \\\n        (\"100002\", \"Brandon\", \"50043\", 95), \\\n        (\"100003\", \"Cheryl\", \"50043\", 80)]\ndistData = sc.parallelize(data)\ndf = distData.toDF([\"studentid\", \"name\", \\\n                 \"module\", \"score\"])\ndf.show()\n</code></pre> studentid name module score 100001 Ace 50043 90 100002 Brandon 50043 95 100003 Cheryl 50043 80"},{"location":"notes/l11_spark/#column-projection","title":"Column Projection","text":"<p>To project (select the columns) we use the <code>.select()</code> method.</p> <pre><code>df.select(df[\"studentid\"], df[\"score\"]).show() # or\n\nfrom pyspark.sql.functions import col\ndf.select(col(\"studentid\"), col(\"score\")).show() \n</code></pre> studentid score 100001 90 100002 95 100003 80 <p>To compute a new column based on the existing one, we use an overloaded version of the <code>.select()</code> method whose first argument is the operation and the second argument is the name of the new column.</p> <pre><code>from pyspark.sql.functions import concat, lit\ndf.select(concat(df[\"studentid\"]\\\n         ,lit(\"@mymail.sutd.edu.sg\"))\\\n   .alias(\"email\")).show()\n</code></pre> email 100001@mymail.sut... 100002@mymail.sut... 100003@mymail.sut... <p>For the full set of builtin funtcions for column operations. <pre><code>https://spark.apache.org/docs/3.0.1/api/python/pyspark.sql.html\n</code></pre></p> <p>There are times we want to create a new column and keeping the old columns.</p> <pre><code>df.withColumn(\"email\",concat(col(\"studentid\"),\\\n   lit(\"@mymail.sutd.edu.sg\")))\\\n   .show()\n</code></pre> studentid name module score email 100001 Ace 50043 90 100001@mymail.sut... 100002 Brandon 50043 95 100002@mymail.sut... 100003 Cheryl 50043 80 100003@mymail.sut..."},{"location":"notes/l11_spark/#row-filtering","title":"Row filtering","text":"<p>For row filterings, we use <code>.filter()</code> method.</p> <pre><code>df.filter(col(\"studentid\") == \"100003\").show()\n</code></pre> studentid name module score 100003 Cheryl 50043 80 <p>And similar for range filter</p> <pre><code>df.filter(col(\"score\") &gt; 90).show()\n</code></pre> studentid name module score 100002 Brandon 50043 95 <p><code>lit()</code> is optional here, pyspark inserts it for us.</p>"},{"location":"notes/l11_spark/#group-by-and-aggregation","title":"Group By and Aggregation","text":"<p>For aggregation, we use <code>.groupBy()</code> </p> <pre><code>df.groupBy(\"module\").avg().show()\n</code></pre> module avg(score) 50043 88.33333333333333"},{"location":"notes/l11_spark/#join","title":"Join","text":"<p>For join, we use <code>.join()</code> </p> <pre><code>moddata = [(\"50043\", \"Database and Big Data Systems\")]\ndistmodData = sc.parallelize(moddata)\nmoddf = distmodData.toDF([\"module\", \"modname\"])\n\ndf.join(moddf, df[\"module\"] == moddf[\"module\"], \"inner\")\\\n   .select(df[\"studentid\"], df[\"name\"], df[\"module\"],\\\n   df[\"score\"], moddf[\"modname\"]).show()\n</code></pre> studentid name module score modname 100001 Ace 50043 90 Database and Big ... 100002 Brandon 50043 95 Database and Big ... 100003 Cheryl 50043 80 Database and Big ..."},{"location":"notes/l11_spark/#spark-sql","title":"Spark SQL","text":"<p>Besides Spark RDD, Spark allows program to use SQL query to perform data transformation and action. </p> <p>For example,  <pre><code>df.createOrReplaceTempView(\"students\")\nspark.sql(\"SELECT * FROM students\").show()\n</code></pre></p> studentid name module score 100001 Ace 50043 90 100002 Brandon 50043 95 100003 Cheryl 50043 80 <p>With some notebook support, we can even use SQL to perform data visualization.</p>"},{"location":"notes/l11_spark/#spark-machine-learning","title":"Spark Machine Learning","text":"<p>Spark comes with two differen Machine Learning libraries.</p> <ul> <li><code>MLLib</code> package - for RDD</li> <li><code>ML</code> package - for dataframe (and dataset)</li> </ul>"},{"location":"notes/l11_spark/#mllib-package","title":"MLLib package","text":"<p>MLLib package offers a lower level access of data type such as vectors and label points. </p>"},{"location":"notes/l11_spark/#vectors","title":"Vectors","text":"<p>In Spark, vectors are local data collection (non-distributed).  There are dense vectors and sparse vectors.</p> <ul> <li> <p>For dense vector - all values need to be specified when it is created <pre><code>from pyspark.mllib.linalg import * \ndv = Vectors.dense(1.0, 0.0, 3.0)\n</code></pre></p> </li> <li> <p>For sparse vector - we don't need to specify all the value, instead we specify the size of the vector as well as the non-zero values.</p> </li> </ul> <pre><code>sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0]) # or\nsv2 = Vectors.sparse(3, [(0, 1.0), (2, 3.0)])\n</code></pre>"},{"location":"notes/l11_spark/#labeled-points","title":"Labeled points","text":"<p>With features extracted as vectors. We need to find a way to label them.</p> <p>Spark MLLib comes with its own labeled point data type</p> <pre><code>from pyspark.mllib.regression import *\n# Create a labeled point with a positive label\n# and a dense feature vector.\npos = LabeledPoint(1.0, Vectors.dense(1.0, 0.0, 3.0))\n# Create a labeled point with a negative label\n# and a sparse feature vector.\nneg = LabeledPoint(0.0, Vectors.sparse(3, \\\n      [(0, 1.0), (2, 3.0)]))\n</code></pre>"},{"location":"notes/l11_spark/#training-an-inference","title":"Training an inference","text":"<p>With labeled points defined and extracted, assuming <pre><code>pos = ... # RDD of labeled points\nneg = ... # RDD of labeled points\n</code></pre></p> <p>we train the model</p> <pre><code>from pyspark.mllib.classification import SVMWithSGD\ntraining = pos + neg\nnumIteration = 20\nmodel = SVMWithSGD.train(training, numIterations)\n</code></pre> <p>which is a support vector machine with SGD algorithm.</p> <p>To perform inference, we need to feed a new sample as a vector to the model.</p> <pre><code>newInstance = Vectors.dense(1.0, 2.0, 3.0)\nmodel.predict(newInstance)\n</code></pre>"},{"location":"notes/l11_spark/#ml-package","title":"ML Package","text":"<p>As the ML package is targetting the higher level data structures (Dataframe and Dataset), machine learning models in ML package are built using the pipeline. </p>"},{"location":"notes/l11_spark/#the-training-pipeline","title":"The Training Pipeline","text":"<p>One of the training pipelines is known as the estimator</p> <p></p> <p>In the diagram above, it illustrate the pipeline of training a classifier using logistic regression. </p> <pre><code>from pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import HashingTF, Tokenizer\ndata = spark.createDataFrame([\n    (0, \"a b c d e spark\", 1.0),\n    (1, \"b d\", 0.0),\n    (2, \"spark f g h\", 1.0),\n    (3, \"hadoop mapreduce\", 0.0),\n    (4, \"spark is scaling\", 1.0),\n    (5, \"random stuff\", 0.0)\n], [\"id\", \"text\", \"label\"])\n\ntrain, test = data.randomSplit([0.7, 0.3], seed=12345)\n# Configure an estimator pipeline, \ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\nhashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\nlr = LogisticRegression(maxIter=10, regParam=0.001)\npipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n\n# Fit the pipeline to train\nmodel = pipeline.fit(train)\n</code></pre>"},{"location":"notes/l11_spark/#the-transformer-pipeline","title":"The transformer pipeline","text":"<p>The inference pipeline on the other hand is known as the transformer </p> <p></p> <pre><code># Configure an Inference pipeline\n# Note now model include tokenizterm hashingTF, and lr\npipeline_model = PipelineModel(stages=[model]) \nprediction = pipeline_model.transform(test)\nresult = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\nresult.show()\n</code></pre>"},{"location":"notes/l11_spark/#sample-code","title":"Sample code","text":"<pre><code>https://colab.research.google.com/drive/1ZI-BG2XaB3AOyzPrXeqO7xqNGYycCJ-U?usp=sharing\n</code></pre>"},{"location":"notes/l11_spark/#spark-streaming","title":"Spark Streaming","text":"<p>Spark offers Streaming API which handles real time (infinite) input data. </p> <p>The real-timeness is approxmiated by chopping the data stream into small batches.  These small batches are fed to the spark application. </p> <p></p> <p>For example the following is a simplified version of a data streaming application that computes the page views by URL over time.</p> <pre><code>from pyspark import SparkContext\nfrom pyspark.streaming import StreamingContext\nsc = SparkContext(\"local[2]\", \"PageView\")\nssc = StreamingContext(sc, 1)\n\nlines = ssc.socketTextStream(\"localhost\", 9999)\npageViews = lines.map(lambda l:parse(l))\nones = pageViews.map(lambda x: (x.url, 1))\ncounts = ones.runningReduce(lambda x,y: x+y)\n</code></pre>"},{"location":"notes/l11_spark/#additional-references","title":"Additional References","text":"<ul> <li>Spark Dataframe</li> <li>https://spark.apache.org/docs/latest/sql-getting-started.html</li> <li>Spark MLLib and ML package</li> <li>https://spark.apache.org/docs/latest/ml-guide.html</li> <li>Spark Streaming </li> <li>https://spark.apache.org/docs/latest/streaming-programming-guide.html</li> </ul>"},{"location":"notes/l12_yarn/","title":"50.043 Yarn","text":""},{"location":"notes/l12_yarn/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this lesson, you are able to</p> <ul> <li>Articulate the purpose of Hadoop YARN</li> <li>Identify the shortcoming of JobTracker and TaskTracker in Hadoop V1</li> <li>Describe how YARN addresses the above issues</li> <li>List and differentiate the different YARN schedulers</li> </ul>"},{"location":"notes/l12_yarn/#hadoop-and-eco-system","title":"Hadoop and Eco System","text":"<p>We have many tools in the big data tool box</p> <ul> <li>Hadoop HDFS - Main Storage</li> <li>Hadoop MapReduce - Low CPU / Low RAM consumption batch job </li> <li>Spark Job - High CPU / GPU / RAM batch job</li> <li>Spark Stream - Realtime stream</li> <li>...</li> </ul> <p>Spark and Hadoop v1 have incompatible resource managers, they don't understand each other. It is crictical to have a unified resource management tool for all kinds of jobs. </p>"},{"location":"notes/l12_yarn/#hadoop-v1-resource-management-system","title":"Hadoop v1 Resource Management System","text":"<p>Hadoop v1 Resource Management System has many issues. </p> <p>The namenode acts as the master node and the job tracker. It is overloaded with  * Resource Management * Scheduling * Job monitoring * Job lifecycle * Fault-tolerence</p> <p>It soon becomes the bottle neck of the system. It does not scale more than 4000 nodes and 40,000 tasks.</p> <p>The datanodes acts as the taskers. They have static task slots, for instance, 2 slots for mapper task and 2 slots for reducer task. The mapper slots cannot be used for reducer tasks. This leads to additional constraints in task schedules and limitation for load balancing. </p>"},{"location":"notes/l12_yarn/#yarn","title":"YARN","text":"<p>YARN is a general resource management system shipped with Hadoop V2 on wards. It is designed with Hadoop and its eco system. It overcomes most of the limitations found in Hadoop V1. It provides an extensible APIs for non-Hadoop tasks. </p> <p></p>"},{"location":"notes/l12_yarn/#hadoop-v1-vs-yarn","title":"Hadoop V1 vs YARN","text":"<p>In the following diagram we find the correspondence between Hadoop V1 resource manager and YARN</p> <p></p> <p>On the topmost leve, the JobTracker is replaced by an application master which in charge of managing an application running on the cluster. The Application master it is not running on the namenode, instead it runs in one of the task slots in a worker node. The resource managemer which coordinates among the hardware resources and many application masters and node managers, is running on namenode. This addresses the \"jack-of-all-trades\" problem with Hadoop V1 namenode. </p> <p>The task trackers in v1 are replaced by node managers. Each worker node has a node manager running. The node manager manages the resource and task status for the worker node. </p> <p>The fixed number of mapper slots and reducer slots are replaced by a container in the worker node, which has the flexibility of running any task (mapper, reducer, application master, and etc), scheduled by the scheduler. </p>"},{"location":"notes/l12_yarn/#yarn-job-submission-work-flow","title":"Yarn Job submission work flow","text":"<p>When a client needs to submit an application to YARN, the following steps are taken. </p> <p></p> <ol> <li>Client submits an application</li> <li>Application Manager (RM) allocates a container to start Application Master</li> <li>Application Master registers with Resource Manager</li> <li>Application Master asks containers from Resource Manager</li> <li>Application Master notifies Node Manager to launch containers</li> <li>Application code is executed in the container</li> <li>Client contacts Application Master to monitor application status</li> <li>Application Master unregisters with Resource Manager</li> </ol>"},{"location":"notes/l12_yarn/#yarn-scheduler","title":"YARN Scheduler","text":"<p>Next we consider how YARN schedule jobs. </p> <p>Job scheduling is tough multiple objective optimization problem. It often needs to meet all or most of the following requirement </p> <ul> <li>It needs to offer the capacity guarantee to the cluster users</li> <li>It needs to fulfill the service level agreement, if the cluster is in subscription model</li> <li>It needs to be fairn.</li> <li>It needs to ensure the utilization level of the cluster</li> <li>... </li> </ul> <p>Things get trickier when the applicaiton masters are allowed to request for resource up-front statically or dynamically during the execution. </p> <p>YARN is shipped with a few scheduler templates, namely  * FIFO - Single queue, first in first out * Capacity - optimize for capacity and resources by different queues * FairScheduler - similar to Capacity, but allows applications to move between queues during execution </p> <p>Besides these, users are allowed to define their own scheduler policy for their own clusters.</p> <p>However to ensure faireness in general is a hard problem too. One of the popular approach is to define a standard metric for multi-resource request for all the incoming application so that an order can be fixed. For instance Dominant Resource Fairness. </p> <pre><code>https://cs.stanford.edu/~matei/papers/2011/nsdi_drf.pdf\n</code></pre>"},{"location":"notes/l1_course_handout/","title":"50.043 Database Systems and Big Data  Course Handout","text":""},{"location":"notes/l1_course_handout/#this-page-will-be-updated-regularly-sync-up-often","title":"This page will be updated regularly. Sync up often.","text":""},{"location":"notes/l1_course_handout/#course-description","title":"Course Description","text":"<p>Database systems manage data which is at the heart of modern computing applications. This course covers the fundamentals of traditional databases, such as Oracle and MySQL, and core ideas of recent big data systems.</p> <p>Students will learn important problems in data management that these systems are designed to solve. They will experience the internal design and implementation of relational databases. They will also understand the internals of state\u2010of\u2010the\u2010art big data platforms, namely Apache Spark, and use them on Amazon cloud (Amazon Web Service). The students will be able to determine the advantages and limitations of different database systems.</p>"},{"location":"notes/l1_course_handout/#resource","title":"Resource","text":"<p>The main resources are lecture slides, tutorial sessions, and online documentations. There are no official textbooks. But the following are useful for reference and deeper understanding of some topics.</p> <ol> <li>Abraham Siberschatz, Henry Korth, S Sudarshan. Database System Concepts. 6<sup>th</sup> edition. (DSC)</li> <li>Raghu Ramakrishnan, Johannes Gehrke. Database management systems. 3<sup>rd</sup> edition (DBM)</li> <li>Hector Garcia-Molina, Jeffrey D. Ullman, Jennifer Widom. Database systems, the complete book. 2<sup>nd</sup> edition. (DS)</li> </ol>"},{"location":"notes/l1_course_handout/#instructors","title":"Instructors","text":"<ul> <li> <p>Dorien Herremans (dorien_herremans@sutd.edu.sg) Office Hour:</p> </li> <li> <p>Kenny Lu (kenny_lu@sutd.edu.sg) Office Hour: Monday 3:00-4:30pm (please send email to arrange)</p> </li> </ul>"},{"location":"notes/l1_course_handout/#tas","title":"TAs","text":"<ul> <li> <p>Ziyan GUO (ziyan_guo@mymail.sutd.edu.sg) Office Hour: Thursday 3:00pm-4:30pm</p> </li> <li> <p>Tao YANG (tao_yang@mymail.sutd.edu.sg) Office Hour: Wednesday 10:30am-12:00nn</p> </li> </ul>"},{"location":"notes/l1_course_handout/#communication","title":"Communication","text":"<p>If you have course/assignment/project related questions, please post it on the dedicated MS teams channel.</p>"},{"location":"notes/l1_course_handout/#grading","title":"Grading","text":"<p>Your final grade is computed as follows:</p> <ol> <li> <p>Homework: 12% There will be 2 homework assignments, 6 points each.</p> </li> <li> <p>Project: 60% Group project, up to 3 per group. Unless notifying the instructors otherwise, all group members have the same grade for the project.</p> </li> <li> <p>Class participation: 3% Ask/answer questions during classes, spot mistakes, etc.</p> </li> <li> <p>Final: 25%</p> </li> </ol>"},{"location":"notes/l1_course_handout/#things-you-need-to-prepare","title":"Things you need to prepare","text":"<ul> <li>If you are using Windows 10 or Windows 11, please install ubuntu subsystems <ul> <li>Win10</li> <li>Win11</li> </ul> </li> <li>If you are using Linux, it should be perfect.</li> <li>If you are using Mac, please install homebrew.</li> <li>Make sure Java &gt;8 is installed and ant is installed. </li> <li>Ubuntu: <code>sudo apt install ant ant-contrib</code></li> <li>Mac: <code>brew install ant ant-contrib</code></li> <li>When you have the AWS educate invitaiton email. Self-study Cohort Class 0.</li> </ul>"},{"location":"notes/l1_course_handout/#project","title":"Project","text":"<p>Please refer to the project page.</p>"},{"location":"notes/l1_course_handout/#submission-policy-and-plagiarism","title":"Submission Policy and Plagiarism","text":"<ol> <li>You will do the assignment/project on your own (own teams) and will not copy paste solutions from someone else.</li> <li>You will not post any solutions related to this course to a private/public repository that is accessible by the public/others.</li> <li>Students are allowed to have a private repository for their assignment which no one can access. </li> <li>For projects, students can only invite their partners as collaborators to a private repository.</li> <li>Failing to follow the Code of Honour will result in failing the course and/or being submitted to the University Disciplinary Committee. The consequences apply to both the person who shares their work and the person who copies the work.</li> </ol>"},{"location":"notes/l1_course_handout/#schedule-26-jan-2025-3-may-2025","title":"Schedule (26 Jan 2025 - 3 May 2025)","text":"Week Lecture Cohort Reference Remarks 1 (26/1) Intro, ER Model AWS academy (AWS edu) setup DBM: Chapter 1-2,  DSC: Chapter 7 Cohort Classes are cancelled, please work on the AWS academy setup. 2 (2/2) Relational Model, Relational Algebra ER Model DBM: Chapter 3-4,  DSC: Chapter 2 &amp; 6 3 (9/2) SQL, NoSQL Relational Model, Relational Algebra DBM: Chapter 4-5, DSC: Chapter 2-4 Project Team Submission (11/2 23:59) 4 (16/2) Functional Dependency, Normal Forms SQL DBM: Chapter 19, DSC: Chapter 8 5 (23/2) Storage, Index Functional Dependency, Normal Forms DBM: Chapter 19,  DSC: Chapter 8 Assignment 1 Submission (28/2 23:59) 6 (\u2154) Query Operations Strorage, Index DBM: Chapter 12-14, DSC: Chapter 12 Project Lab 1 Submission (7/3 23:59) 7 (9/3) Recess Week Recess Week Self-study flintrock and spark cluster setup (edimension video tutorial) 8 (16/3) Query Optimization Query Operations DBM: Chapter 15 ,  DSC: Chapter 13 Project Lab 2 Submission (21/3 23:59) 9 (23/3) Transaction Recovery and Concurrency Query Optimization DBM: Chapter 16-18,  DSC: Chapter 14-16 10 (30/3) HDFS Transactions 31/3 Monday is a public holiday, lecture is cancelled. 11 (6/4) MapReduce HDFS, MapReduce Project Lab 3 Submission (11/4 23:59) 12 (13/4) Spark Spark Assignment 2 Submission (18/4 23:59) 13 (20/4) Yarn, **GuestLecture ** Spark 2 Project Lab 4 Submission (25/4 23:59) 14 (27/4)"},{"location":"notes/l1_course_handout/#make-up-and-alternative-assessment","title":"Make Up and Alternative Assessment","text":"<p>Make ups for Final exam will be administered when there is an official Leave of Absence from OSA. There will be only one make up. There will be no make-up if students miss the make up test. </p>"},{"location":"notes/l1_er/","title":"50.043 Entity Relationship Model","text":""},{"location":"notes/l1_er/#learning-outcomes","title":"Learning Outcomes","text":"<p>By this end of this unit, you should be able to </p> <ol> <li>Identify components of an Entity Relationship Diagram</li> <li>Interpret the design requirements given an ER diagram</li> <li>Draw ER diagrams based on user requirements</li> </ol>"},{"location":"notes/l1_er/#components-of-an-er-diagram","title":"Components of an ER Diagram","text":"<p>In the most common scenario, an Entity Relationship Model is described in a form a diagram, AKA ER Diagram. </p> <p>An ER Diagram may consists of some of the following</p> <ol> <li>Entity set. An entity set captures a set of objects or items to be stored. It is represented as a rectangular box with the entity name inside. </li> <li>Attribute. An attribute describe a property of an entity. An attribute is represented as an oval shape with the attribute name inside. Attribute serves as (part of) the primary key of the entity will be underlined.</li> <li>Relationship. A relationship defines the relationship between entities. It is represented as a diamond shape with the relationship name inside. Relationships are often annotated with cardinality constraints. We will discuss it shortly.</li> </ol> <p></p> <p>For example in the above ER diagram, we find two entities, i.e. <code>Student</code> and <code>Class</code>. Each <code>Student</code> entity has three attributes, <code>NRIC</code>, <code>Name</code> and <code>DoB</code>, with <code>NRIC</code> as the primary key. Likewise, each <code>Class</code> entity has three attributes, <code>Number</code> (as primary key), <code>Name</code> and <code>Location</code>. </p> <p>There exists a binary relationship between <code>Student</code> and <code>Class</code> entities, <code>Enrolled in</code>. It describes a business constraints that students can be enrolled into classes.  The annotation <code>N</code> describes that each class may have more than 1 students. The annotation <code>M</code> defines that each student may take more than one modules.  Note that in this module, for simplicity, we only require the upper bound to be specified in cardinality constraints, which could be either <code>1</code> or <code>N</code> (or <code>M</code>). Note that <code>N</code> and <code>M</code> are just meta terms to represent \"many\". </p>"},{"location":"notes/l1_er/#self-referencing-relationship","title":"Self-referencing relationship","text":"<p>There is an edge case in which a relationship may be self-referencing w.r.t to an entity. </p> <p></p> <p>In the above ER diagram, we omit the attributes of the Article entity for simplicity. Each article can be referenced by many other articles. Each article can reference many other articles. The extra annotation <code>Citing</code> and <code>Cited</code> label the role of the article involved in the relationship.</p>"},{"location":"notes/l1_er/#tertiary-relationship","title":"Tertiary relationship","text":"<p>There are situations in which a relationship involves more than two entities. </p> <p></p> <p>In the above ER diagram, the <code>publish</code> relationship involves three entities, <code>Article</code>, <code>Book</code> and <code>Publisher</code>. The cardinality constraints should be interpreted by pairing up two source entities and one target entity. In the above example, we have </p> <ol> <li>Given an article and a book, they can only be published by 1 publisher. </li> <li>Given an article and a publisher, the article can only appeared one book (published by that publisher).</li> <li>Given a book and a publisher, the book may contain many different articles.</li> </ol>"},{"location":"notes/l1_intro/","title":"50.043 Introduction to Database System","text":""},{"location":"notes/l1_intro/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this unit, you should be able to</p> <ol> <li>List the problems handled by database management systems\u200b</li> <li>Describe the techniques used in database system to solve these problems\u200b</li> </ol> <p>\u200b</p>"},{"location":"notes/l1_intro/#what-is-a-database","title":"What is a database?","text":"<p>Generally speaking, a database is an organized collection of data. </p> <p>For instance, </p> <ul> <li>your excel sheet that keep tracks of your monthly finance.</li> <li>a text file that stores the list of items you want to buy for christmas or CNY.</li> <li>a collection of student records in an LMS.</li> <li>a collection of product information and stock level for a minimart chain.</li> </ul>"},{"location":"notes/l1_intro/#what-is-a-database-management-system","title":"What is a database management system?","text":"<p>It is a system (hopefully a software system) that manages databases. Many DBMS orchestrates and manages multiple databases simultanenously. For instance, Oracle, MySQL, MS SQL, MongoDB, Firebase, Amazon RDs and etc. </p> <p>It is confusing when people use to the term \"database\" to refer to a DBMS. </p>"},{"location":"notes/l1_intro/#characteristics-of-a-database-management-system","title":"Characteristics of a database management system","text":"<p>For a DBMS to be pragmatically useful, here are some of the characteristics that it may possess</p> <ol> <li>It should be efficient. Queries and data operation should be evaluated and performed in an efficient way.</li> <li>It should be crash consistent. In the event of unexpected crash and system distruption, the data stored should remain consistent.</li> <li>It should support concurrent access. There are many users and softwares that could access the data without interfering each other.</li> <li>It should support data abstraction. Data and their relationships are allowed to be described in a logical manner without the users / software to be exposed with the detail implementation.</li> </ol>"},{"location":"notes/l1_intro/#common-techniques-available-in-database-management-systems","title":"Common Techniques available in database management systems","text":"<ol> <li>Storage and Index. DBMSes have their own internal storage system. It often differs from those available in the operating system, due to use case difference. Internal storage system enables DBMSes to organize and retrieve data in a systematic and efficient way. Indices are inspired from data structure and algorithms. With indices, data queries and operation can be further optimized. </li> <li>Transaction. Many DBMSes support transactional operations. Trasaction groups multiple operations to be performed as an atomic sequence of actions to be performed. Conflicts caused by concurrent access can be resolved by studying and manipulating multiple transactions. Transactions are logged, so that in the event of system crashes, partial data changes can be reverted back to the nearest consistent state.</li> <li>Data Model and SQL. Many tranditional DBMSes support data model and SQL. This combination is a simple and expressive data abstraction for many applications. Softwares developed based on an existing database (system) could be possible migrate to another with marginal technical effort. </li> </ol> <p>Recently many big data systems extend their support to these techniques and it proves the essentiality of them.</p>"},{"location":"notes/l1_intro/#overview-of-data-modelling","title":"Overview of Data Modelling","text":"<p>For the first part of this module, we dwell into the details of data modelling. </p> <p></p> <p>In the upper diagram, we describe the user interaction with the database as blackbox. </p> <p>In the lower diagram, we define the processes of data modelling. </p> <ol> <li> <p>Conceptual Model - in this step, we try to capture the user needs from user study, and identify what data the application has and need.</p> </li> <li> <p>Logical Model - in this step, supported by the user needs identified, we study how data are been accessed, updated and maintained from the business perspective. We design a high level structure of the data storage and the set of possible constraints over the data to enforce varies integrity derived from the business logics. </p> </li> <li> <p>Physical Model - in this step, we translate the logical model into a specific database (supported by the database management system that has been deployed).</p> </li> </ol>"},{"location":"notes/l2_relational_algebra/","title":"50.043 -  Relational Algebra","text":""},{"location":"notes/l2_relational_algebra/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this unit, you should be able to </p> <ol> <li>Interpret relational algebra terms (queries)</li> <li>Define relational algebra terms to query a relational model</li> </ol>"},{"location":"notes/l2_relational_algebra/#relational-algebra","title":"Relational Algebra","text":"<p>Given that relational model is a logical model (abstracting away the implementation details), we need something operations defined on the same layout to manipulate the data in a relational model. (So that we don't need to deal with the implementation details yet.)</p> <p>Like math algebra, Relational Algebra is a way to express data operation through symbols and relation manipulations. </p> <ol> <li>The data manipulation operations are defined in terms of a sequence of operation applications.</li> <li>Each symbolic operator takes one or more relation(s) (or intermediate relations) as operands and produces one result relation.</li> </ol> <p>For example, given a relation of  Publish(article_id, book_id, publisher_id)</p> <p>The instance of a Publish relation is given</p> article_id book_id publisher_id a1 b1 p1 a2 b1 p1 a1 b2 p2 <p>The query \"finding all articles that are published by both publishers p1 and p2\" can be expressed as the following relational algebra term.</p> \\[ \\Pi_{article\\_id}(\\sigma_{(publisher\\_id='p1')}(Publish)) \\cap \\Pi_{article\\_id}(\\sigma_{(publisher\\_id='p2')}(Publish)) \\]"},{"location":"notes/l2_relational_algebra/#selection","title":"Selection","text":"<p>A selection operator, \\(\\sigma_{P}(R)\\), takes a logical predicate \\(P\\) and a relation \\(R\\) and returns all the tuple in \\(R\\) that satisfy \\(P\\). </p> <p>Note that \\(P\\) can be</p> <ol> <li>a simple predicate such as a equality test $name = \"tom\" or </li> <li>a conjunction or disjunction of predicates, e.g. \\(name = \"tom\"\\ {\\tt AND} \\ age &gt; 21\\).</li> </ol> <p>For example, the following relational algebra expression returns a relation with all tuples from \\(Publish\\) with \\(publisher\\_id\\) equals to <code>p1</code>.</p> \\[\\sigma_{(publisher\\_id='p1')}(Publish)\\]"},{"location":"notes/l2_relational_algebra/#projection","title":"Projection","text":"<p>A projection operator \\(\\Pi_{A_1,A_2,...}(R)\\), takes a set of attribute names \\(A_1,...,A_n\\) and a relation \\(R\\), and returns a relation that contains the data of columns \\(A_1,...,A_n\\) in \\(R\\).</p> <p>For example, the following expression returns a relation of all \\(article\\_id\\)s from \\(Publish\\) with \\(publisher\\_id\\) equals to <code>p1</code>.</p> \\[\\Pi_{article\\_id}(\\sigma_{(publisher\\_id='p1')}(Publish))\\]"},{"location":"notes/l2_relational_algebra/#intersection","title":"Intersection","text":"<p>An intersection operation  \\(R \\cap S\\) finds all common tuples from \\(R\\) and \\(S\\), assuming \\(R\\) and \\(S\\) sharing a common schema.</p> <p>We have seen an example of this earlier.</p>"},{"location":"notes/l2_relational_algebra/#union","title":"Union","text":"<p>A union operation \\(R \\cup S\\) returns a relation containing all tuples from \\(R\\) and all tuples from \\(S\\), assuming \\(R\\) and \\(S\\) sharing a common schema.</p>"},{"location":"notes/l2_relational_algebra/#difference","title":"Difference","text":"<p>A difference operation \\(R - S\\) returns a relation containing all tuples from \\(R\\) that are not in \\(S\\), assuming \\(R\\) and \\(S\\) sharing a common schema.</p>"},{"location":"notes/l2_relational_algebra/#cartesian-product","title":"Cartesian Product","text":"<p>A cartesian product operation \\(R \\times S\\) returns a relation containing all possible combination of some tuple from \\(R\\) and some tuple from \\(S\\). (Note that \\(R\\) and \\(S\\) might have different schema.)</p> <p>For example, consider </p> <p>\\(R(A,B)\\) and \\(S(C,D)\\)</p> A B a1 101 a2 102 C D a3 103 a4 104 <p>\\(R\\times S\\) yields</p> R.A R.B S.C S.D a1 101 a3 103 a1 101 a4 104 a2 102 a3 103 a2 102 a4 104 <p>Cartesian Product is one of the four possible join operators.</p> <p>Let's discuss the other three.</p>"},{"location":"notes/l2_relational_algebra/#inner-join","title":"Inner Join","text":"<p>The inner join operator \\((R \\bowtie_{R.A = S.B, R.C=S.D,...} S)\\), returns a relation that containing tuples from \\(R\\times S\\) that satisfy \\(R.A = S.B, R.C=S.D,...\\).</p> <p>Let \\(R(A,B,C)\\) and \\(S(D,E,F)\\) be relations.</p> A B C a1 101 0 a2 102 1 a3 103 0 D E F a3 103 'a' a1 107 'b' a5 105 'c' <p>\\(R \\bowtie_{R.A =S.D} S\\) produces</p> R.A R.B R.C S.D S.E S.F a1 101 0 a1 107 'b' a3 103 0 a3 103 'a'"},{"location":"notes/l2_relational_algebra/#natural-join","title":"Natural Join","text":"<p>The natural join operator \\((R \\bowtie S)\\), returns a relation that containing tuples from \\(R\\times S\\) that satisfy \\(R.A = S.A, R.B=S.B,...\\), where \\(A\\), \\(B\\), ... are the common attributes between \\(R\\) and \\(S\\).</p> <p>Note that the common column are merged after natural join.</p> <p>Let \\(R(A,B,C)\\) and \\(S(D,E,F)\\) be relations.</p> A B C a1 101 0 a2 102 1 a3 103 0 A E F a3 103 'a' a1 107 'b' a5 105 'c' <p>\\(R \\bowtie S\\) produces</p> R.A R.B R.C S.E S.F a1 101 0 107 'b' a3 103 0 103 'a'"},{"location":"notes/l2_relational_algebra/#right-outer-join","title":"Right outer join","text":"<p>Right outer join \\(R \u27d6_{R.A=S.B} S\\) produces a relation containing all entries from the inner join and all the remaining tupel from \\(S\\) which we could not find a match from \\(R\\), whose attributes will be filled up with NULL.</p> A B C a1 101 0 a2 102 1 a3 103 0 D E F a3 103 'a' a1 107 'b' a5 105 'c' <p>\\(R \u27d6_{R.A =S.D} S\\) produces</p> R.A R.B R.C S.D S.E S.F a1 101 0 a1 107 'b' a3 103 0 a3 103 'a' NULL NULL NULL a5 105 'c' <p>Likewise for left outer join.</p>"},{"location":"notes/l2_relational_algebra/#renaming","title":"Renaming","text":"<p>Renaming operation \\(\\rho_{R'(A_1,...,A_n)}(R)\\) produces a new relation \\(R'(A_1,...,A_n)\\) containing all tuples from \\(R\\), assuming the \\(S_R\\) and \\(S_{R'}\\) share the same types.</p> <p>We omit the attribute name \\(A_1,...,A_n\\) when we only want to rename the relation name. Likewise we omit the relation name if we only want to rename the attributes.</p>"},{"location":"notes/l2_relational_algebra/#aggregation","title":"Aggregation","text":"<p>Aggregation operation <sub>\\(A\\)<sub>1</sub>,..., \\(A\\)<sub>n</sub></sub> \\(\\gamma_{F_1(B_1),F_2(B_2),...} (R))\\) produces a new relation \\(R'\\) which contains</p> <ul> <li>\\(A_1,...,A_n\\) as the attribute names to group by</li> <li>\\(F_1(B_1),...,F_m(B_m)\\) as the aggregated values where<ul> <li>\\(F_1, ..., F_m\\) are aggregation functions such as <code>SUM()</code>, <code>AVG()</code>, <code>MIN()</code>, <code>MAX()</code>, <code>COUNT()</code>.</li> <li>\\(A_1, ..., A_n\\), \\(B_1, ..., B_m\\) are attributes from \\(R\\).</li> </ul> </li> </ul> <p>For example, given \\(R(A,B,C)\\)</p> A B C a1 101 0 a2 102 1 a3 103 0 <p>\\(\\rho_{(C,CNT)}(\\)<sub>\\(C\\)</sub>\\(\\gamma_{{\\tt COUNT}(B)}(R))\\) produces</p> C CNT 0 2 1 1 <p>Sometimes we can rewrite the above expression as </p> \\[_{C} \\gamma_{{\\tt COUNT}(B)\\ {\\tt as}\\ CNT}(R)\\] <p>without using the renaming operator.</p>"},{"location":"notes/l2_relational_algebra/#alternatives","title":"Alternatives","text":"<p>Alternative to relational algebra, relational calculus is designed to serve a similiar idea. The difference between them is that relational algebra is more procedural (like C, Java and Python) where relational calculus is more declarative (like CSS and SQL). If you are interested to find out more please refer to the text books. Note that relational calculus will not be assessed in this module.</p>"},{"location":"notes/l2_relational_model/","title":"50.043 - Relational Model","text":""},{"location":"notes/l2_relational_model/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this unit, you should be able to </p> <ol> <li>Describe the components of a relational model</li> <li>Translate an ER model into a relational model</li> </ol>"},{"location":"notes/l2_relational_model/#relational-model","title":"Relational Model","text":"<p>Recall the following diagram</p> <p></p> <p>Relational Model is an instance of Logical Modelling. Unlike Conceptual Modelling, Logical Modelling focuses on how data are being stored and processed in the abstraction level. The implementation details can be decided at a latter stage. It is analogous to software development, we could decide the data structure and algorithms without thinking about how the code should be written. </p>"},{"location":"notes/l2_relational_model/#what-are-the-other-options","title":"What are the other options?","text":"<p>Relational Model is not the only logical model available. Alternatively we find graph model and key-value model are the other popular options. </p>"},{"location":"notes/l2_relational_model/#gist-of-relational-model","title":"Gist of Relational Model","text":"<p>Relational Model defines data in <code>Relation</code>. </p> <ol> <li>Each <code>Relation</code> is an unordered set containing relationship of attributes. Attributes in Relational Model are sometimes referred to as fields.</li> <li>Note that we should not confuse them with the relationship and attributes mentioned in ER model. <ul> <li>Relational model is a logical model, describing how data are stored and queried. ER model is a conceptual model, describing what data the application have / need.</li> <li><code>Relation</code> in Relational model is mathematical relation, while the relationship in ER model is the connection in business domain. </li> </ul> </li> <li><code>Relation</code> is an set of <code>Tuple</code>s. Each <code>Tuple</code> is a sequence of attribute values in the <code>Relation</code>. </li> </ol> <p>For instance</p> Student Number Name Email DoB 1234 James james@istd 1/1/2000 5678 Vansesa vanessa@epd 2/4/1999 3093 David david@esd 3/7/2000 <p>The above table is an instance of a <code>Relation</code>, of student profile. * The first row in the Schema of the <code>Relation</code>. The second rows onwards are tuples.  * Each column defines an attribute (or a field).</p>"},{"location":"notes/l2_relational_model/#formal-definitions","title":"Formal Definitions","text":""},{"location":"notes/l2_relational_model/#definition-relation","title":"Definition (Relation)","text":"<p>Let \\(D_1\\), ..., \\(D_n\\) be domains (of attribute values). We define a relation \\(R\\) as follows</p> \\[ R \\subseteq D_1 \\times D_2 \\times ... \\times D_n \\]"},{"location":"notes/l2_relational_model/#definition-tuple","title":"Definition (Tuple)","text":"<p>Let \\(R\\) be a relation \\(R \\subseteq D_1 \\times D_2 \\times ... \\times D_n\\). Then a tuple \\(t\\) is an element of \\(R\\) \\(t = (d_1,...,d_n) \\in R\\) where for all \\(i\\in [1,n]\\) we have \\(d_i \\in D_i\\).</p>"},{"location":"notes/l2_relational_model/#definition-schema","title":"Definition (Schema)","text":"<p>Let \\(R\\) be a relation \\(R \\subseteq D_1 \\times D_2 \\times ... \\times D_n\\). Then the schema of \\(R\\), denoted as \\(S_R\\) is a mapping of attribute names to domains (i.e. \\(D_1, ..., D_n\\))</p> <p>For instance, recall the earlier example, the student profile relation \\(R \\subseteq (Int \\times String \\times String \\times Date)\\). where \\(S_R = \\{Student Number : Int, Name : String, Email : String, DoB : Date \\}\\). The tuples in the above example forms an instance of the student profile relation. </p> \\[ I_R =  \\left \\{ \\begin{array}{cccc}      (1234,&amp; James,&amp; james@istd,&amp; 1/1/2000 ), \\\\      (5678,&amp; Vanessa,&amp; vanessa@epd, &amp; 2/4/1999), \\\\      (3093,&amp; David, &amp; david@esd, &amp;3/7/2000)      \\end{array} \\right \\}  \\]"},{"location":"notes/l2_relational_model/#mapping-relational-model-to-tables","title":"Mapping Relational Model to Tables","text":"<p>There are many ways of translating a relational model into a set of database tables (from logical model to physical model).</p> <p>One simplest way is to </p> <ol> <li>translate each relation into a table</li> <li>translate each domain into a table column, having the domain constraint being satisfied by the table column type.</li> <li>translate tuples of the relation instance into a row of a table.</li> <li>relational schema can be converted into table schema (directly derived from step 2).</li> <li>translate additional constraints into table constraints or foreign key constraints.</li> </ol>"},{"location":"notes/l2_relational_model/#er-to-relational-translation","title":"ER to Relational translation","text":"<p>Converting an ER model (conceptual) to a relation model (logical) is more involved.  </p> <p>The translation is iteratively defined by the following rules.</p> <ul> <li>Rule 1: map an entity set to a relation and preserve its primary key and fields.</li> <li>Rule 2: translate a relationship to a relation by <ul> <li>combining all keys from entity sets to make the new primary key for the result relation.</li> <li>the final composite primary key is determined by the cardinality constraints of the relationship.</li> </ul> </li> <li>Rule 3: merge multiple relations with the same key into a single relation<ul> <li>Pros: avoids data redundancy</li> <li>Cons: introduces NULL values</li> </ul> </li> </ul> <p>Let's consider an example, recall the following ER diagram</p> <p></p> <p>Let's assume each entity has a primary key id, and a non-primary-key attribute name. All fields are of type String.</p> <p>We apply the first rule to obtain the first 3 relations in the following,</p> <ol> <li>Article(id, name) </li> <li>Book(id,name)</li> <li>Publisher(id, name)</li> <li>Publish(article_id, book_id, publisher_id)</li> </ol> <p>The 4<sup>th</sup> relation is obtained by applying the second rule to translation the <code>publish</code> relationship.</p> <p>Note that by default we take all three ids to form a composite primary key. </p> <p>The cardinality constraint suggests that</p> <ul> <li>an article_id and a book_id can determine a publisher_id </li> <li>an article_id and a publisher_id can determine a book_id </li> </ul> <p>Hence we can reduce the primary key composition of 4. to be </p> <ul> <li>Publish(article_id, book_id, publisher_id); or</li> <li>Publish(article_id, book_id, publisher_id)</li> </ul> <p>It is a common mistake to think of fixing article_id as a primary key would suffice. Note that the above ER diagram provides no cardinality information between Book and Publisher alone. And the following instance is a counter example to the argument</p> article_id book_id publisher_id a1 b1 p1 a2 b1 p1 a1 b2 p2 <p>The Rule 3 is not applicable in this example, hence we are done.</p>"},{"location":"notes/l3_nosql/","title":"50.043 - NoSQL","text":""},{"location":"notes/l3_nosql/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this unit, you should be able to</p> <ol> <li>articulate the trade-off made between relational model and other data model.</li> <li>contrast the different use-cases for SQL and NoSQL.</li> <li>List the pros and cons for different NoSQL-driven data storage.</li> </ol>"},{"location":"notes/l3_nosql/#motivation","title":"Motivation","text":"<p>NoSQL existed long ago. It recently received more attention due to the shift of infrastruture from on-premise to cloud, from single server to distributed system.</p> <p>Recall from week 1 lesson, we learnt that database system should be efficient, crash consitent, supporting concurrent access, offering data abstraction. </p> <p>As we scale out to multiple machines, we have to give up some of the above. It is technically challenging to have a distributed system that is consistent.  Another less alarming issue is that it is hard to perform efficient data join across machines (though there has been progress made). </p> <p>Due to various reasons, developers turn to the older alternative, key-value data structure. </p>"},{"location":"notes/l3_nosql/#key-value-store","title":"Key-value store","text":"<p>Key-value store is simple. Many of us have seen the in-memory version, e.g. dictionary of Python, HashMap of Java, etc. The idea of key-value storage is to put data associated with a key in a look-up table. Keys must be unique, otherwise existing data will be overwritten. </p> <p>Storing data in key-value pairs offers some obvious advantages 1. easy to retrieve, like part of your code 2. fit in many applications, mobile app, web app.  3. fast, since it is just a lookup and/or set. 4. easy to scale, since we could split the data by key-space. </p> <p>However key-value pair storage has several limitations. 1. limited API, we can only get or set. There is no range query. 2. Join query is out of the question. 3. it does not differetiate the data type you store by default everything is binary. 4. it does not support inner structure in your data. 5. there is no immediate consistency gauranteed, i.e. sometimes your change takes a while to be reflected on the web app.</p> <p>Due to the above reasons, it pushes the processing load to the application-tier (which is your source code, in Python, Java or C etc.)</p>"},{"location":"notes/l3_nosql/#document-store","title":"Document store","text":""},{"location":"notes/l3_nosql/#unstructured-document","title":"Unstructured Document","text":"<p>In this category, the data store saves data in an unstructured manner. Treating everything as text or sequence of bytes.</p> <p>The advantage is it saves every thing, like your file system. The disadantage is that it offers no processing API.</p>"},{"location":"notes/l3_nosql/#semi-structured-document","title":"Semi-structured Document","text":"<p>It is a tree-like structure. Data are stored like JSON/XML document. Some implementation offers limited support of indices and range search, e.g. MongoDB.</p> <p>The advantage is that semi-structured data are close to their in-memory counter-parts living in the software application, (binary tree, graph, JSON). It offers flexibility of going back and fro between database and app without the need of joining and object-relational mapping. </p> <p>Most of the semi-structured data are self-explanatory, respecting data types and data structure. </p> <p>One obvious disadvantage is that semi-strutured data do not eliminate data redundancy. Immediate consistency is handled loosely. Range queries could be harder to implememented.</p>"},{"location":"notes/l3_sql/","title":"50.043 - SQL","text":""},{"location":"notes/l3_sql/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this unit you should be able to use SQL to</p> <ol> <li>create and alter databases and tables</li> <li>create and alter table constraints</li> <li>inject data into tables</li> <li>retrieve data from tables</li> <li>update/delete data from tables</li> </ol>"},{"location":"notes/l3_sql/#sql","title":"SQL","text":"<p>SQL is a high-level language for data definition and data manipulation. By high-level, we mean it is  * expressive * closer to the programmers</p> <p>SQL is a declarative programming language. By declarative, we use SQL to specify what to do but not how to do. (The \"how-to\" parts are left to the underlying runtime to decide, i.e. the DBMS query operation module).</p> <p>SQL is almost universal and cross platforms. Modern big data and non-relation databases extend to support SQL.</p> <p>Note that different DBMSes have different subset of SQL statements. In this unit, we try to cover the common ones.</p>"},{"location":"notes/l3_sql/#data-definition-language","title":"Data definition language","text":"<p>Let's consider the DDL of SQL. </p>"},{"location":"notes/l3_sql/#create-and-drop-database","title":"Create and drop database","text":"<p>Note that in a DBMS, there may be many different databases, identified by their names. </p> <p>To create a database, we may use the following SQL statement.</p> <pre><code>create database if not exists db_name;\n</code></pre> <p>where <code>db_name</code> is the name of the database; <code>if not exists</code> means the create statement only applies when there is no existing database in the DBMS with name <code>db_name</code>. </p> <p>Note that SQL is case-insensitive. </p> <p>To drop a database</p> <pre><code>drop database if exists db_name;\n</code></pre> <p>To rename a database</p> <pre><code>alter database db_name rename to my_db; \n</code></pre> <p>However in some DBMS, e.g. MySQL, the above statement is rejected. Instead, we need to dump the old database and load it into the new database. </p>"},{"location":"notes/l3_sql/#create-table","title":"Create table","text":"<pre><code>create table if not exists my_db.article (\n    id char(100) primary key,\n    name char(100)\n);\n</code></pre> <p>In the above statement, we create the <code>article</code> table in the database <code>my_db</code>. <code>article</code> has two attributes, <code>id</code> and <code>name</code>. Both attributes are of type <code>char(100)</code>, i.e. character sequence with max length = 100. <code>id</code> is set to be primary key of the table. </p> <p>Similarly, we create the <code>book</code>, <code>publisher</code> and <code>publish</code> tables.</p> <pre><code>create table if not exists  my_db.book (\n    id char(100) primary key,\n    name char(100)\n);\n\ncreate table if not exists my_db.publisher (\n    id char(100) primary key,\n    name char(100)\n);\n</code></pre> <p>Finally, we create the table <code>publish</code> which was translated from a tertiary relationship.</p> <p><pre><code>create table if not exists my_db.publish (\n    article_id char(100),\n    book_id char(100),\n    publisher_id char(100),\n    primary key (article_id, book_id),\n    foreign key (article_id) references my_db.article(id),\n    foreign key (book_id) references my_db.book(id),\n    foreign key (publisher_id) references my_db.publisher(id)\n);\n</code></pre> Since the primary key is a composite key, it is specified in a separate clause (line 5). In addition, we have to create three foreign key constraints on <code>article_id</code>, <code>book_id</code> and <code>publisher_id</code> to ensure their existence in the entity table <code>article</code>, <code>book</code> and <code>publisher</code>. </p>"},{"location":"notes/l3_sql/#alter-table","title":"Alter table","text":"<p>We can alter a table (e.g. name, attribute name, attribute type, constraints) using the alter statement. For instance, </p> <pre><code>-- drop primary key\nalter table my_db.publish drop primary key;\n-- recreate primary key\nalter table my_db.publish add primary key (article_id, publisher_id);\n</code></pre> <p>The first alter statement drop the primary key, and the second one recreate a new primary using <code>(article_id, publisher_id)</code> composition. </p>"},{"location":"notes/l3_sql/#some-dbms-implementation-specific-details","title":"Some DBMS Implementation Specific Details","text":"<p>Note that in some DBMS implementation, we need to drop the foreign key constraints before we can drop the primary key. For instance MySQL, the foreign key constraint automatically create an index on the attribute, in this case <code>article_id</code> and <code>book_id</code>. It uses the existing index from the existing primary key constraint. </p> <p>In case of MySQL, we need to first find out the name of the foreign key constraints </p> <pre><code>select table_schema, table_name, column_name, constraint_name  from information_schema.key_column_usage where table_name = 'publish';\n+--------------+------------+--------------+-----------------+\n| TABLE_SCHEMA | TABLE_NAME | COLUMN_NAME  | CONSTRAINT_NAME |\n+--------------+------------+--------------+-----------------+\n| my_db        | publish    | article_id   | PRIMARY         |\n| my_db        | publish    | book_id      | PRIMARY         |\n| my_db        | publish    | article_id   | publish_ibfk_1  |\n| my_db        | publish    | book_id      | publish_ibfk_2  |\n| my_db        | publish    | publisher_id | publish_ibfk_3  |\n+--------------+------------+--------------+-----------------+\n5 rows in set (0.00 sec)\n</code></pre> <p>Then execute the following statements before  the <code>drop primary key</code> statement.</p> <pre><code>alter table my_db.publish drop foreign key publish_ibfk_1;\nalter table my_db.publish drop foreign key publish_ibfk_2;\n</code></pre> <p>Then execute the following statements after the <code>add primary key</code> statement</p> <pre><code>alter table my_db.publish add foreign key (article_id) references my_db.article(id) ;\nalter table my_db.publish add foreign key (book_id) references my_db.book(id);\n</code></pre> <p>For more detals of alter table statement for MySQL, refer to the MySQL documentation.</p>"},{"location":"notes/l3_sql/#drop-table","title":"Drop table","text":"<p>To drop a table, we could run</p> <pre><code>drop table my_db.publish; \n-- but don't do it, it is irreversible, instead we should probably to rename it to something else.\n</code></pre>"},{"location":"notes/l3_sql/#injecting-value","title":"Injecting value","text":"<p>To inject values, we use the insert statement.</p> <pre><code>insert into my_db.article (id, name) values \n('a1', 'article 1'),\n('a2', 'article 2'); \n\ninsert into my_db.book (id, name) values \n('b1', 'book 1'),\n('b2', 'book 2');\n\ninsert into my_db.publisher (id, name) values\n('p1', 'publisher 1'),\n('p2', 'publisher 2');\n</code></pre> <p>Note that we can omit the schema <code>(id, name)</code> when values for all columsn are present. Furthermore, when inserting values into a table with foreign key constraint, e.g. <code>publish</code> references <code>article</code>, <code>book</code> and <code>publisher</code>, the values to be inserted must respect the existence of the referenced keys from the referenced tables.</p> <pre><code>insert into my_db.publish (article_id, book_id, publisher_id) values\n('a1', 'b1', 'p1'),\n('a2', 'b1', 'p1'),\n('a1', 'b2', 'p2');\n</code></pre>"},{"location":"notes/l3_sql/#mass-importing-and-exporting","title":"Mass importing and exporting","text":"<p>In some situation, it is inefficient to inject values one by one via insert statement. Many DBMS implementations offer means to import and export data from text file or other format. For MySQL, please refer to this document and this document.</p>"},{"location":"notes/l3_sql/#querying-table","title":"Querying table","text":"<p>To retrieve data stored in a table, we use the select statement.</p> <p><pre><code>select article_id, book_id, publisher_id  from my_db.publish;\n</code></pre> In the above statement we retrieve all records (tuples) from the <code>publish</code> table. In this case, since we are retrieving all columns, we could re-write the above as follows,</p> <pre><code>select * from my_db.publish;\n</code></pre>"},{"location":"notes/l3_sql/#export-to-csv","title":"Export to CSV","text":"<p>In some implemntation, such as MySQL, we could use the select statement to export the data in a table into a CSV file. </p> <pre><code>select * from my_db.publish into outfile '/tmp/publish.csv' fields terminated by ','; \n</code></pre>"},{"location":"notes/l3_sql/#join-query","title":"Join-Query","text":"<p>When querying multiple table, we would use the inner join.</p> <pre><code>select * from my_db.publish \ninner join my_db.article \non my_db.publish.article_id = my_db.article.id;\n</code></pre> <p>For breivity, we could give aliases to the tables being joined.</p> <p><pre><code>select * from my_db.publish p \ninner join my_db.article a \non p.article_id = a.id;\n</code></pre> The above queries produce</p> <pre><code>+------------+---------+--------------+----+-----------+\n| article_id | book_id | publisher_id | id | name      |\n+------------+---------+--------------+----+-----------+\n| a1         | b1      | p1           | a1 | article 1 |\n| a1         | b2      | p2           | a1 | article 1 |\n| a2         | b1      | p1           | a2 | article 2 |\n+------------+---------+--------------+----+-----------+\n</code></pre> <p>The left- and right- outter join queries can be expressed in a similar way by replacing <code>inner</code> by <code>left</code> or <code>right</code>.</p>"},{"location":"notes/l3_sql/#where-clause","title":"Where clause","text":"<p>Suppose we would like to find all the article names that are published by publisher <code>p1</code>.</p> <p><pre><code>select a.name from my_db.publish p \ninner join my_db.article a \non p.article_id = a.id\nwhere p.publisher_id = 'p1';\n</code></pre> which produces</p> <pre><code>+-----------+\n| name      |\n+-----------+\n| article 1 |\n| article 2 |\n+-----------+\n</code></pre> <p>Note that instead of <code>inner join</code>, we can rewrite the above query using <code>equi-join</code> which is pushing the id matching to the filtering operation.</p> <pre><code>select a.name from my_db.publish p, my_db.article a \nwhere p.article_id = a.id\nand p.publisher_id = 'p1';\n</code></pre> <p>This is because the following equation holds in relational algebra</p> \\[ publish \\bowtie_{publish.article_id = article.id} article = \\sigma_{publish.article_id = article.id}(publish \\times article) \\]"},{"location":"notes/l3_sql/#self-join","title":"Self join","text":"<p>Suppose we want to find all articles that are published by both publisher <code>p1</code> and <code>p2</code>.</p> <p>The following query </p> <p><pre><code>select a.* from my_db.publish p, my_db.article a\nwhere p.article_id = a.id\nand p.publisher_id = 'p1'\nand p.publisher_id = 'p2';\n</code></pre> won't produce any result. This is due to the fac that the entire conjunction predicate in the <code>where</code> clause is applied to per tuple level. Since there is no tuple having <code>publisher_id</code> as <code>p1</code> and <code>p2</code> at the same time, the result is an empty set.</p> <p>In such situation, we need to join a table to itself. </p> <p><pre><code>select a.* from my_db.publish p1, my_db.publish p2, my_db.article a\nwhere p1.article_id = a.id\nand p2.article_id = a.id\nand p1.publisher_id = 'p1'\nand p2.publisher_id = 'p2';\n</code></pre> In the above, we \"clone\" the <code>publish</code> table twice, then the join are performed among the two clones and the article table.</p>"},{"location":"notes/l3_sql/#nested-query","title":"Nested query","text":"<p>Alternatively to self-join, we could express the above query using nested query.</p> <pre><code>select a1.* from my_db.publish p1, my_db.article a1\nwhere p1.article_id = a1.id\nand p1.publisher_id = 'p1'\nand a1.id in (select a2.id from my_db.publish p2, my_db.article a2\n              where p2.article_id = a2.id\n              and p2.publisher_id = 'p2' \n             );\n</code></pre> <p>In the above, we find a nested query, the outer query joins <code>publish</code> with <code>article</code> and filters out those tuples with <code>publisher_id</code> as <code>p1</code>. The last predicate checks the article id must be found in the result of the nested query. The nested query joins the clones of the two tables and filter tuples with <code>publisher_id</code> equal to <code>p2</code>. </p>"},{"location":"notes/l3_sql/#aggregation","title":"Aggregation","text":"<p>For analytic purpose, we need to aggregate values by group.</p> <p>For example, the following statement counts the number of tuples in the <code>publish</code> table.</p> <pre><code>select count(*) from my_db.publish;\n</code></pre> <p>Suppose we would like to counts the number of published article published by publisher <code>p1</code>.</p> <p><pre><code>select publisher_id, count(*) from my_db.publish \ngroup by publisher_id;\n</code></pre> <pre><code>+--------------+----------+\n| publisher_id | count(*) |\n+--------------+----------+\n| p1           |        2 |\n| p2           |        1 |\n+--------------+----------+\n</code></pre></p> <p>In the above the <code>group by</code> clause specifies the attribute <code>publisher_id</code> is the attribute the groups created by, i.e. tuples within each group should have the same <code>publisher_id</code>. </p> <p>The above SQL statement is equivalent to the following relational algebra express</p> \\[ _{publisher\\_id}\\gamma_{count(*)}(publish) \\]"},{"location":"notes/l3_sql/#sorting","title":"Sorting","text":"<p>Suppose we want to sort the result of the last query by the counts in ascending order.</p> <pre><code>select publisher_id, count(*) as cnt from my_db.publish \ngroup by publisher_id\norder by cnt asc;\n</code></pre> <p>In the above the <code>as cnt</code> creates an alias for the column <code>count(*)</code> for the ease of references. The <code>order by</code> clause specifies the order of the returned results.</p> <pre><code>+--------------+-----+\n| publisher_id | cnt |\n+--------------+-----+\n| p2           |   1 |\n| p1           |   2 |\n+--------------+-----+\n</code></pre>"},{"location":"notes/l3_sql/#update-and-delete","title":"Update and delete","text":"<p>To update  tuples/records in table, we use the update statement.</p> <p><pre><code>update my_db.publisher set name = 'publisher one' \nwhere name = 'publisher 1';\n</code></pre> The above SQL statement updates all tuples's name to <code>publisher one</code> in <code>publisher</code> table with the existing name as <code>publisher 1</code>.</p> <p>To delete tuples/records, we use the delete statement.</p> <pre><code>delete from my_db.publisher where name = 'publisher 1';\n</code></pre>"},{"location":"notes/l4_fd/","title":"50.043 - Functional Dependency","text":""},{"location":"notes/l4_fd/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this unit, you should be able to</p> <ul> <li>explain functional dependencies existing in data</li> <li>compute the closure of a set of functional dependencies</li> <li>compute the canonical cover of a set of functional dependencies</li> <li>identify candidate keys from a relation</li> </ul>"},{"location":"notes/l4_fd/#data-anomalies","title":"Data Anomalies","text":"<p>One of the challenges in designing database (not DBMS) is to eliminate data anomalies.</p> <p>The following are the most common kinds of data anomalies:</p> <ol> <li>update anomalies</li> <li>insert anomalies</li> <li>delete anomalies</li> </ol>"},{"location":"notes/l4_fd/#a-motivating-scenario","title":"A motivating scenario","text":"<p>From this point onwards, we treat the terms \"relation\" and \"table\" interchangable.</p> <p>Let's recall the example of article-book-publisher</p> <p></p> <p>In the above we reuse the running example from the earlier class, with one adjustment. Your client suggests that they want to include the date of publish whenever an article is published. Hence naturally you include a date attribute to the  <code>publish</code> relationship.</p> <p>Apply the same steps of ER-to-Relational translation, we have the following relations</p> <ol> <li>Article(id, name) </li> <li>Book(id,name)</li> <li>Publisher(id, name)</li> <li>Publish(article_id, book_id, publisher_id, date)</li> </ol> <p>We create the tables based on the above relational model and load up the data. </p> <p>Specifically the table <code>Publish</code> looks like the following</p> article_id book_id publisher_id date a1 b1 p1 11/5/2019 a2 b1 p1 11/5/2019 a1 b2 p2 21/3/2020 <p>Then we realized something awkward and verified its validity with the client. The date of publish is tied to the book being published. That means, we will have some duplicate dates in the <code>Publish</code> table. </p> <p>This sounds a bit unfortunate, we may shrug it off. However, something far more serious immediately follows.</p> <ul> <li>Suppose we would like to update the date of publish of a book, say <code>b1</code>, we need to update all entries in <code>Publish</code> table with <code>book_id</code> as <code>b1</code>. This is known as the update anomaly.  </li> <li>Suppose we would like to insert a new book to <code>Book</code> table. However, we have nowhere to set its date until we find an <code>article_id</code> and a <code>publisher_id</code> to insert that info into the <code>Publish</code> table. This is known as the insert anomaly.</li> <li>Suppose we would like to delete all entries from the <code>Publish</code> table with <code>book_id</code> as <code>b1</code>. After that, we lose the publishing date of <code>b1</code> entirely. This is known as the delete anomaly.</li> </ul> <p>In summary, in-proper relation model design results in data anomalies. To avoid that, we need a proper method to capture this kind of business requirement, i.e. the publish date is tied to a book. </p>"},{"location":"notes/l4_fd/#functional-dependencies","title":"Functional Dependencies","text":"<p>Functional dependencies define a relationship among data in a relation. It is a representation of certain business requirement as we encountered in the earlier example.</p> <p>Formally speaking, a functional dependency is a constraint between two sets of attributes. </p>"},{"location":"notes/l4_fd/#definition-of-fd","title":"Definition of FD","text":"<p>Let \\(X_1,...,X_m\\), \\(Y_1, ..., Y_n\\) be attributes. we write </p> \\[ X_1,...,X_m \\rightarrow Y_1, ..., Y_n \\] <p>to denote a functional dependency between \\(X_1, ..., X_m\\) and \\(Y_1, ..., Y_n\\). </p> <p>We refer \\(X_1,...,X_m\\) as the antecedent and \\(Y_1, ..., Y_n\\) as the consequent.</p>"},{"location":"notes/l4_fd/#mathematic-meaning-of-fd","title":"Mathematic Meaning of FD","text":"<p>Given a FD \\(X_1,...,X_m \\rightarrow Y_1, ..., Y_n\\), we conclude that values of \\(X_1,...,X_m\\) functionally determine the values of \\(Y_1, ..., Y_n\\). </p> <p>Let \\(t\\) be a tuple, we write \\(t[X]\\) to refer to the value of attribute \\(X\\) in tuple \\(t\\).</p> <p>Formally speaking, an FD \\(X_1,...,X_m \\rightarrow Y_1, ..., Y_n\\) holds in a relation \\(R\\) iff  \\(\\forall t,t' \\in R\\) we have  \\(t[X_1] = t'[X_1] \\wedge ... \\wedge t[X_m] = t'[X_m]\\) implies \\(t[Y_1] = t'[Y_1] \\wedge ... \\wedge t[Y_n] = t'[Y_n]\\).</p> <p>For example in the article-book-publisher example, for any tuples <code>t1</code> and <code>t2</code> in <code>Publish</code> table, <code>t1[book_id] = t2[book_id]</code>  implies <code>t1[date] = t2[date]</code>.</p>"},{"location":"notes/l4_fd/#validity-of-fds","title":"Validity of FDs","text":"<p>Since we are applying relation model to business problems. It is insufficient to observe and validate whether a FD holds in an instance of a relation (i.e. by observing the values in a table!). To verify the validity of FDs, we need to check with the domain expert, project stack holders and end users. In otherwords, FDs must come from the busines requirements, not be inferred from the data.</p>"},{"location":"notes/l4_fd/#functional-dependency-closure","title":"Functional Dependency Closure","text":"<p>Given a relation \\(R\\) and a set of FDs \\(F\\) that holds on \\(R\\). There are many other sets of FDs that are equivalent to \\(F\\).</p> <p>For instance, let \\(F = \\{ X \\rightarrow YZ \\}\\), we find that \\(F' = \\{ X \\rightarrow Y, X \\rightarrow Z\\}\\) and \\(F \\equiv F'\\). </p> <p>Some points to take note. </p> <ul> <li>For brevity, we omit the commas when there is no confusion, i.e. \\(YZ\\) is a short hand of \\(Y,Z\\). </li> <li>Informally, we say \\(F \\equiv F'\\) if \\(F\\) and \\(F'\\) impose the same set constraint on the values in \\(R\\).</li> </ul> <p>Similarly, if we let \\(F'' = F \\cup F'\\), we have \\(F \\equiv F' \\equiv F''\\). </p> <p>Now we consider finding the greatest superset \\(F^+\\) such that \\(F \\equiv F\\)  and for any other \\(G\\) that is \\(F \\equiv G\\), we have \\(G \\subseteq F^+\\). \\(F^+\\) is called the closure of \\(F\\).</p>"},{"location":"notes/l4_fd/#computing-f","title":"Computing \\(F^+\\)","text":"<p>To compute \\(F^+\\) we need some rule-based rewriting system. </p>"},{"location":"notes/l4_fd/#reflexivity-rule","title":"Reflexivity rule","text":"<p>Let \\(Y\\) and \\(X\\) be sets of attributes, such that \\(Y \\subseteq X\\). Then \\(X \\rightarrow Y\\).</p> <p>For instance, {<code>date</code>} \\(\\subseteq\\) { <code>book_id, date</code> }, thus we have <code>book_id,date</code> \\(\\rightarrow\\) <code>date</code></p>"},{"location":"notes/l4_fd/#augmentation-rule","title":"Augmentation rule","text":"<p>Let \\(Y\\), \\(Z\\) and \\(X\\) be sets of attributes, such that \\(X \\rightarrow Y\\). Then \\(XZ \\rightarrow YZ\\). (Note \\(XZ\\) is shorthand for \\(X\\cup Z\\)). </p> <p>For instance, given <code>book_id</code> \\(\\rightarrow\\) <code>date</code>, we have <code>book_id,publisher_id</code> \\(\\rightarrow\\) <code>date,publisher</code>.</p>"},{"location":"notes/l4_fd/#transitivity-rule","title":"Transitivity rule","text":"<p>Let \\(Y\\), \\(Z\\) and \\(X\\) be sets of attributes, such that \\(X \\rightarrow Y\\) and \\(Y \\rightarrow Z\\). Then \\(X \\rightarrow Z\\). </p>"},{"location":"notes/l4_fd/#question-split-rule","title":"Question - Split rule","text":"<p>Given the above three rules, can you prove \\(X \\rightarrow YZ\\) implies \\(X \\rightarrow Y \\wedge X \\rightarrow Z\\)? </p> <p>This derived rule is also known as the split rule.</p>"},{"location":"notes/l4_fd/#algorithm","title":"Algorithm","text":"<ol> <li>At the start, let \\(F^+ = F\\).</li> <li>Find pick one of the three Axioms to apply, to generate a FD, let's say \\(X\\rightarrow Y\\). Let \\(F^+ = F^+ \\cup \\{X\\rightarrow Y\\}\\).</li> <li>repeat step 2 until \\(F^+\\) does not change any more.</li> </ol> <p>For example, given \\(F = \\{ book\\_id \\rightarrow date \\}\\), we compute \\(F^+\\) as follows,</p> step new FD rule 1 \\(article\\_id, book\\_id, publisher\\_id, date \\rightarrow article\\_id\\) Ref 2 \\(article\\_id, book\\_id, publisher\\_id, date \\rightarrow book\\_id\\) Ref 3 \\(article\\_id, book\\_id, publisher\\_id, date \\rightarrow publisher\\_id\\) Ref 4 \\(article\\_id, book\\_id, publisher\\_id, date \\rightarrow date\\) Ref 5 \\(article\\_id, book\\_id, publisher\\_id \\rightarrow article\\_id\\) Ref 6 \\(article\\_id, book\\_id, publisher\\_id \\rightarrow book\\_id\\) Ref 7 \\(article\\_id, book\\_id, publisher\\_id \\rightarrow publisher\\_id\\) Ref 8 \\(book\\_id, publisher\\_id, date \\rightarrow book\\_id\\) Ref 9 \\(book\\_id, publisher\\_id, date \\rightarrow publisher\\_id\\) Ref 10 \\(book\\_id, publisher\\_id, date \\rightarrow date\\) Ref 11 \\(article\\_id, publisher\\_id, date \\rightarrow article\\_id\\) Ref 12 \\(article\\_id, publisher\\_id, date \\rightarrow publisher\\_id\\) Ref 13 \\(article\\_id, publisher\\_id, date \\rightarrow date\\) Ref 14 \\(article\\_id, book\\_id, date \\rightarrow article\\_id\\) Ref 15 \\(article\\_id, book\\_id, date \\rightarrow book\\_id\\) Ref 16 \\(article\\_id, book\\_id, date \\rightarrow date\\) Ref 17 \\(article\\_id, book\\_id \\rightarrow article\\_id\\) Ref 18 \\(article\\_id, book\\_id \\rightarrow book\\_id\\) Ref 19 \\(article\\_id, publisher\\_id \\rightarrow article\\_id\\) Ref 20 \\(article\\_id, publisher\\_id \\rightarrow publisher\\_id\\) Ref 21 \\(article\\_id, date \\rightarrow article\\_id\\) Ref 22 \\(article\\_id, date \\rightarrow date\\) Ref 23 \\(book\\_id, date \\rightarrow book\\_id\\) Ref 24 \\(book\\_id, date \\rightarrow date\\) Ref 25 \\(publisher\\_id, date \\rightarrow publisher\\_id\\) Ref 25 \\(publisher\\_id, date \\rightarrow date\\) Ref <p>No other rules are applicable, then we are done.</p>"},{"location":"notes/l4_fd/#canonical-cover","title":"Canonical Cover","text":"<p>A canonical cover of \\(F\\) is the smallest possible subset of \\(F\\) such that its closure is \\(F^+\\). </p> <p>The above statement is intuitive but not precise. To be precise, we need to define the standard form of FDs. </p>"},{"location":"notes/l4_fd/#standard-form-definition","title":"Standard form definition","text":"<p>An FD is in standard form iff its RHS is a single attribute.</p> <p>It follows that for any set of FDs, we can convert it into an equivalent set with standard form FDs. (Hint: we know \\(X\\rightarrow YZ\\) implies \\(X \\rightarrow Y\\) and \\(X \\rightarrow Z\\) holds.)</p>"},{"location":"notes/l4_fd/#formal-definition","title":"Formal definition","text":"<p>Let \\(F\\) denote a set of FDs, we say \\(F_c\\) is the canonical cover iff </p> <ol> <li>All FDs in \\(F_c\\) are in standard form; and</li> <li>\\(F_c^+ \\subseteq F^+ \\wedge F^+ \\subseteq F_c^+\\); and</li> <li>\\(\\neg \\exists G \\subset F_c\\) such that  \\(G^+ \\subseteq F^+ \\wedge F^+ \\subseteq G^+\\)</li> </ol>"},{"location":"notes/l4_fd/#algorithm-to-compute-f_c","title":"Algorithm to compute \\(F_c\\)","text":"<ol> <li>Convert \\(F\\) to standard form.</li> <li>Minimize the lhs of each FD, by applying Reflexitivity, Augmentation and Transitivity.</li> <li>Remove redundant FDs, by applying Reflexitivity, Augmentation and Transitivity.</li> </ol> <p>For example, consider </p> \\[ F = \\left [ \\begin{array}{ccc}      AB &amp; \\rightarrow &amp; C &amp; (1) \\\\     A &amp; \\rightarrow &amp; BC &amp; (2) \\\\     B &amp; \\rightarrow &amp; C &amp; (3) \\\\     A &amp; \\rightarrow &amp; B &amp; (4)     \\end{array}     \\right ] \\] <p>First applying split rule to (2)</p> \\[ F = \\left [ \\begin{array}{ccc}      AB &amp; \\rightarrow &amp; C &amp; (1) \\\\     A &amp; \\rightarrow &amp; B &amp; (2) \\\\     A &amp; \\rightarrow &amp; C &amp; (2') \\\\     B &amp; \\rightarrow &amp; C &amp; (3) \\\\     A &amp; \\rightarrow &amp; B &amp; (4)     \\end{array}     \\right ] \\] <p>Then we apply Augmentation and Transitivity rules to (1) and (2) to minimize LHS of rule (1)</p> \\[ F = \\left [ \\begin{array}{ccc}      A &amp; \\rightarrow &amp; C &amp; (1) \\\\     A &amp; \\rightarrow &amp; B &amp; (2) \\\\     A &amp; \\rightarrow &amp; C &amp; (2') \\\\     B &amp; \\rightarrow &amp; C &amp; (3) \\\\     A &amp; \\rightarrow &amp; B &amp; (4)     \\end{array}     \\right ] \\] <p>Now we find that (2') is a duplicate of (1) and (4) is a duplicate of (2).</p> \\[ F = \\left [ \\begin{array}{ccc}      A &amp; \\rightarrow &amp; C &amp; (1) \\\\     A &amp; \\rightarrow &amp; B &amp; (2) \\\\     B &amp; \\rightarrow &amp; C &amp; (3)      \\end{array}     \\right ] \\] <p>Finally we find that (1) is derivable by applying transitivity to (2) and (3).</p> \\[ F = \\left [ \\begin{array}{ccc}      A &amp; \\rightarrow &amp; B &amp; (2) \\\\     B &amp; \\rightarrow &amp; C &amp; (3)      \\end{array}     \\right ] \\] <p>The above is minimal.</p> <p>Note that the algorithm described above is non-confluent, i.e. depending on the order of FDs being picked a different canonical cover might be generated. </p>"},{"location":"notes/l4_fd/#application-of-canonical-cover","title":"Application of canonical cover","text":"<p>Canonical cover is very useful. We can use it to reduce the number of constraints (which is expensive to verified). We leverage on Canonical cover to identify candidate key for a relation.</p>"},{"location":"notes/l4_fd/#some-extra-terminologies-different-kinds-of-keys","title":"Some extra terminologies - different kinds of keys","text":"<p>In database, we call a set of attribute of a relation as a</p> <ul> <li>Super key if it functionally determines all other attributes </li> <li>Candidate key if it is a minimal set of attributes that functionally determines all other attributes.</li> <li>Primary key if it is one of the candidate key. (We just fix one.)</li> </ul>"},{"location":"notes/l4_normal_form/","title":"50.043 - Normal Forms","text":""},{"location":"notes/l4_normal_form/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this unit, you should be able to </p> <ul> <li>apply lossless decomposition to a relation</li> <li>verify a relation is in 1NF/2NF/BCNF/3NF</li> <li>decompose a relation into 2NF/BCNF/3NF</li> </ul>"},{"location":"notes/l4_normal_form/#decomposition","title":"Decomposition","text":"<p>By now, we know that having an FD in which the LHS of the constraint is not a the primary key of the relation imposes data anomalies. </p> <p>Recall the relation Publish(article_id, book_id, publisher_id, date)</p> <p>with the fillowing data</p> article_id book_id publisher_id date a1 b1 p1 11/5/2019 a2 b1 p1 11/5/2019 a1 b2 p2 21/3/2020 <p>There exists an FD book_id \\(\\rightarrow\\) date. Note that <code>book\\_id</code> is not the primary key (though it is part of the primary key).</p> <p>To fix the issue, we need to decompose <code>publish</code> into two smaller relations. But how?</p> <p>The idea is to decompose it based on the FD. From the FD, we find that book_id determines date. We should move book_id and date into another relation. At the same time we should keep the book_id in <code>publish</code> so that the relationship between author_id, book_id and publisher_id is not lost. </p>"},{"location":"notes/l4_normal_form/#lossless-decomposition","title":"Lossless decomposition","text":"<p>Give a relation \\(R\\), a decompsition of \\(R\\), say \\(R_1\\) and \\(R_2\\) is lossless, iff \\(R_1 \\bowtie R_2 \\equiv R\\).</p> <p>To ensure a decomposition is lossless, we pick an FD constraint \\(X \\rightarrow Y\\) of \\(R\\) then let \\(R_1 = R_1(XY)\\) and \\(R_2 = R_2( attr(R) - Y)\\), assuming \\(X \\cap Y = \\emptyset\\). We write \\(attr(R)\\) to compute the set of attributes of \\(R\\). It follows that \\(X\\) is the common attributes among \\(R_1\\) and \\(R_2\\), the natural join between \\(R_1\\) and \\(R_2\\) will leverage \\(X\\) hence \\(R_1 \\bowtie R_2 \\equiv R\\).</p> <p>For instance, in the <code>Publish</code> relation above, we may decompose it by the FD book_id \\(\\rightarrow\\) date. </p> <ul> <li>Publish1(article_id, book_id, publisher_id)</li> <li>Publish2(book_id, date)</li> </ul> article_id book_id publisher_id a1 b1 p1 a2 b1 p1 a1 b2 p2 book_id date b1 11/5/2019 b2 21/3/2020 <p>Note that we eliminate the data anomalies. (Eventually, we might merge <code>Publish2</code> with the <code>Book</code> relation, which is a seperate topic.)</p>"},{"location":"notes/l4_normal_form/#normal-forms","title":"Normal Forms","text":"<p>The next question we need to consider is how far should we decompose relation?</p> <p>Normal forms define a set of criteria which allows us to check whether the result of decomposition is good enough.</p>"},{"location":"notes/l4_normal_form/#1nf","title":"1NF","text":"<p>A relation is in 1NF iff its schema is flat, (i.e. contains no sub-structure) and there is no repeating group (i.e. there is no repeating column).</p> <p>For example the following relations are not in 1NF</p> student_id name phones 1234 Peter Parker [95598221, 82335354] <p>This relation's schema is not flat.</p> student_id name phone1 phone2 1234 Peter Parker 95598221 82335354 <p>This relation has a set of repeating columns, <code>phone1</code>, <code>phone2</code>. (Though in reality, we could be lenient here, maybe we could rename it to <code>primary contact</code>, <code>secondary contact</code>.)</p>"},{"location":"notes/l4_normal_form/#2nf","title":"2NF","text":"<p>A relation is in 2NF iff </p> <ol> <li>it is in 1NF and </li> <li>all non-key attributes are fully dependent on candidate key. </li> </ol> <p>In other words, the relation is at least 1NF and there should be no partial dependency.</p> <p></p> <p>For example, in the running example </p> <p>Publish(article_id, book_id, publisher_id, date) </p> <p>is in 1NF but not in 2NF, because the attribute <code>date</code> is not fully dependent on the primary key <code>article_id,book_id</code>. It is partially dependent on <code>book_id</code>. </p>"},{"location":"notes/l4_normal_form/#boyd-codd-normal-form-bcnf","title":"Boyd-Codd Normal Form (BCNF)","text":"<p>Given a relation \\(R\\) with FDs \\(F\\), \\(R\\) is in BCNF iff for all non-trivial dependency \\(X \\rightarrow Y \\in F^+\\), \\(X\\) is a super key. </p> <p>An FD is trivial iff its lhs is a superset of the rhs.</p> <p>For example, </p> <ul> <li>Publish1(article_id, book_id, publisher_id)</li> <li>Publish2(book_id, date)</li> </ul> <p>are in BCNF, because the only non trial FDs are  1. <code>article_id,book_id</code> \\(\\rightarrow\\) <code>publisher_id</code> 2. <code>article_id,publisher_id</code> \\(\\rightarrow\\) <code>book_id</code> (recall the ER diagram) 3. <code>book_id</code> \\(\\rightarrow\\) <code>date</code>. </p> <p>Note that FD #2 does not violate the BCNF requirement, because <code>article_id,publisher_id</code> is a candidate key of <code>Publish1</code> hence also a super key.</p>"},{"location":"notes/l4_normal_form/#lemma-a-relation-r-is-in-bcnf-implies-r-is-in-2nf","title":"Lemma: A relation \\(R\\) is in BCNF implies \\(R\\) is in 2NF.","text":"<p>The proof is omitted. You are encouraged to try proving it.</p>"},{"location":"notes/l4_normal_form/#algorithm-to-decompose-into-bcnf","title":"Algorithm to decompose into BCNF","text":"<p>Given a relation \\(R\\) and a set of FDs \\(F\\). The algorithm of decomposing \\(R\\) into BCNF is described as follows. </p> <ol> <li>Compute \\(F^+\\)</li> <li>Let \\(Result = \\{R\\}\\)</li> <li>While \\(R_i \\in Result\\) not in BCNF, do    3.1. Choose \\(X\\rightarrow Y \\in F^+\\) such that \\(X\\) and \\(Y\\) are attribtues in \\(R_i\\) but \\(X\\) is not a super key of \\(R_i\\).   3.2. Decompose \\(R_i\\) into \\(R_{i1}\\) \\(R_{i2}\\) with \\(X\\rightarrow Y\\).   3.3. Update \\(Result = Result - \\{ R_i\\}  \\cup \\{ R_{i1}, R_{i2} \\}\\)</li> </ol>"},{"location":"notes/l4_normal_form/#a-slightly-more-optimized-algorithm","title":"A slightly more optimized algorithm","text":"<ol> <li>def \\(normalize(R)\\)<ol> <li>Let \\(C = attr(R)\\)</li> <li>find an attribute set \\(X\\) such that \\(X^+ \\neq X\\) and \\(X^+ \\neq C\\).<ol> <li>if \\(X\\) is not found, then \\(R\\) is in BCNF</li> <li>else <ol> <li>decompose \\(R\\) into \\(R_1(X^+)\\) and \\(R_2(C-X^+ \\cup X)\\)</li> <li>\\(normalize(R_1)\\)</li> <li>\\(normalize(R_2)\\)</li> </ol> </li> </ol> </li> </ol> </li> <li>\\(normalize(R)\\)</li> </ol> <p>Consider \\(R(A,B,C,D)\\) with FDS \\(\\{AB \\rightarrow C, A\\rightarrow D, C\\rightarrow B\\}\\).</p> <p>First we find all attribute closures.</p> <ul> <li>\\(A^+ = AD\\)</li> <li>\\(B^+ = B\\)</li> <li>\\(C^+ = CB\\)</li> <li>\\(D^+ = D\\)</li> <li>\\(AB^+ = ABCD\\)</li> <li>\\(ABC^+ = ABCD\\)</li> <li>...</li> </ul> <p>We find that \\(AB\\) is a candidate key of \\(R\\).</p> <p>At step 1.2, we found \\(A^+\\), since \\(A^+ = AD \\neq ABCD\\). We decompose  \\(R\\) into </p> <ul> <li>\\(R_1(A,D)\\)</li> <li>\\(R_2(A,B,C)\\)</li> </ul> <p>\\(R_1\\) is already in BCNF. \\(R_2\\) is not, because found \\(C^+ = BC\\). We decompose \\(R_2\\) into</p> <ul> <li>\\(R_{21}(B,C)\\)</li> <li>\\(R_{22}(A,C)\\)</li> </ul> <p>Then we are done.</p>"},{"location":"notes/l4_normal_form/#3nf","title":"3NF","text":"<p>Given a relation \\(R\\) with FDs \\(F\\), \\(R\\) is in 3NF iff for all non-trivial dependency \\(X \\rightarrow Y \\in F^+\\),  1. \\(X\\) is a super key or 2. \\(Y\\) is part of a candidate key</p> <p>The following diagram shows some example</p> <p></p> <p>In the first diagram, \\(X \\rightarrow A\\), assuming \\(KEY\\) is the only candidate key, hence \\(X\\) is not a super key. Further more \\(A\\) is not part of a candidate key. Thus it is a counter example of 3NF.</p> <p>In the second diagram, \\(X\\rightarrow A\\), \\(X\\) is not a supere key and \\(A\\) is part of a candidate key. Thus it is in 3NF.</p>"},{"location":"notes/l4_normal_form/#lemma-a-relation-in-3nf-is-also-in-bcnf","title":"Lemma: A relation in 3NF is also in BCNF.","text":"<p>It can be proven from by the definitions. </p>"},{"location":"notes/l4_normal_form/#bcnf-vs-3nf","title":"BCNF vs 3NF","text":"<p>BCNF is easier to compute, we just keep finding a FD that violates the definition and keep decomposing until none is found.</p> <p>Though BCNF decomposition is lossless, it is not dependency preserving.</p> <p>A FD set \\(F_i\\) is preserved by \\(R_i\\) iff for each \\(X_1...X_n \\rightarrow Y \\in F_i\\), \\(X_1,...,X_n,Y\\) are attributes of \\(R_i\\).</p> <p>Recall the previous example \\(R(A,B,C,D)\\) with FDS \\(\\{AB \\rightarrow C, A\\rightarrow D, C\\rightarrow B\\}\\).</p> <p>Applying BCNF-decomposition will yield \\(R_1(A,D), R_{21}(B,C), R_{22}(A,B)\\) which do not preserve \\(AB\\rightarrow C\\).</p>"},{"location":"notes/l4_normal_form/#algorithm-to-compute-3nf","title":"Algorithm to compute 3NF","text":"<p>With that difference in mind, we present the algorithm to compute 3NF as folows. </p> <ol> <li>Apply the BCNF algorithm to decompose \\(R\\), let's say the result is a set of relations \\(R_1, ..., R_n\\).</li> <li>Let \\(F_1,...,F_n\\) be the list of FDs preserved by \\(R_1, ..., R_n\\).</li> <li>Compute \\((F_1 \\cup ... \\cup F_n)^{+}\\). Let \\(\\bar{F} = F - (F_1 \\cup ... \\cup F_n)^{+}\\).</li> <li>For each \\(X_1...,X_n\\rightarrow Y \\in \\bar{F}\\), create a new relation \\(R'(X_1,...,X_n,Y)\\)</li> </ol> <p>For example, recall the previous example \\(R(A,B,C,D)\\) with FDS \\(\\{AB \\rightarrow C, A\\rightarrow D, C\\rightarrow B\\}\\).</p> <p>After the BCNF decomposition, we realize \\(R_1(A,D), R_{21}(B,C), R_{22}(A,B)\\) do not preserve \\(AB\\rightarrow C\\). We create (or restore) \\(R_2(A,B,C)\\). </p> <p>\\(R_2\\) subsumes \\(R_{21}(B,C)\\) and \\(R_{22}(A,B)\\), hence we remove \\(R_{21}\\) and \\(R_{22}\\) and keep \\(R_2\\). </p> <p>Alternatively, we could have used the BCNF algorithm but do not decompose \\(R_2\\) since it does not violate 3NF.</p>"},{"location":"notes/l5_access_method/","title":"50.043 - Access Method","text":""},{"location":"notes/l5_access_method/#learning-outcomes","title":"Learning Outcomes","text":"<p>By this end of this unit, you should be able to</p> <ol> <li>Describe the different layout options of a Heap Page</li> <li>Describe the different layout options of a Heap File</li> <li>Explain the purposes of a database index.</li> <li>Explain the different collision handling mechanisms of Hash Table</li> <li>Explain the structure of a B+ Tree</li> <li>Explain the lookup / insert / delete operations over a B+ Tree.</li> <li>Describe the difference between a clustered B+ Tree index and a unclustered B+ Tree index.</li> </ol>"},{"location":"notes/l5_access_method/#data-access-methods","title":"Data Access Methods","text":"<p>As discussed in the earlier classes, through SQL (and relational algebra), we scan data in a table, search for data based on filtering condition, update data, insert and delete data.</p> <p>We consider the actual implementation of these access methods.</p> <p>Like any other software implementation, we need to consider the data structure together with the algorithm.</p>"},{"location":"notes/l5_access_method/#page-internal","title":"Page Internal","text":"<p>Recall inside a data page, we find </p> <ul> <li>a set of tuples/records</li> <li>a header </li> <li>the index </li> <li>the log</li> </ul> <p>Let's consider the first two components.  </p>"},{"location":"notes/l5_access_method/#layouts-of-page","title":"Layouts of Page","text":"<p>There are few options of how tuples are organized in a page.</p>"},{"location":"notes/l5_access_method/#option-1-unordered-sequential-layout","title":"Option 1 - Unordered sequential layout","text":"<p>Tuples are stored inside a page, one after another, like a fixed size array. Elements inside the array, i.e. the pages, are unordered. We often call this page structue as a Heap Page.</p> <p>One advantage is that sequential layout is easy to implement, especially for sequential scanning, and insertion.</p> <p>The clear disadvantage is that after some tuples being deleted</p> <ol> <li>we see holes in between tuples</li> <li>the space utilization is unclear</li> </ol>"},{"location":"notes/l5_access_method/#option-2-heap-page-with-bit-array-header","title":"Option 2 - Heap Page with bit array header","text":"<p>In this version we improve over the previous version of Heap Page by including a header. The header is an array of bits whose size equals to the maximum number of tuples in a page (with the meta-data discounted). The i-th bit is 0 implies the i-th tuple slot is unused, otherwise, the slot is used.</p> <p>With this approach we address the deletion issue. To insert a tuple, we need to scan through the header array to look for free slot.</p> <p>There are still some issues to be addressed. </p> <ol> <li>we have to assume that tuples are all in a single fixed size </li> <li>it is hard to search for a particular tuple except sequential scan.</li> </ol> <p>The first issue can be addressed by allowing long tuple spanning across multiple slots. We need to store extra header information.</p> <p>The second issue will be addressed altogther when we consider the internal layout of a database file</p>"},{"location":"notes/l5_access_method/#file-internal","title":"File Internal","text":"<p>From the earlier section, we note that a database file stores data belonging to a particular table. In the database file, we find a collection of pages.</p>"},{"location":"notes/l5_access_method/#layout-of-files","title":"Layout of Files","text":"<p>The issues related to layout of database files are similar to those with pages. We have a few options.</p>"},{"location":"notes/l5_access_method/#option-1-unordered-sequential-layout_1","title":"Option 1 - Unordered sequential layout","text":"<p>Simple approach. We store one page after another. The good news is that all pages are of the same size. </p> <p>A similar issue with this approach is that it is not easy to find pages with free space. </p>"},{"location":"notes/l5_access_method/#option-2-linked-list","title":"Option 2 - Linked list","text":"<p>In this approach, we maintain two linked lists in the file as meta data. One of the list stores all pages which are full. The other one stores all pages which are not yet full.</p>"},{"location":"notes/l5_access_method/#option-3-directory","title":"Option 3 - Directory","text":"<p>An alternative to the linked lists approach is to use a set of directories. Each directory contains a fixed set of slots. Each slot contains a pointer to (i.e. the location of) a page and a freespace bit. Directory slot may also point to another directory if expansion is required. Directories are small in size so that they could fit in the memory. As a file grows with more pages, a new directory could be created just like a data page. </p> <p></p> <p>In the event that we would like to keep track of how much free space per page has, we could change the freespace bit into a freespace integer field. This change supports the need of having variable length tuple. </p>"},{"location":"notes/l5_access_method/#columnar-layout","title":"Columnar Layout","text":"<p>Another alternative we could consider is that instead of storing data as tuple, we could store them based on column, which is known as columnar storage. We omit the detail here and defer the discuss during the cohort lesson. </p>"},{"location":"notes/l5_access_method/#a-quick-summary-so-far","title":"A Quick Summary so far","text":"<p>We have consider the internal layout of a page and the internal layout of a file. To maintain the free/occupied slots information, we maintain the meta data in pages, as well as files. </p> <p>It is clear that storing pages sequentially in a file has certain advantage in particular for sequential scan. The following table sumarizes the average cost of data operation performed on data file with sequential layout and one with random layout. Let \\(t_{r+s}\\) denote rotation and seek time, \\(t_{tf}\\) denote the transfer time and \\(D\\) denote the number of pages in the file.</p> sequential random Insert a record \\(t_{s+r} + 2 \\cdot t_{tf}\\) \\(t_{s+r} + 2  \\cdot t_{tf}\\) Lookup a record \\(t_{s+r} + t_{tf}  \\cdot D / 2\\) \\((t_{s+r} + t_{tf}) \\cdot D / 2\\) Insert a record \\(t_{s+r} + t_{tf}  \\cdot D\\) \\((t_{s+r} + t_{tf})  \\cdot D\\) <p>One issue yet to be addressed is to enable searching for a tuple without resort to sequential scanning which is expensive.</p>"},{"location":"notes/l5_access_method/#index","title":"Index","text":"<p>A way to resolve to search issue with Heap file is to incorporate indices.</p> <p>An index maps values of the searching criteria into a set of record ids. Each record id stores the exact location of a tuple, i.e. page id and tuple slot number.</p> <p>A table may have multiple indices. </p> <p>The idea of indices is similar to adding labels to a thick phyiscal text book. The labels enable us to quickly \"jump\" to a section of interest. </p> <p>There many different options of index. </p>"},{"location":"notes/l5_access_method/#hash-table","title":"Hash table","text":"<p>A hash table maps values of searching criteria into the target set via a hash function. The input to the hash function is the value, and the output is the set of locations of the records. </p>"},{"location":"notes/l5_access_method/#desired-properties-of-a-hash-function","title":"Desired Properties of a hash function","text":"<p>The perfect hash function should be </p> <ol> <li>efficient - takes no time to compute the hash</li> <li>low (or better  to be no) collision. For all \\(x_1 \\neq x_2\\), \\(hash(x_1) \\neq hash(x_2)\\).</li> </ol>"},{"location":"notes/l5_access_method/#dealing-with-collision","title":"Dealing with collision","text":"<p>In reality, collision can't be eliminated. </p> <p>To deal with collision, there are two approaches on the storage level</p> <ol> <li>Open Hashing (a.k.a. separate chaining) we store collided objects in linked-list pointed by the same key.   In the above diagram the values 1000 and 9530 are hashed into the same cell of the hash table, hence 9530 is stored in a chain. </li> <li>Closed Hashing (a.k.a. open addressing) we try to store collided objects in the same hash table.</li> </ol> <p>We discuss a few more alternatives of closed hasing</p>"},{"location":"notes/l5_access_method/#linear-probing","title":"Linear Probing","text":"<p>Linear Probing is a close hashing collision resolution. The idea is that if both values are hashed to the same cell of the hash table, one of them will be stored in the next available slot. </p> <p></p> <p>In the diagram above, both values A and C are hased into the same cell of the hash table. When we try to insert C, we have to place C to the cell available right after the cell where A resides. </p> <p>First of all, for this to work, we need to know the max size of the table, in case the current collided cell is the last element of the table.  Secondly the search time of the value given a key, is linear to the size of the hash table in the worst case. </p>"},{"location":"notes/l5_access_method/#cuckoo-hashing","title":"Cuckoo Hashing","text":"<p>This hashing technique is given this name because it behaves like how parent birds feed their chicks. They often have multiple young chicks in the same nest. When feeding, they often forget which one had been fed before, so they just keep trying one by one. </p> <p>The cuckoo hashing operates with multiple hash functions (\\(n\\) functions) and multiole hash tables (\\(n\\) tables). It goes as follows,</p> <ol> <li>Give a value \\(v\\) to be stored, we hash it with all the hash functions.</li> <li>Given the hashed values \\(cell_1,...,cell_n\\), we search for the first \\(cell_i\\) such that \\(cell_i\\) in \\(table_i\\) is free. <ol> <li>If found, we put \\(v\\) at \\(cell_i\\) in \\(table_i\\).</li> <li>otherwise, we pick one \\(cell_j\\) which is currently occupied by value \\(u\\).<ol> <li>we replace \\(u\\) by \\(v\\)</li> <li>we go to look for a new cell for \\(u\\). (recursive call)</li> </ol> </li> </ol> </li> </ol> <p>We illustrate the idea using the following example. Let \\(n = 2\\)</p> <ul> <li>Step one we would like to insert \\(A\\).  We put \\(A\\) in table 1 at cell \\(H_1(A)\\).</li> <li>Step two we want to insert \\(B\\).  We put \\(B\\) in table 2 at cell \\(H_2(B)\\), because \\(H_1(B)\\) collides with \\(H_1(A)\\).</li> <li>Step three we want to insert \\(C\\).</li> </ul> <p></p> <p>We realized that both \\(H_1(C)\\) and \\(H_2(C)\\) are occupied due to collision. We vacate \\(B\\) and insert \\(C\\).</p> <p></p> <p>We need to re-insert \\(B\\). We put it at cell \\(H_1(B)\\), and vacate \\(A\\). We re-insert \\(A\\) at \\(H_2(A)\\).</p> <p></p> <p>The all values are in the hash table(s).</p> <p></p> <p>The advantage of this approach is that the lookup cost is always \\(O(1)\\). The downside is that we might run into infinite vacate-insert loop. We need to some mechanism to detect the loop, and rehash everything with a new set of hash functions, or add more tables (and new hash functions).</p>"},{"location":"notes/l5_access_method/#bucket-hashing","title":"Bucket Hashing","text":"<p>One issue with Cuckoo Hashing is that eventually, we need to rehash everything. </p> <p>The idea behind Bucket Hashing (a.k.a extensible hashing) is to </p> <ul> <li>store hashed values in buckets (relative small fixed size arrays, so that the sequential search is not expensive). All buckets have the same size.</li> <li>use the \\(n\\) least significant bits (LSB) of the hashed value to decide in  which bucket it should be placed.</li> <li>increase \\(n\\) and add new bucket gradually as some bucket becomes full.</li> </ul> <p>The Bucket Hashing algorithm starts with a global slot array, a bucket. It maintains a set of integer values of \\(n\\) LSB. \\(G\\) denotes the global value of \\(n\\) LSB, there is one and only one. \\(L\\) denotes the local values of \\(n\\) LSB, there is one \\(L\\) per bucket. For all buckets, \\(L \\leq G\\). The algorithm start with some small numbers of \\(G\\) and \\(L\\). </p>"},{"location":"notes/l5_access_method/#bucket-hashing-insertion","title":"Bucket Hashing Insertion","text":"<p>For simplicity, we treat \\(X\\) the same as \\(hash(X)\\). To insert a value \\(X\\),</p> <ol> <li>lookup the bucket for \\(X\\) based on the last \\(G\\) bits of \\(X\\).<ol> <li>if the bucket \\(i\\) is not full, insert \\(X\\) there.</li> <li>otherwise<ol> <li>if the bucket \\(i\\) having \\(L &lt; G\\)<ol> <li>add a new bucket \\(j\\) having same \\(L\\) as \\(i\\), </li> <li>redistribute data from \\(i\\) to \\(i\\) and \\(j\\). </li> <li>increase \\(L\\) for both buckets \\(i\\) and \\(j\\).</li> <li>update the slot array</li> </ol> </li> <li>otherwise<ol> <li>double the slot array</li> <li>add a new bucket \\(j\\) having same \\(L\\) as \\(i\\)</li> <li>redistribute data from \\(i\\) to \\(i\\) and \\(j\\). </li> <li>increase \\(L\\) for both buckets \\(i\\) and \\(j\\).</li> <li>increase \\(G\\) </li> <li>update the slot array</li> </ol> </li> </ol> </li> </ol> </li> </ol> <p>For example, we start with the following empty table</p> <p></p> <p>First, we insert hashed values <code>00000</code>, <code>01100</code> and <code>11010</code>.</p> <p></p> <p>Next, we insert a hashed value <code>10001</code>, but the bucket is full. We need to create a new bucket with \\(L=0\\) and redistribute the data, and update slot array, and increase the \\(L\\) values for both buckets.</p> <p></p> <p>In the third step, we insert hashed values <code>01011</code> and <code>01111</code>. Both values go to the new bucket</p> <p></p> <p>In the fourth step, we insert another value <code>01110</code>. Both the bucket store the 1-LSB as <code>0</code> is full. We add double the slot array, add a new bucket with \\(L=1\\). We redistribute the data into the new bucket and update the references in the slot array.  We set \\(G = 2\\) and increase \\(L\\) for the two affected buckets.</p> <p></p>"},{"location":"notes/l5_access_method/#bucket-hashing-lookup","title":"Bucket Hashing Lookup","text":"<p>To lookup a value \\(X\\)</p> <ol> <li>lookup slot array given \\(G\\) LSB of (hash of) \\(X\\). <ol> <li>if the bucket is found<ol> <li>scan through the bucket for \\(X\\) sequentually</li> </ol> </li> <li>otherwise, report not found</li> </ol> </li> </ol>"},{"location":"notes/l5_access_method/#time-complexity","title":"Time complexity","text":"<p>The insertion time and lookup time are both \\(O(1)\\).</p>"},{"location":"notes/l5_access_method/#b-tree","title":"B+ Tree","text":"<p>One disadvantage of Hash table is that it only supports point query.  For example, it works well with query </p> <p><pre><code>select * from book where book_id = 'b1';\n</code></pre> because we could hash <code>book_id</code>.</p> <p>Hashing does not help to speed up range search query. </p> <p>For instance, we want to search for books that are publish in the year of 2019. </p> <p><pre><code>select * from book where date &gt;= '2019-01-01' and date &lt;= '2019-12-31';\n</code></pre> Even if we hash all <code>date</code>, there is no guarantee the values are store in the same bucket/page/block.</p>"},{"location":"notes/l5_access_method/#recap-binary-search-tree","title":"Recap Binary Search Tree","text":"<p>A binary search tree is a binary tree in which all nodes from left sub-tree are smaller than the current node's value and all nodes from right sub-tree are larger than the current node's value.</p> <pre><code>graph\n  N0(\"10\")--&gt;N1(\"7\")\n  N0(\"10\")--&gt;N2(\"20\")\n  N1(\"7\")--&gt;N3(\"5\")\n  N1(\"7\")--&gt;N4(\"9\")\n  N2(\"20\")--&gt;N5(\"15\")\n  N2(\"20\")--&gt;N6(\"26\")\n  N3(\"5\")--&gt;N7(\"1\")\n  N3(\"5\")--&gt;N8(\"6\")</code></pre>"},{"location":"notes/l5_access_method/#recap-a-balanced-tree","title":"Recap a balanced tree","text":"<p>A binary tree is balanced iff for any non-leaf node, n, the height difference between the left and the right sub-trees should not exceed 1. </p> <p>A binary tree is perfectly balanced iff for any non-leaf node, n, the height difference between the left and the right sub-trees should be 0. </p>"},{"location":"notes/l5_access_method/#b-tree_1","title":"B+ Tree","text":"<p>A B+ Tree is a generalization of a perfect balanced binary search tree, with the following adjustment</p> <ol> <li>Let \\(d\\) denotes the order.</li> <li>The root node may have \\(1\\) to \\(2*d\\) entries.</li> <li>Each non-root node may have \\(d\\) to \\(2*d\\) entries (half-full).</li> <li>The each in a node consist of a key and a value, except for the first entry i.e. \\(v_1, (k_2, v_2), (k_2, v_2), ... , (k_{n}, v_n)\\). </li> <li>The values in non-leaf nodes are reference to the children.</li> <li>The values in the leaf nodes are reference to records in the data file/page/block.</li> <li>Given two consecutive entries \\((k_i, v_i), (k_{i+1}, v_{i+1})\\), where \\(v_{i}\\) is a reference to a subtree, for all values in subtree, their index values must be in between \\(k_i\\) and \\(k_{i+1}\\). For the first entry, its lower bound is definedby the key coming from the parent. (Similar observation applies to the last entry.)</li> <li>All the leaf nodes for a doublely linked list, which enables the range search operation.</li> </ol> <p>For example, the following figure shows a B+Tree with \\(d=2\\)</p> <p></p>"},{"location":"notes/l5_access_method/#look-up","title":"Look-up","text":"<p>To search for a value with key \\(k\\) in a B+ Tree, we start from the root node</p> <ol> <li>find the value \\(v\\) between \\(k_1\\) and \\(k_2\\) such that \\(k_1 \\leq k &lt; k_2\\). Note that \\(k_1\\) might not exist if \\(k_2\\) is the first entry's key, \\(k_2\\) might not exist if \\(k_1\\) is the last entry's key.<ol> <li>if the current node is a non-leaf node, we move the current node to the node pointed by \\(v\\), we repeat the process recursively</li> <li>if the current node is a leaf node, we return the disk data pointed by \\(v\\).</li> </ol> </li> </ol> <p>It is clear that the time complexity is \\(O(log(N))\\)</p>"},{"location":"notes/l5_access_method/#insertion","title":"Insertion","text":"<p>To insert a value with key \\(k\\) into a B+ Tree, we follow the algorithm as follows</p> <ol> <li>find the leaf node \\(L\\) with the current interval where \\(k\\) should fit, (same as the look-up operation). </li> <li>insert_to_leaf(\\(L\\), \\(k\\))</li> <li>def insert_to_leaf(\\(L\\),k)<ol> <li>if \\(L\\) is not full, just insert, we are done!</li> <li>otherwise<ol> <li>create a new node \\(L'\\). </li> <li>(update linked list, only for leaf node), let \\(L'' = succ(L)\\),  then \\(succ(L) = L'\\) and \\(succ(L') = L''\\).</li> <li>redistribute entries evenly between \\(L\\) and \\(L'\\). </li> <li>copy up the middle key, with the value as a pointer to \\(L'\\), that is to insert a new data entry in the parent node. insert_to_nonleaf(parent(\\(L\\)), middle_key)</li> </ol> </li> </ol> </li> <li>def insert_to_nonleaf(\\(N\\), k)<ol> <li>if \\(N\\) is not full, just insert, we are done!</li> <li>otherwise<ol> <li>create a ne wnode \\(N'\\).</li> <li>redistribute entries evenly between \\(N\\) and \\(N'\\).</li> <li>push up (note, not copy) the middle key, with the value as a pointer to \\(N'\\). <ol> <li>if \\(N\\) is not a root node, insert_to_nonleaf(parent(N), middle_key)</li> <li>otherwise, create an empty node as the new root node, insert_to_nonleaf(parent(N), middle_key)</li> </ol> </li> </ol> </li> </ol> </li> </ol> <p>For example, we would like to insert the entry <code>8</code> into the following B+ Tree with \\(d=2\\). For breivity we omitted the disk refernce associated with <code>8</code>.</p> <p></p> <p>We locate the leaf node. However it is full, it has 4 entries, \\(4 = 2*d\\). We create a new leaf node and redistribute the entries and re-join the linked list. </p> <p></p> <p>We copy up the middle key to the parent. Now we need to insert <code>5</code> to the root node.</p> <p></p> <p>The root node is full, we must create a new root child node, redistribute the entries, and create a new root node and push the middle key <code>17</code> to the new root node.</p> <p></p> <p>The time complexity of the insertion algorithm is \\(O(log(N))\\)</p>"},{"location":"notes/l5_access_method/#deletion","title":"Deletion","text":"<p>Given a node \\(N\\), let \\(|N|\\) denote the number of entries in \\(N\\).</p> <p>To delete a value with key \\(k\\) from a B+ Tree, we follow the algorithm as follows</p> <ol> <li>find the leaf node \\(L\\) with the current interval where \\(k\\) should fit, (same as the look-up operation). </li> <li>delete_from_leaf(\\(L\\), \\(k\\))</li> <li> <p>def delete_from_leaf(\\(L\\), \\(k\\))</p> <ol> <li>remove the entry with \\(k\\)<ol> <li>if \\(L\\) is at least half-full (i.e. \\(|L -\\{k\\}| \\geq d\\)), we are done!</li> <li>otherwise<ol> <li>if \\(L\\) has a sibling \\(L'\\), and \\(k'\\) is the key from the parent that divides \\(L\\) and \\(L'\\), such that \\(|L \\cup \\{k'\\} \\cup L'-\\{k\\}| \\geq 2*d\\) (Notes:if both left and right siblings having sufficient entries, we favor the left sibling)<ol> <li>find the new middle key in \\(L \\cup \\{k'\\} \\cup L'-\\{k\\}\\), say \\(k''\\), replace \\(k'\\) by \\(k''\\) in \\(parent(L)\\)</li> <li>if \\(|L \\cup \\{k'\\} \\cup L'-\\{k\\}|-1 \\geq 2*d\\)<ol> <li>re-distribute \\(L \\cup \\{k'\\} \\cup L' - \\{k,k''\\}\\) among the two leaf nodes.</li> <li>otherwise, re-distribute \\(L \\cup \\{k'\\} \\cup L'- \\{k\\}\\) among the two leaf nodes, i.e. \\(k''\\) is copied up.</li> </ol> </li> </ol> </li> <li>otherwise<ol> <li>merge \\(L\\) with its sibling \\(L'\\) into a new node as \\(L \\cup \\{k'\\} \\cup L' - \\{k\\}\\), where \\(k'\\) is the key from the parent dividing \\(L\\) and \\(L'\\).</li> <li>delete_from_nonleaf(\\(parent(L)\\), \\(k'\\))</li> </ol> </li> </ol> </li> </ol> </li> </ol> </li> <li> <p>def delete_from_nonleaf(\\(N\\), \\(k\\))</p> <ol> <li>remove the entry with \\(k\\)</li> <li>if \\(N\\) is a root node and \\(|N -\\{k\\}| &gt; 0\\), we are done!</li> <li>if \\(N\\) is a root node and \\(|N -\\{k\\}| == 0\\), we remove \\(N\\) entirely.</li> <li>if \\(N\\) is not a root node and \\(N\\) is at least half full, we are done.</li> <li>otherwise<ol> <li>if \\(N\\) has a sibling \\(N'\\), and \\(k'\\) is the key from the parent that divides \\(N\\) and \\(N'\\), such that \\(|N \\cup N' - \\{k\\}| \\geq 2*d\\), <ol> <li>find the new middle key in \\(N \\cup \\{k'\\} \\cup N'\\), say \\(k''\\), replace \\(k'\\) by \\(k''\\) in \\(parent(N)\\), redistribute \\(|N \\cup N' - \\{k\\}|\\) among the two nodes. </li> </ol> </li> <li>otherwise<ol> <li>merge \\(N\\) with its sibling \\(N'\\) into a new node as \\(N \\cup \\{k'\\} \\cup N' - \\{k\\}\\), where \\(k'\\) is the key from the parent dividing \\(N\\) and \\(N'\\).</li> <li>delete_from_nonleaf(\\(parent(N)\\), \\(k'\\))</li> </ol> </li> </ol> </li> </ol> </li> </ol> <p>For example, continueing from what we have from the previous example, </p> <p></p> <p>We delete the key <code>22</code>. Since the node is still half full, not merging or redistribution occurs.</p> <p></p> <p>Next we delete key <code>20</code>, which makes the leaf node no longer half-full. We borrow entry <code>24</code> from the sibling on the right. The key <code>24</code> from the parent node is replaced by the new middle key <code>27</code>.</p> <p></p> <p>In the third step, we delete key <code>24</code>. Merging the remaining entry <code>19</code> with <code>27</code>,<code>29</code> into a single leaf node. </p> <p></p> <p>As a consequence, we need to delete <code>27</code> from the parent node.</p> <p></p> <p>The merging effect cascade up, since the non-leaf node containing <code>30</code> is not half-full.</p> <p></p> <p>The time complexity of deletion is also \\(O(log(N))\\).</p>"},{"location":"notes/l5_access_method/#clustered-vs-unclustered-b-tree","title":"Clustered vs Unclustered B+ Tree","text":"<p>A B+ Tree is clustered iff data in the heap file are sorted according to the index attribute (keys in the B+ Tree).</p> <p>Due to its physical storage nature, there is only one clustered index per table. We may have many unclustered indices per table. </p> <p>Clustered index in general out-performs unclustered indices when being applied in range queries.</p>"},{"location":"notes/l5_storage/","title":"50.043 - Storage","text":""},{"location":"notes/l5_storage/#learning-outcomes","title":"Learning Outcomes","text":"<p>By this end of this unit you should be able to</p> <ol> <li>Explain the internal structure a hard disk</li> <li>Compute the estimated cost for a data operation</li> <li>Explain the roles and functionality of a disk manager</li> <li>Explain the roles and functionality of a Buffer Pool</li> <li>Describe the LRU and Clock Replacement Policy</li> </ol>"},{"location":"notes/l5_storage/#motivation","title":"Motivation","text":"<p>Starting from this week onwards, we look deep into the internal of the database manage systems. </p> <p>In particular, we want to find out is how queries are executed and how data are fetched from and updated back to the database.</p> <p>The answer the question, we first need to understand how data are stored in the database. </p>"},{"location":"notes/l5_storage/#hardware","title":"Hardware","text":"<p>For simplicity, we only consider the mechanincal hard disk storage. </p> <p>We do not consider in memory database because it is not common, due to its cost. </p> <p>We do not consider SSD and non-volatile memory because they similar to hard disk storage except that there is no rotational delay and seek time delay is marginal, but the general cost computation model is still applicable. </p>"},{"location":"notes/l5_storage/#mechanical-rotational-hard-disk","title":"Mechanical Rotational Hard disk","text":"<p> (image from Database Management Systems, R. RamaKrishnan and J. Gehrke)</p> <p>The figure above illustrate the internal of a mechanical hard disk, which is a cylindrical structure. It consists of multiple platter of disks stacked up vertically. Each platter consists of multiple tracks. Each track is divided into multiple sectors. Disk heads read from/write to disk track. Only one head is allowed to operate at any point in time. Data are organized into blocks (which is determined by the governing system, DBMS storage or OS file system). A block might span across multiple consecutive sectors. Different system use different block size.</p> <p>We break down the time needed for each type of disk head movement that related to disk I/O.</p> <ul> <li>Seek time \\(t_s\\): to move arm to correct platters &amp; track\u200b, it is roughly  20ms\u200b.</li> <li>Rotate time \\(t_r\\) : to rotate to correct sector\u200b, roughly 10ms\u200b</li> <li>Transfer time \\(t_{tf}\\) : to read/write a block of data\u200b, 0.1ms\u200b</li> </ul>"},{"location":"notes/l5_storage/#random-vs-sequential-access-cost","title":"Random vs sequential access cost","text":"<p>Given \\(D\\) number of blocks of data, the cost of accessing the data randomly is </p> \\[ T_1 = D * (t_s + t_r + t_{tf}) \\] <p>and the cost of accessing the data sequentially is </p> \\[ T_2 = t_s+t_r + D * t_{tf} \\]"},{"location":"notes/l5_storage/#note","title":"Note","text":"<p>For SSD, seek time is a lot faster and there is zero rotate time. Transfer time is also magnitude faster. But the basic components of the breakdown remain the same. Typically we see significant improvement in random access time when upgrading to SSD.</p>"},{"location":"notes/l5_storage/#database-storage","title":"Database Storage","text":"<p>Given the estimated cost of data access, it is clear that we should minimize the factor being multipled with \\(t_s\\) and \\(t_r\\) as much as we can. </p> <p>For most the DBMS storage system, we favor a larger block size as it reduces the number of blocks given the same set of data in size and data related to each other tend to be fetched together in one block. </p> <p>When some records (or tuples) are needed, the entire block of data is read from the disk. Same idea applies when the records needed to be written back to the disk. </p> <p>To minimize the overhead of reading/writing the data blocks back and forth and to boost the performance of the data processing, the DBMS need to  1. have some logical mapping over the physical storage - disk manager 2. have some in memory cache - buffer pool</p>"},{"location":"notes/l5_storage/#disk-manager","title":"Disk Manager","text":"<p>Different DBMSes have different ways of managing the logical mapping. One common approach is to store data as a collection of files. The disk manager keeps track of the mapping from database tables to files, which is known as the system catalogue. These database files are no ordinary ones which can be accessed by other software in the OS. They have to be access through the Disk Manager.</p> <p>Each file managed by the Disk Manager is organized as a set of pages. </p>"},{"location":"notes/l5_storage/#page-vs-block","title":"Page vs Block","text":"<p>Page is the basic unit of data storage from the memory (RAM) perspective, determined by the DBMS configuration. </p> <p>Block is the basic unit of data storage from the physical disk (hard disk) perspective, determined by the OS configuration. </p> <p>These two might or might not be in sync in terms of size. </p> <p>From now onwards, for the ease of reasoning, we assume a page is of the same size of a block unless we specifically define otherwise.</p>"},{"location":"notes/l5_storage/#what-is-inside-a-page","title":"What is Inside a Page","text":"<p>Besides a set of tuples/records, a page contains </p> <ul> <li>the meta information (also known as the header)</li> <li>the index and the log</li> </ul> <p>we will defer the discussion until the next unit.</p>"},{"location":"notes/l5_storage/#cache-system-buffer-pool","title":"Cache system - Buffer Pool","text":"<p>Knowing that fetching data block (or page) to and fro is expensive. We need a fixed set of working space in the RAM, which serves as the Buffer Pool. </p> <p>Any read operation from the DBMS must check whether the target page is in the Buffer Pool, if it is in the pool, the operation should be performed on the cached copy.  When the cached copy needs to be evicted, it will be only be written to the disk if it is dirty, i.e. some data has changed. Otherwise, it will be discarded. </p> <p>The Buffer Pool has a fixed set of slots (known as frames). Each page is loaded into a frame when it is fetched from the disk.</p> <p>The Buffer Pool maintains the following information,</p> <ol> <li>Page to frame mapping</li> <li>Whether a page is pinned. A page is pinned means it is being accessed currently. When a data operation (query/insert/update/delete) is performed it will pin all the required pages. When the operation is completed, the pages are unpinned. Note that in a full scale system, we might see concurrent accesses to pages, thus, in most of the situation, the Buffer Pool maintains a pin counter per slot.</li> <li>Whether a page is dirty.</li> <li>Extra information to support the eviction policy.</li> </ol>"},{"location":"notes/l5_storage/#eviction-policy","title":"Eviction Policy","text":"<p>When a new page needs to be fetched from the disk but the Buffer Pool is full, we need to free up some slot. An eviction policy helps to decide which frame slot should be freed. </p> <p>In the ideal situation, the (dream) eviction policy should evict the cached page requested farthest in the future, because we won't need it again so soon compared to the rest. In reality, such policy does not exist as we can't see the future.</p> <p>Instead, we use several policies based on the situation</p>"},{"location":"notes/l5_storage/#lru-policy","title":"LRU Policy","text":"<p>The Least Recently Used (LRU) Policy works as follows,</p> <ol> <li>pinned frames should not be evicted</li> <li>keep track of the timestamp when frame's pin was set to 0. </li> <li>evict the page in the frame with the smallest (oldest) timestamp.</li> </ol> <p>In details steps.</p> <p>Initialize all frames to be empty with pincount 0</p> <ol> <li>when a page is being pinned<ol> <li>if the page is already in the pool, increase the pincount, return the page in the frame.</li> <li>otherwise, <ol> <li>find the frames with 0 pincount, find the one oldest timestamp, write it to disk if it is dirty.</li> <li>load the new page to this vacant frame, set pincount = 1</li> </ol> </li> </ol> </li> <li>when a page is being unpinned, decrease the pincount and update the timestamp to the current time.</li> </ol>"},{"location":"notes/l5_storage/#example","title":"Example","text":"<p>For example, consider the a buffer pool with 3 frame slots. The folloing sequence of pages are requested in order (pin, then unpin immediately)</p> \\[ 1,2,3,4,1,2 \\] <p>pg = page, pc = pincount, ts = timestamp</p> time page req'ed frame 0 frame 1 frame 2 1 1 pg:1, pc:0, ts: 1 2 2 pg:1, pc:0, ts: 1 pg:2, pc:0, ts:2 3 3 pg:1, pc:0, ts: 1 pg:2, pc:0, ts:2 pg:3, pc:0, ts:3 4 4 pg:4, pc:0, ts: 4 pg:2, pc:0, ts:2 pg:3, pc:0, ts:3 5 1 pg:4, pc:0, ts: 4 pg:1, pc:0, ts:5 pg:3, pc:0, ts:3 6 2 pg:4, pc:0, ts: 4 pg:1, pc:0, ts:5 pg:2, pc:0, ts:6"},{"location":"notes/l5_storage/#clock-replacement-policy","title":"Clock Replacement Policy","text":"<p>While LRU works, its implementation could be less complex. </p> <p>The Clock Replacement Policy is a simpler approach which approximates LRU. (The formal proof is not discussed in this module.)</p> <p>The Clock Replacement Policy does keep track of timestamps, instead it keep track of the reference bit (0 or 1) per frame and a hand (index to frame slots). During the initialization phase, all frames are empty with pincount and reference bit set to 0, the hand is set to 0.</p> <p>The Clock Replacement Policy works as follows, </p> <ol> <li>when a page is being pinned<ol> <li>if the page is already in the pool, set the frame's reference bit to 1, increase the pincount, return the page in the frame.</li> <li>otherwise, we need to find a slot for it, by checking the frame pointed by the hand<ol> <li>if it is pinned, (somebody is using it), advance hand to the next frame</li> <li>otherwise<ol> <li>if the frame's reference bit is 1, set it to 0, advance hand</li> <li>otherwise<ol> <li>write the current payload to disk if it is dirty</li> <li>load the new page to this slot</li> <li>set reference bit to 1, and set pincount to 1</li> <li>advance the hand to the next frame</li> </ol> </li> </ol> </li> </ol> </li> </ol> </li> <li>when a page is being unpinned, decrease the pincount.</li> </ol> <p>Compared to LRU, the Clock Replacement Policy uses less memory, reference bit vs timestamp. The Clock Replacement Policy is more efficient, because there is no need to search for the oldest timestamp.</p>"},{"location":"notes/l5_storage/#example_1","title":"Example","text":"<p>Let's re-run the same example </p> <p>\\(1,2,3,4,1,2\\) </p> <p>with Clock Replacement Policy</p> <p>pg = page, pc = pincount, ref = reference bit</p> time page req'ed frame 0 frame 1 frame 2 hand 1 1 pg:1, pc:0, ref: 1 1 2 2 pg:1, pc:0, ref: 1 pg:2, pc:0, ref: 1 2 3 3 pg:1, pc:0, ref: 1 pg:2, pc:0, ref: 1 pg:3, pc:0, ref: 1 0 4 4 pg:1, pc:0, ref: 0 pg:2, pc:0, ref:1 pg:3, pc:0, ref:1 1 5 4 pg:1, pc:0, ref: 0 pg:2, pc:0, ref:0 pg:3, pc:0, ref:1 2 6 4 pg:1, pc:0, ref: 0 pg:2, pc:0, ref:0 pg:3, pc:0, ref:0 0 7 4 pg:4, pc:0, ref: 1 pg:2, pc:0, ref:0 pg:3, pc:0, ref:0 1 8 1 pg:4, pc:0, ref: 1 pg:1, pc:0, ref:1 pg:3, pc:0, ref:0 2 9 2 pg:4, pc:0, ref: 1 pg:1, pc:0, ref:1 pg:2, pc:0, ref:1 0"},{"location":"notes/l5_storage/#other-optimization","title":"Other optimization","text":"<p>There is no one size fit all solution here. Modern databases often employ some heuristics, called prefetching. The idea is to anticipate subsequent pages will soon be needed when an operation requests for a page (say, a sequential scan of table). This eliminates some of the cost of rotation time and seek time. </p>"},{"location":"notes/l6_query_operation/","title":"50.043 Query Operations","text":""},{"location":"notes/l6_query_operation/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this unit, you should be able to </p> <ul> <li>assess estimated cost of database operation, namely, select, sort and join</li> <li>describe the external sort algorithm</li> <li>describe nested loop join, block nested loop join, index nested join, sort merge join and hash join</li> </ul>"},{"location":"notes/l6_query_operation/#recap-relational-algebra","title":"Recap Relational Algebra","text":"<p>Recall that in week 2, we investigated into relation model and relational alegbra. We argue that relational algebra is a good abstraction to describe data processing operation over a relational model. </p> <p>Given that we know a bit more of the internal implementation of a physical database, (the data structure aspect) we start look into possible ways of implemeting the relational algebra operations in a physical datase (the algorithm aspect).</p>"},{"location":"notes/l6_query_operation/#selection-operation","title":"Selection Operation","text":"<p>Let \\(R\\) denote a relation (or in other words, a table), we consider the selection operation \\(\\sigma_{C}(R)\\) where \\(C\\) is a logical predicate. The result of this operation is a new relation whose tuples are those coming from \\(R\\) and satisfying the predicate \\(C\\). </p> <p>There are several ways to implement this operation. </p>"},{"location":"notes/l6_query_operation/#sequential-scanning","title":"Sequential Scanning","text":"<p>Knowing that the data in \\(R\\) are stored in a Heap file as a set of Heap pages, we could scan \\(R\\) by reading the data from \\(R\\) page by page. For each tuple in the page, we remove it when it does not satisfy \\(C\\) and retain it otherwise. </p> <p>The cost (in terms of number of Disk I/Os) of using the sequential scanning approach is \\(B(R)\\), where \\(B(R)\\) denotes the number of pages storing \\(R\\).</p>"},{"location":"notes/l6_query_operation/#scanning-with-b-tree-index","title":"Scanning with B+ Tree index","text":"<p>A smarter (but not always) way is to make use to the B+ tree index created over the attributes mentioned in \\(C\\). </p>"},{"location":"notes/l6_query_operation/#case-1-clustered-index","title":"Case 1: Clustered index","text":"<p>In this case the B+ Tree index is clustered, i.e. the data in the pages are stored according to the order of the index attribute. </p> <p>We have to traverse down the B+ Tree to find the boundaries of \\(C\\), and for each leaf node value (reference) we retrieve the page storing that tuple and check whether that tuple satisfies \\(C\\).</p> <p>Assuming each node in the B+ Tree occupies a page, the cost of this approach is </p> \\[ log_d(|R|/d) + \\alpha(C,R) \\cdot B(R) \\] <p>Where \\(|R|\\) denotes the number of tuples in \\(R\\), \\(d\\) denotes the order of the B+Tree. \\((|R|/d)\\) is number of leaf nodes in the B+ Tree, which is the worst case, every node is just half-full. For simplicity, we assume root node is having at least \\(d\\) entries. \\(\\alpha(C,R)\\) denotes the selectivity of the predicate \\(C\\) over \\(R\\), i.e. the number of tuples \\(R\\) satisfying \\(C\\) divided by \\(|R|\\).</p> <p>The first term \\(log_d(|R|/d)\\) estimates the cost of traversing the B+ Tree to find the correct starting leaf node. \\(\\alpha(C,R) \\cdot B(R)\\) estimates the cost of scanning the tuples of \\(R\\). Since the index is clustered, tuples that are stored in the order of the index attributes, we can fetch adjacient tuple w.r.t the attribute referred in \\(C\\) in a single page.</p>"},{"location":"notes/l6_query_operation/#case-2-unclustered-index","title":"Case 2: Unclustered Index","text":"<p>When the B+ Tree is not clustered, the estimated cost becomes</p> \\[ log_d(|R|/d) + \\alpha(C,R) \\cdot | R | \\] <p>A page read is needed for every tuple (in the worst case). </p> <p>As we can see that when \\(\\alpha(C,R) &gt; (1 / pagesize)\\), it is definitely more viable to abandon the index and go for the sequential scan.</p> <p>One possible improvement of using unclustered index is </p> <ol> <li>traverse down the B+ Tree to find the correct starting and ending tuple locations (i.e. page id and slot id), </li> <li>sort all the tuple locations by page ids before </li> <li>reading the sorted pages and filter.</li> </ol> <p>This will bring us back to the same estimated cost as the clustered index (modular the cost of sorting).</p>"},{"location":"notes/l6_query_operation/#sort-operation","title":"Sort Operation","text":"<p>Sorting is essential. </p> <p>From Data Driven World and Algorithms, we learned various sorting algorthms. All of these algorithms assumes that the data can fit in the RAM. In the settings of database operations, this assumption is no longer valid. </p>"},{"location":"notes/l6_query_operation/#external-sort","title":"External Sort","text":"<p>The external sorting algorithm is an instance of the merge sort. </p>"},{"location":"notes/l6_query_operation/#recap-merge-sort","title":"Recap Merge sort","text":"<p>The merge sort algorithm is as follows</p> <ol> <li>def merge_sort(input)<ol> <li>if input size &lt; 2<ol> <li>return input, it is already sorted</li> </ol> </li> <li>otherwise<ol> <li>divide input into halves, say, i1 and i2</li> <li>call merge_sort(i1) to get s1</li> <li>call merge_sort(i2) to get s2</li> <li>combine(s1,s2) to get sorted</li> <li>return sorted</li> </ol> </li> </ol> </li> <li>def combine(s1, s2)<ol> <li>create an empty list out</li> <li>while s1 is not empty and s2 is not empty<ol> <li>let e1 be the first element of s1</li> <li>let e2 be the first element of s2</li> <li>if e1 &lt; e2<ol> <li>add e1 to out</li> <li>remove e1 from s1</li> </ol> </li> <li>otherwise<ol> <li>add e2 to out</li> <li>remove e2 from s2</li> </ol> </li> </ol> </li> <li>if s1 is not empty<ol> <li>concatenate s1 to out</li> </ol> </li> <li>if s2 is not empty <ol> <li>concatentate s2 to out</li> </ol> </li> <li>return out </li> </ol> </li> </ol>"},{"location":"notes/l6_query_operation/#external-sort_1","title":"External sort","text":"<p>The external sort algorithm is a generalized version of merge sort, which operates without loading the full set of data into RAM.</p>"},{"location":"notes/l6_query_operation/#external-sort-by-example","title":"External sort by example","text":"<p>The algorithm makes use of the buffer pool.</p> <p>Let's consider an example. Suppose we have a buffer pool of 3 frames and would like to sort the following 8 pages of data. Each page contains two tuples. </p> <p></p>"},{"location":"notes/l6_query_operation/#phase-0","title":"Phase 0","text":"<p>In this phase, we divide the input pages into \\(\\lceil8/3 \\rceil = 3\\) runs and sort each run.  </p> <p>From the next phase onwards, we merge each the sorted runs into larger runs, until all are merged into a single one run.</p>"},{"location":"notes/l6_query_operation/#phase-1","title":"Phase 1","text":"<p>We merge 3 runs into two runs. </p> <p>Firstly we merge the two runs on the left into one.</p> <p></p> <p>When merging two runs into one, we divide the buffer pool's frames into the input frames and output frame. There is only one output frame, the rest are inputs. </p> <p>In this running example, we have two input frames and one output frame. </p> <p></p> <p>We merge the run <code>[(2,3), (4,6), (9,9)]</code> with <code>[(3,5), (6,7), (8,11)]</code>. We use list notation to denote a run, and a tuple to denote a page.</p> <ol> <li>We load the page <code>(2,3)</code> into an input frame and <code>(3,5)</code> into another input frame.</li> <li>We find the smallest leading tuple from both frames, i.e. <code>2</code> and move it in the output frame</li> <li>We repeat the same operation, and find <code>3</code>, which is moved to the output frame</li> <li>The output frame is full and write it to disk.</li> <li>The first input frame is not empty (because, we assume both <code>2</code> and  <code>3</code> are moved to the output frame). We load the second page <code>(4,6)</code> into this frame.</li> <li>The process is repeated until both runs are processed.</li> </ol> <p>The 3<sup>rd</sup> run <code>[(0,1), (2,)]</code> is alone, hence there is no merging required.</p> <p>At the end of phase 1, we have a new set of runs</p> <p><code>[ [(2,3), (3,4), (5,6), (6,7), (8,9), (9,11)],  [(0,1), (2,1)] ]</code></p>"},{"location":"notes/l6_query_operation/#phase-2","title":"Phase 2","text":"<p>In this phase we merge the output from the phase into a single run</p> <p></p>"},{"location":"notes/l6_query_operation/#external-sort-algorithm","title":"External sort algorithm","text":"<p>We consider the pseudo-code of the exernal sort algorithm.  The algorithm is defined by a pseudo function <code>ext_sort(in_pages, bpool)</code>, which has the following input and output.</p> <ul> <li>input <ul> <li>bpool - the buffer pool (in RAM)</li> <li>in_pages - the pages to be sorted (on disk)</li> </ul> </li> <li>output<ul> <li>sorted results (on disk)</li> </ul> </li> </ul> <p>We find the pseudo code as follows</p> <ol> <li>def ext_sort(in_pages, bpool)<ol> <li>let runs = divide_n_sort(in_pages, bpool)</li> <li>while size_of(runs) &gt; 1<ol> <li>let runs = merge(runs, bpool)</li> </ol> </li> <li>return runs</li> </ol> </li> </ol> <p>At step 1.1, we call a helper function <code>divide_n_sort(in_pages, bpool)</code> to generate the initial runs, (phase 0).  Steps 1.2 to 1.3 define the merging phases (phase 1, phase 2, .., etc). We repeatly call the helper function <code>merge(runs, run_size, bpool)</code> to merge the current runs set until all runs are merged into a single run.</p>"},{"location":"notes/l6_query_operation/#helper-function-divide_n_sort","title":"Helper function divide_n_sort","text":"<p>Next we consider the helper function <code>divide_n_sort(in_pages, bpool)</code>, which has the following input and output</p> <ul> <li>input<ul> <li>bpool - the buffer pool (in RAM)</li> <li>in_pages - the pages to be sorted (on disk)</li> </ul> </li> <li> <p>output</p> <ul> <li>runs - list of lists (on disk). Each inner list (a run) consists of a set of sorted data. (on disk)</li> </ul> </li> <li> <p>def divide_n_sort(in_pages, bpool)</p> <ol> <li>let count = 0</li> <li>let m = size_of(bpool)</li> <li>initialize runs to be an empty list (on disk)</li> <li>while (m * count) &lt; size_of(in_pages)<ol> <li>load the m pages from in_pages at offset (m * count)</li> <li>sort data in bpool</li> <li>group these m sorted pages into one run (on disk)</li> <li>append run to runs (on disk)</li> <li>increase count by 1 </li> </ol> </li> <li>return runs</li> </ol> </li> </ul>"},{"location":"notes/l6_query_operation/#helper-function-merge","title":"Helper function merge","text":"<p>We consider the pseudo code of the function <code>merge(runs, bpool)</code></p> <ul> <li>input <ul> <li>bpool - the buffer pool (in RAM)</li> <li>runs - the list of lists (on disk), each inner list (a run) consists of a set of sorted data. (on disk)</li> </ul> </li> <li> <p>output</p> <ul> <li>next_runs - the runs in next phase (on disk)</li> </ul> </li> <li> <p>def merge(runs, bpool)</p> <ol> <li>initialize next_runs to be an empty list (on disk)</li> <li>let m = size_of(bpool)</li> <li>let l = size_of(runs)</li> <li>divide bpool into m-1 and 1 frames<ol> <li>let in_frames = m-1 frames of bpool for input </li> <li>let out_frame = 1 frame of bpool for output</li> </ol> </li> <li>let count = 0</li> <li>while (m-1)*count &lt; l<ol> <li>let batch =  extract m-1 runs from runs at offset (m-1) * count</li> <li>let out_run  =  merge_one(batch, in_frames, out_frame)</li> <li>append out_run to next_runs</li> <li>increment count by 1</li> </ol> </li> <li>return next_runs</li> </ol> </li> </ul> <p>The merge function processes the runs by merging every batch (consist of b-1 runs) into a larger run for the next phase, where b is the size of the buffer pool. It utilizes another helper function <code>merge_one(runs, bpool)</code> to merge a batch, which has the following signature and definition</p> <ul> <li>input<ul> <li>batch - a segment from global runs. In variant, size_of(batch) &lt;= size_of(in_frames)</li> <li>in_frame - frames set aside for input pages</li> <li>out_frame - output frame set aside for output.</li> </ul> </li> <li> <p>output</p> <ul> <li>output_run (on disk)</li> </ul> </li> <li> <p>def merge_one(batch, in_frames, out_frame)</p> <ol> <li>initialize output_run to be an empty collection</li> <li>let m = size_of(in_frames)</li> <li>while (batch is not empty) and not (all in_frames isempty)<ol> <li>for i in (0 to m)<ol> <li>if in_frames[i] is empty and batch[i] is not empty <ol> <li>load a page batch[i] to in_frames[i]</li> </ol> </li> </ol> </li> <li>find the smallest record from the leading tuples from in_frames[0] to in_frames[m], move it to out_frame</li> <li>if out_frame is full, write it to output_run</li> </ol> </li> <li>return output_run</li> </ol> </li> </ul>"},{"location":"notes/l6_query_operation/#cost-estimation-for-external-sort","title":"Cost estimation for External sort","text":"<p>Assuming the size of <code>in_pages</code> is \\(n\\), and the buffer pool size is \\(m\\), we consider the estimated cost (disk I/Os) of external sort algorithm.</p> <p>The <code>ext_sort</code> function has two main parts. The phase 0, the <code>divide_n_sort</code> function call and the while loop, i.e.  phase i where i&gt;0.</p> <ul> <li>In <code>divide_n_sort</code>, we read each page to be sorted once and write back to disk once. Hence the cost is \\(2 \\cdot n\\).</li> <li>In each iteration of the while loop in ext_sort function, we merge every \\(m-1\\) runs into a single run, until all runs are merged. There should be \\(\\lceil log_{(m-1)}(\\lceil n/m \\rceil) \\rceil\\) iterations. For each iteration, we read and write each page once. Hence the cost is \\(2 \\cdot  n \\cdot  \\lceil log_{(m-1)}(\\lceil n/m \\rceil)\\rceil\\).</li> </ul>"},{"location":"notes/l6_query_operation/#join-operation","title":"Join Operation","text":"<p>Lastly, we consider the implementation of the join operation.</p> <p>Let \\(R\\) and \\(S\\) be relations. There are several ways of implementing \\(R \\bowtie_{c} S\\). </p> <ul> <li>The most naive implementation is to compute \\(\\sigma_{c}(R \\times S)\\). It is costly, because of the computation of cartesian product.</li> <li>Alternatively, we may use a nested loop</li> </ul>"},{"location":"notes/l6_query_operation/#nested-loop-join","title":"Nested Loop Join","text":"<p>One possible approach is to use nested loop </p> <ol> <li>for each tuple \\(t\\) in \\(R\\)<ol> <li>for each tuple \\(u\\) in \\(S\\)<ol> <li>if \\(t\\) and \\(u\\) satisfy \\(c\\), output \\(t\\) and \\(u\\).</li> </ol> </li> </ol> </li> </ol> <p>The cost of this approach is \\(B(R) + |R| \\cdot B(S)\\). The \\(B(R)\\) is total cost of loading all tuples from \\(R\\) once. For each tuple of \\(R\\), it will be compared with every tuple in \\(S\\), hence \\(|R| \\cdot B(S)\\).</p> <p>If we flip the outer/inner relation roles of \\(R\\) and \\(S\\), we get the following cost \\(B(S) + |S| \\cdot B(R)\\).</p>"},{"location":"notes/l6_query_operation/#block-nested-loop-join","title":"Block Nested Loop Join","text":"<p>One obvious issue with the nested loop join is that it only utilizes three frames of the buffer pool, 1 frame for \\(R\\) and 1 frame for \\(S\\) and 1 frame for the output. The frame of \\(S\\) is evicted everytime a tuple in \\(R\\) is checked. </p> <p>We could speed up nested loop join by utilizing all frames of the buffer pool. </p> <p>Assuming the buffer pool is of size \\(m\\), we divide the buffer pool into \\(m-2\\) frames for loading \\(R\\) and 1 frame for loading \\(S\\) and 1 frame for output</p> <ol> <li>for each \\(m-2\\) pages in \\(R\\), we extract each tuple \\(t\\)<ol> <li>for each page in \\(S\\), we extract each tuple \\(u\\).<ol> <li>if \\(t\\) and \\(u\\) satisfy \\(c\\), output \\(t\\) and \\(u\\).</li> </ol> </li> </ol> </li> </ol> <p>The cost of this approach is \\(B(R) + \\lceil B(R) / (m - 2) \\rceil \\cdot B(S)\\). The cost will be \\(B(S) + \\lceil B(S) / (m - 2) \\rceil \\cdot B(R)\\) if the outer/inner relations are swapped.</p>"},{"location":"notes/l6_query_operation/#index-nested-loop-join","title":"Index Nested Loop Join","text":"<p>If the join predicate \\(c\\) is \\(R.a = S.b\\) and an index attribute exists for \\(S.b\\), we can optimize the nested loop join by using the indexed relation as the inner relation. </p> <ol> <li>for each tuple \\(t\\) in \\(R\\)<ol> <li>find the tuple \\(u\\) in \\(S\\) with \\(u.b = t.c\\) using index <ol> <li>output \\(t\\) and \\(u\\).</li> </ol> </li> </ol> </li> </ol> <p>If the index is clustered, the cost of this approach is \\(B(R) + |R| \\cdot K\\) where \\(K\\) is a constant dependent on the structure of the index or the height of the B+ Tree index. E.g. if the height of the B+ Tree is 3, then \\(K = 4\\) assuming each node occupying a page. </p> <p>When the index is not clustered, the cost will become \\(B(R) + |R| \\cdot K \\cdot j\\) where \\(j\\) is the maximum number of tuples of \\(S\\) sharing one key. </p> <p>An extreme case where \\(R\\) is sorted by \\(a\\) and \\(S.b\\) is a clustered index. The total cost is \\(B(R) + B(S)\\). </p>"},{"location":"notes/l6_query_operation/#sort-merge-join","title":"Sort Merge Join","text":"<p>The fourth alternative is sort merge join. </p> <ol> <li>Sort \\(R\\) by the attribute used in \\(c\\)</li> <li>Sort \\(S\\) by the attribute used in \\(c\\)</li> <li>Merge the sorted \\(R\\) and sorted \\(S\\) like external sort</li> </ol> <p>The cost of step 1 is</p> \\[2\\cdot B(R) \\cdot (1 + \\lceil log_{m-1}(\\lceil B(R) / m \\rceil)\\rceil)\\] <p>The cost of step 2 is </p> \\[2\\cdot B(S) \\cdot (1 + \\lceil log_{m-1}(\\lceil B(S) / m \\rceil)\\rceil)\\] <p>The cost of step 3 is </p> \\[B(R) +  B(S)\\]"},{"location":"notes/l6_query_operation/#hash-join","title":"Hash Join","text":"<p>The fourth alternative is to make use of an in-memory hash table.</p> <p>Assuming we hash the join attribute used in \\(c\\) of \\(S\\) and stored it in a hash table, and the hash table is fitting in the RAM, (one big assumption), we can compute join by</p> <ol> <li>for each tuple \\(u\\) in \\(S\\)<ol> <li>add \\(u\\) (projected attributes) to HT</li> </ol> </li> <li>for each tuple \\(t\\) in \\(R\\)<ol> <li>lookup \\(t\\)'s attribute in HT<ol> <li>output \\(t\\) and the value in HT</li> </ol> </li> </ol> </li> </ol> <p>The cost of this approach is \\(B(R) +B(S)\\) because we scan both tables exactly once.</p>"},{"location":"notes/l6_query_operation/#grace-hash-join","title":"Grace Hash Join","text":"<p>The Hash Join is perhaps impractical, as in most of the cases, the hash table can't fit in the RAM. </p> <p>The assumption - there exists a hash function \\(h\\) such that </p> <ol> <li>\\(R\\) can be partitioned into \\(n\\) partitions by \\(h\\) and \\(n \\leq m - 1\\), and none of the partitions have more than \\(m-1\\) pages, or </li> <li>\\(S\\) can be partitioned into \\(n\\) partitions by \\(h\\) and \\(n \\leq m - 1\\), and none of the partitions have more than \\(m-1\\) pages.</li> </ol> <p>The algorithm works as follows</p> <ol> <li>partition \\(R\\) and \\(S\\) by \\(h\\)</li> <li>If \\(S\\) is the relation that satisfies that assumption<ol> <li>for each partition \\(p_i\\) in \\(S\\), <ol> <li>load pages in \\(p_i\\) into the buffer pool, and build a in-memory hashtable using a different hash function \\(h_2\\)</li> <li>load data (page by page) from partition \\(q_i\\) in \\(R\\) (values in \\(q_i\\) and \\(p_i\\) should share the same hash values of \\(h\\))</li> <li>look up the data in the hash table with \\(h_2\\), output if a match is found.</li> </ol> </li> </ol> </li> </ol> <p>The total cost of the Grace Hash Join is </p> \\[ 3 \\cdot (B(R) + B(S)) \\]"},{"location":"notes/l8_query_optimization/","title":"50.043 - Query Optimization","text":""},{"location":"notes/l8_query_optimization/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this unit, you should be able to</p> <ol> <li>name the stages in a query life cycle</li> <li>identify the process involved in different stages of a query life cycle</li> <li>identify the different execution model</li> <li>explain the process of query optimization</li> <li>estimate the cost of complex query operations</li> </ol>"},{"location":"notes/l8_query_optimization/#query-life-cycle","title":"Query Life Cycle","text":"<p>From the previous lessons, we learn how to implement the primitive relational algebra operation in a physical database system, namely, select, join and sort.</p> <p>However in real word applications, we often encounter queries far more complex.  Recall the article/book/publisher example</p> <ul> <li>article(id, name)</li> <li>book(id, name, date)</li> <li>publisher(id, name) </li> <li>publish(article_id, book_id, publisher_id)</li> </ul> <p>We would like to find out article names that are published in year <code>2010</code></p> \\[ \\Pi_{article.name} (\\sigma_{book.date = 2010}(article \\bowtie_{article.id=publish.article\\_id} (publish \\bowtie_{publish.book\\_id = book.id} book))) ~~~~ {\\tt (E1)}  \\] <p>Note that the above relation algebra expression is equivalent to the following thanks to the associativity of the \\(\\bowtie\\) operator.</p> \\[ \\Pi_{article.name} (\\sigma_{book.date = 2010}((article \\bowtie_{article.id=publish.article\\_id} publish) \\bowtie_{publish.book\\_id = book.id} book)) ~~~~ {\\tt (E2)} \\] <p>Besides these two alternatives, we also find the following expression producing the same result by \"pushing\" the selection operation down to the <code>book</code> relation, since it is only constrainted by the <code>book</code>'s attribute.</p> \\[ \\Pi_{article.name} (article \\bowtie_{article.id=publish.article\\_id} (publish \\bowtie_{publish.book\\_id = book.id} \\sigma_{book.date = 2020}(book))) ~~~~ {\\tt (E3)} \\] <p>We call them different logical query plans of the same query. We might argue that (E3) might perform better as we push down the selection operator and reduce the size of the joined relation. </p> <p>Sometimes for ease of reasoning we might consider representing the relational algbra expression in a tree structure, for instance, (E3) can be represented as</p> <pre><code>graph\na(\"&amp;Pi;&lt;sub&gt;article.name&lt;/sub&gt;\") --- b(\"&amp;bowtie;&lt;sub&gt;article.id=publish.article_id&lt;/sub&gt;\")\nb(\"&amp;bowtie;&lt;sub&gt;article.id=publish.article_id&lt;/sub&gt;\") --- c(\"article\")\nb(\"&amp;bowtie;&lt;sub&gt;article.id=publish.article_id&lt;/sub&gt;\") --- d(\"&amp;bowtie;&lt;sub&gt;publish.book_id = book.id&lt;/sub&gt;\")\nd(\"&amp;bowtie;&lt;sub&gt;publish.book_id = book.id&lt;/sub&gt;\") --- e(\"publish\") \nd(\"&amp;bowtie;&lt;sub&gt;publish.book_id = book.id&lt;/sub&gt;\") --- f(\"&amp;sigma;&lt;sub&gt;book.date = 2020&lt;/sub&gt;\") \nf(\"&amp;sigma;&lt;sub&gt;book.date = 2020&lt;/sub&gt;\")  --- g(\"book\")</code></pre> <p>In reality, we also need to consider the selectivity of selection condition, the size of the relations,  the different join algorithms, availability of indices, as well as the sortedness of the relation. Each intermediate operation can be computed \"on-the-fly\" or \"materalized.\" Each possible combination performs differently. We refer to these combinations as physical query plans. For instance,</p> <pre><code>graph\na(\"&amp;Pi;&lt;sub&gt;article.name&lt;/sub&gt;&lt;br/&gt;[iterator]\") --- b(\"&amp;bowtie;&lt;sub&gt;article.id=publish.article_id&lt;/sub&gt;\") \nb(\"&amp;bowtie;&lt;sub&gt;article.id=publish.article_id&lt;/sub&gt;\") --- c(\"article &lt;br/&gt; [heap scan]\")\nb(\"&amp;bowtie;&lt;sub&gt;article.id=publish.article_id&lt;/sub&gt;&lt;br/&gt;[block nested loop]\") --- d(\"&amp;bowtie;&lt;sub&gt;publish.book_id = book.id&lt;/sub&gt;\")\nd(\"&amp;bowtie;&lt;sub&gt;publish.book_id = book.id&lt;/sub&gt;\") --- e(\"publish &lt;br/&gt; [heap scan]\") \nd(\"&amp;bowtie;&lt;sub&gt;publish.book_id = book.id&lt;/sub&gt; &lt;br/&gt; [block nested loop]\") --- f(\"&amp;sigma;&lt;sub&gt;book.date = 2020&lt;/sub&gt;\") \nf(\"&amp;sigma;&lt;sub&gt;book.date = 2020&lt;/sub&gt;\")  --- g(\"book&lt;br/&gt;[heap scan]\")</code></pre> <p>The physical plan above scan the heap file to filter books with <code>date = 2020</code>, then a block nested join is performed on the <code>publish</code> relation and the immediate results coming from the filter. Finally, we apply another block nested loop to join the <code>article</code> relation with the immediate results coming from the nested join. The top level project is performed \"on-the-fly\". </p> <p>Alternatively, we might have a slightly different phyiscal query plan,</p> <pre><code>graph\na(\"&amp;Pi;&lt;sub&gt;article.name&lt;/sub&gt;&lt;br/&gt;[iterator]\") --- b(\"&amp;bowtie;&lt;sub&gt;article.id=publish.article_id&lt;/sub&gt;\") \nb(\"&amp;bowtie;&lt;sub&gt;article.id=publish.article_id&lt;/sub&gt;\") --- c(\"article &lt;br/&gt; [heap scan]\")\nb(\"&amp;bowtie;&lt;sub&gt;article.id=publish.article_id&lt;/sub&gt;&lt;br/&gt;[index nested loop]\") --- d(\"&amp;bowtie;&lt;sub&gt;publish.book_id = book.id&lt;/sub&gt;\")\nd(\"&amp;bowtie;&lt;sub&gt;publish.book_id = book.id&lt;/sub&gt;\") --- e(\"publish &lt;br/&gt; [heap scan]\") \nd(\"&amp;bowtie;&lt;sub&gt;publish.book_id = book.id&lt;/sub&gt; &lt;br/&gt; [block nested loop]\") --- f(\"&amp;sigma;&lt;sub&gt;book.date = 2020&lt;/sub&gt;\") \nf(\"&amp;sigma;&lt;sub&gt;book.date = 2020&lt;/sub&gt;\")  --- g(\"book&lt;br/&gt;[heap scan]\")</code></pre> <p>In the above physical query plan, we assume following indices are preesent</p> <ul> <li>article.id (it's a primary key anyway)</li> </ul> <p>The physical plan applies the index nested loop to the outer join by using <code>article</code> as the inner relation. Though we preform a heap scan on the <code>article</code> relation, the operation is also a clustered index scan, since <code>article</code> is having primary key index on <code>id</code>.</p> <p>Note that there could be many physical query plans generated from a logical plan. Given a set of physical query plans, we need to find the \"best\" one (with the lowest I/O cost). It turns out that it is very challenging problem, and we will discuss about this shortly. This step is called query optimization.</p> <p>Suppose we identify a good physical query plan, we translate it into actual code to be executed in the database host machine. This final step is called query execution.</p> <pre><code>stateDiagram-v2\ns1 : query\ns2 : logical plans\ns1 --&gt; s2 : query rewriter\ns3 : physical plans\ns2 --&gt; s3 : plan generation / cost estimation\ns4 : result \ns3 --&gt; s4 : execution \n</code></pre>"},{"location":"notes/l8_query_optimization/#execution-model","title":"Execution Model","text":"<p>The input of an execution model is the logical query plan. Every operation in the query plan can be executed differently.</p> <p>There are several options</p> <ol> <li>Iterator Model (seen in our project!)</li> <li>Materialization Model</li> <li>Vector Model</li> </ol>"},{"location":"notes/l8_query_optimization/#iterator-model","title":"Iterator Model","text":"<p>The main idea of the iterator model is \"on the fly\" or \"lazy\", i.e. tuples are not retrieved/computed until they are needed. The idea is actualized by implementing the physical access operation, filter operation, join operation and etc has iterators. One possible implementation in Java is to use the <code>Iterator</code> interface, in which </p> <pre><code>classDiagram\nclass Iterator~T~ {\n    bool hasNext()\n    T next()\n}\n&lt;&lt;interface&gt;&gt; Iterator</code></pre> <p>The <code>hasNext()</code> method checks whether the iterator is currently not empty. The <code>next()</code> method advances the internal pointer and produces the next item.</p> <p>For instance \\(\\sigma_{book.date = 2020}(book)\\), the table access of <code>book</code> is an instance of <code>Iterator&lt;Tuple&gt;</code>, The filter operator \\(\\sigma\\) takes the <code>book</code> iterator and returns an iterator instance <code>Iterator&lt;Tuple&gt;</code>, too. When the top most iterator is executed, it calls its children's iterators to produce the next value. </p> <p>One strong advantage of the iterator model is that it is memory efficient. For instance, in the running example, we don't need to load all the tuples from the book table into the buffer pool before the filtering takes place. There might be worst case scenarios in which the intermediate results are too huge for the RAM and needed to be written back to the disk and be reloaded when required.</p> <p>The downside of the iterator model is that sometimes we need to re-compute some common sub-queries, e.g. \\((\\sigma_{book.date = 2020}(book)) \\bowtie (\\sigma_{book.date = 2020}(book))\\), we need to compute \\(\\sigma_{book.date = 2020}(book)\\) twice. </p> <p>We would be in the worst situation if an immediate result is needed as a whole, e.g. need to build a hash table, need to sort the immediate data first. In these situation, we have to block and consume all the children's data first.</p>"},{"location":"notes/l8_query_optimization/#materialization-model","title":"Materialization Model","text":"<p>In materialization model, immediate results are fully computed and emit before the outer operation starts its computation. We can think of each operator having its own local storage holding the immediate results.</p> <p>The advantage of this model is that we can re-use immediate results if they needed more than once and we ca use the immediate results to build immediate index, hash table, etc. It performs better when the immediate results are eventually part of the final results.</p>"},{"location":"notes/l8_query_optimization/#vectorized-model","title":"Vectorized Model","text":"<p>The vectorized model, (AKA the batch model) is a hybrid of iterator and materialization model. Every operator in the vectorized model implements the <code>next()</code> method. The <code>next()</code> method returns a batch of tuples instead of a single one. </p> <p>The advantage of this model is to strike a balance between iterator and materialization. It is memory efficient, as we don't compute and store the entire immediate result, yet we can compute Hash table (e.g. grace hash join) when needed.</p>"},{"location":"notes/l8_query_optimization/#query-planning","title":"Query Planning","text":"<p>The process of query planning involves enumerating a large enough subset of all possible equivalent query given the input expression. </p> <p>Two relaional algebra expressions are consider equivalent if they produce the same set of results.</p>"},{"location":"notes/l8_query_optimization/#equivalence-rules","title":"Equivalence rules","text":"<p>Let \\(R\\), \\(R'\\) and \\(R''\\) be relations, the subset of requivalence rules is as follows,</p> <ol> <li>\\(\\sigma_{c_1}(\\sigma_{c_2}(R)) \\equiv \\sigma_{c_1 \\wedge c_2}(R)\\) </li> <li>\\(\\sigma_{c_1}(\\sigma_{c_2}(R)) \\equiv \\sigma_{c_2}(\\sigma_{c_1}(R))\\)</li> <li>\\(\\Pi_{a_1}(\\Pi_{a_2} ... (\\Pi_{a_n}(R))) = \\Pi_{a_1}(R)\\) if \\(a_1 \\subseteq a_2 \\subseteq ... \\subseteq a_n\\)</li> <li>\\(\\sigma_{c}(R \\times R') \\equiv R \\bowtie_{c} R'\\).</li> <li>\\(R \\bowtie_{c} R' \\equiv  R' \\bowtie_{c} R\\).</li> <li>\\(R \\bowtie_{c_1} (R' \\bowtie_{c_2} R'') \\equiv (R \\bowtie_{c_1} R') \\bowtie_{c_2} R''\\)</li> <li>\\(\\sigma_{c}(R) \\equiv R\\) if \\(c \\not \\subseteq attr(R)\\)</li> <li>\\(\\sigma_{c_1}(R\\bowtie_{c_2} R') \\equiv \\sigma_{c_1}(R) \\bowtie_{c_2} \\sigma_{c_1}(R')\\)</li> <li>\\(R \\cap R' \\equiv R' \\cap R\\)</li> <li>\\(R \\cup R' \\equiv R \\cup R\\)</li> <li>\\(R \\cap (R' \\cap R'') \\equiv (R \\cap R') \\cap R''\\)</li> <li>\\(R \\cup (R' \\cup R'') \\equiv (R \\cup R') \\cup R''\\)</li> <li>\\(\\Pi_{a_1 \\cup a_2} (R \\bowtie_{c} R') \\equiv \\Pi_{a_1}(R) \\bowtie_{c} \\Pi_{a_2}(R')\\)  if \\(attr(c) \\subseteq a_1 \\cap a_2\\).</li> <li>\\(\\Pi_{a_1 \\cup a_2} (R \\bowtie_{c} R') \\equiv \\Pi_{a_1 \\cup a_2}(\\Pi_{a_1\\cup a_3}(R) \\bowtie_{c} \\Pi_{a_2\\cup a_4}(R') )\\) if \\(attr(c) \\subseteq a_3 \\cap a_4\\)</li> <li>\\(\\Pi_{a}(R \\cup R') \\equiv \\Pi_{a}(R) \\cup \\Pi_{a}(R')\\)</li> <li>\\(\\Pi_{a}(\\sigma_{c}(R)) \\equiv \\sigma_{c}(\\Pi_{a}(R))\\) if \\(attr(c) \\subseteq a\\)</li> <li>\\(\\sigma_{c}(R - R') \\equiv \\sigma_{c}(R) - R'\\)</li> <li>\\(\\sigma_{c}(R \\cap R') \\equiv \\sigma_{c}(R) \\cap \\sigma_{c}(R')\\)</li> <li>\\(\\sigma_{c}(R \\cup R') \\equiv \\sigma_{c}(R) \\cup \\sigma_{c}(R')\\)</li> </ol> <p>With this subset of rules, we can already enumerate a substantially large set of alternative logical query plans from the original plan.</p>"},{"location":"notes/l8_query_optimization/#cost-estimation","title":"Cost Estimation","text":"<p>Recall that given a logical query plan, there are still multiple alternatives of physical query plans depending on the choice of access method, execution model, index and etc.</p> <p>The ultimate goal is to find the physical plan that has the lowest I/O cost. Without running the plans, we could only estimate the cost.</p> <p>From the previous lesson, we learned how to estimate the I/O cost for different access methods, selection, join and etc. The cost is subject to two extra information.</p> <ol> <li>the sizes of the relations</li> <li>the selectivity of the predicate.</li> </ol> <p>In DBMS, the sizes of the relations are recorded periodically in the catalog. The catalog sub-system keeps track of the following the statistics which can be used to approximate the selectivity.</p> <ol> <li>\\(N(R)\\) the number of tuples in \\(R\\)</li> <li>\\(V(a,R)\\) the number of unique values of attribute \\(a\\) in \\(R\\).</li> <li>\\(min(a,R)\\) the minumum value of attribute \\(a\\) in \\(R\\)</li> <li>\\(max(a,R)\\) the maximum value of attribtue \\(a\\) in \\(R\\).</li> </ol>"},{"location":"notes/l8_query_optimization/#cardinality-estimation","title":"Cardinality Estimation","text":"<p>Given the above catalog statistics, we can estimate the selectivity ratio, \\(\\alpha(c, R)\\).</p>"},{"location":"notes/l8_query_optimization/#equality-predicate","title":"Equality Predicate","text":"<p>In case \\(c\\) is \\(a = v\\) where \\(a\\) is an attribute of \\(R\\) and \\(v\\) is a value, \\(\\alpha(a = v, R) = 1 / V(a,R)\\).</p>"},{"location":"notes/l8_query_optimization/#range-equality-predicate","title":"Range Equality Predicate","text":"<p>In case \\(c\\) is \\(a &gt; v\\) where \\(a\\) is an attribute of \\(R\\) and \\(v\\) is a value, \\(\\alpha(a &gt; v, R) = (max(a,R) - v) / (max(a,R) - min(a,R) + 1)\\) assuming values of \\(a\\) in \\(R\\) follow a uniform distribution.</p> <p>In case the distribution is not uniform, we may use a histogram with binning to obtain a better estimation. However building historgram for a large table could be expenive. In case of large data set, DBMS often collect a small sample to build the histogram.</p>"},{"location":"notes/l8_query_optimization/#conjunction-predicate","title":"Conjunction Predicate","text":"<p>In case \\(c\\) is \\(c_1 \\wedge c_2\\), assuming values constrained by \\(c_1\\) are independent of those constrained by \\(c_2\\), \\(\\alpha(c_1 \\wedge c_2, R) = \\alpha(c_1, R) \\cdot \\alpha(c_2, R)\\). </p>"},{"location":"notes/l8_query_optimization/#search-algorithm","title":"Search Algorithm","text":"<p>The last piece of the query optimization is to search for the best physical plan given </p> <ul> <li>a method to enumerate alternative</li> <li>a way to estimate the I/O cost given a plan</li> </ul>"},{"location":"notes/l8_query_optimization/#the-challenge","title":"The challenge","text":"<p>A naive approach would generate all possible plans, apply estimation to all of them and find the best one. However enumerating all possible plans could be expensive. For instance, enumerating all plan of a \\(n\\)-way join will yield \\(4^{n}\\) possibilities. </p> <p>Recall from our algorithm class that the number of possible Binary Search Trees with \\(n\\) different keys is catalan number </p> \\[C_n = (2\\cdot n)! / ((n + 1)! \\cdot n!)\\] <p>As \\(n\\) grows, catalan numbers grow as </p> \\[4^n / (n^{3/2} \\cdot \\sqrt{\\pi}) \\] <p>A set of possible query plans with \\(n\\)-way join shares the same size!</p>"},{"location":"notes/l8_query_optimization/#selinger-algorithm","title":"Selinger Algorithm","text":"<p>The selinger algorithm is one of the classic algorthm for searching for optimal plan. </p> <p>The idea behind is to apply the following heuristics</p> <ol> <li>Only consider the left-skew trees.<ol> <li>This is because in many different physical plans, we need to scan the right sub-tree multiple times in join. It is better to keep the right sub-tree as simple as possible.</li> <li>If we apply this heuristic, we cut down the search base to </li> </ol> </li> </ol> \\[     \\left (\\begin{array}{c}             n \\\\             n - 1             \\end{array}     \\right ) \\cdot     \\left (\\begin{array}{c}             n - 1 \\\\             n - 2             \\end{array}     \\right ) \\cdot ... \\cdot       \\left (\\begin{array}{c}             2\\\\             1             \\end{array}     \\right ) =       n! \\] <ol> <li> <p>Considering the left-skew trees with \\(n\\) relations still computing \\(n!\\) possible plans. To further cut down the search, the algorithm assumes the best overall plan consists of best sub-plans. It proceeds by finding the best plans for leaf nodes, then \"walk-up\" the trees by finding the best plans to combine intermediate steps.  For example, </p> <ul> <li>Pass 1. find the best plans for each relations (leaf nodes)</li> <li>Pass 2. find the best plans to join any 2 relations</li> <li>Pass 3. find the best plans to join any 3 relations</li> <li>...</li> </ul> </li> </ol> <p>For each pass \\(i\\) where \\(i&gt;1\\), the best plans are computed based on the result from pass \\(i-1\\). Overall this cut down the final search space of an \\(n\\)-way join to</p> \\[ \\left (\\begin{array}{c}  n \\\\  n-1  \\end{array} \\right )  \\] <p>And the overall time-complexity of </p> \\[ \\left (\\begin{array}{c}             n \\\\             n - 1             \\end{array}     \\right ) +     \\left (\\begin{array}{c}             n  \\\\             n - 2             \\end{array}     \\right ) + ... +       \\left (\\begin{array}{c}             n\\\\             1             \\end{array}     \\right ) = O(2^n)  \\] <p>For instance, recall our running example, </p> \\[ \\Pi_{article.name} (article \\bowtie_{article.id=publish.article\\_id} publish \\bowtie_{publish.book\\_id = book.id} \\sigma_{book.date = 2020}(book)) \\] <p>For simplicity, we only focus on the join expression</p> \\[ article \\bowtie_{article.id=publish.article\\_id} publish \\bowtie_{publish.book\\_id = book.id} \\sigma_{book.date = 2020}(book) \\] <p>Assuming there is no index created on \\(book.date\\).</p> <ul> <li>Pass 1 <ul> <li>we find the cheapest way to access \\(\\sigma_{book.date = 2020}(book)\\), (because \\(\\sigma\\) is not a join), is an heap scan.</li> <li>the cheapest way to acccess \\(publish\\) is heap scan.</li> <li>the cheapest way to accesss \\(artcle\\) is heap scan.</li> </ul> </li> <li>Pass 2 <ul> <li>immediate relation 1: <ul> <li>we join \\(publish\\) and \\(article\\) by taking the cheapest access methods from Pass 1, hence the cheapest way to join \\(publish\\) and \\(article\\) is an index nested join on \\(article.id\\), let's say the cost is 1000.</li> </ul> </li> <li>immediate relation 2:<ul> <li>we join \\(publish\\) and \\(\\sigma_{book.date = 2020}(book)\\), let's say the cheapest way to join is an block nested join with \\(\\sigma_{book.date = 2020}(book)\\) materialized, the cost is 2750.</li> </ul> </li> <li>immediate  relation 3:<ul> <li>we join \\(article\\) and \\(\\sigma_{book.date = 2020}(book)\\). There is no common attribute, hence it is a cross product. The cost is 25000.</li> </ul> </li> </ul> </li> <li>Pass 3<ul> <li>alternative 1:<ul> <li>we join \\(\\sigma_{book.date = 2020}(book)\\) with immediate relation 1 from Pass 2. Let's say the cheapest cost is 5000 with a block nested loop.</li> </ul> </li> <li>alternative 2:<ul> <li>we join \\(article\\) with immediate relation 2 from Pass 2. Let's say the cheapest cost is 6000 with an index nested loop.</li> </ul> </li> <li>alternative 3:<ul> <li>we join \\(publish\\) with the immediate rlation 3, with an index nested loop, the cost is 50000.</li> </ul> </li> </ul> </li> </ul> <p>Overall, the cheapest plan is alterantive 1. </p> <p>Note that in real world DBMS, the query optimizer would prepare two best plans for each relation for all passes, an unordered plan and a best index plan. With these adjustment, we have double the size of the top-level search space and the overall time complexity.</p>"},{"location":"notes/l8_query_optimization/#optional-reading-translating-sql-into-relational-algebra","title":"Optional Reading - Translating SQL into relational algebra","text":"<ul> <li>Translating SQL into the Relational Algebra</li> </ul>"},{"location":"notes/l9_transaction_concurrency/","title":"50.043 - Transactions and Concurrency Control","text":""},{"location":"notes/l9_transaction_concurrency/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this unit, you should be able to</p> <ol> <li>Explain serializability, confict serializability.</li> <li>Apply conflict serializability check to schedule.</li> <li>Explain Strict 2PL and 2PL</li> <li>Apply Strict 2PL and 2PL to generate conflict serialiable schedules.</li> </ol>"},{"location":"notes/l9_transaction_concurrency/#recall-isolation","title":"Recall Isolation","text":"<p>Recall from the previous unit, we learn that the isoliation property ensures that multiple concurrent transactions must not be dependent on one another. In other words, if the DBMS executing these concurrent transactions by interleaving, the result be the same as executing them in a sequential order. </p>"},{"location":"notes/l9_transaction_concurrency/#serializability","title":"Serializability","text":"<p>Suppose we have a set of tasks \\(t_1,...,t_n\\), each task \\(t_i\\) consists of a sequence of instructions \\(c^i_1,..., c^i_{m_i}\\). </p>"},{"location":"notes/l9_transaction_concurrency/#schedule","title":"Schedule","text":"<p>We call a sequence of distinct instructions \\(c_1,...,c_k\\) a schedule of tasks \\(t_1,...,t_n\\), iff </p> <ol> <li>for all \\(i\\in\\{1,n\\}\\), for all \\(p \\in \\{1, m_i\\}\\), we find \\(c^i_p \\in \\{c_1,...,c_k\\}\\).</li> <li>for all \\(q \\in \\{1,k\\}\\), there exist \\(i\\in\\{1,n\\}\\) and \\(p \\in  \\{1, m_i\\}\\) such that \\(c_q = c^i_p\\).</li> <li>for all  \\(q \\in \\{1,k\\}\\) and \\(q' \\in \\{q,k\\}\\), there not exist \\(i\\in\\{1,n\\}\\) and \\(p \\in  \\{1, m_i\\}\\) and \\(p' \\in \\{p, m_i\\}\\) such that \\(c_q = c^i_{p'}\\) and \\(c_{q'} = c^i_p\\).</li> </ol> <p>The first rule enforces all instructions in the tasks can be found in the schedule.  The second rule enforces all instructions in the schedule can be found in the tasks. The third rule enforce that the relative order between instructions from the same task are maintained in the schedule.</p>"},{"location":"notes/l9_transaction_concurrency/#serial-schedule","title":"Serial Schedule","text":"<p>Given tasks \\(t_1,...,t_n\\) and a schedule  \\(c_1,...,c_k\\), we say \\(c_1,...,c_k\\) is a serial schedule of \\(t_1,...,t_n\\) iff</p> <ol> <li>there not exists a sub-sequence \\(c_{q-1}, c_{q}, c_{q+1}\\) of \\(c_1,...,c_k\\) such that \\(c_{q-1}\\) and \\(c_{q+1}\\) are instructions from task \\(t_j\\) and \\(c_q\\) is an instruction from task \\(t_i\\), for some \\(i\\) and \\(j\\) in \\(\\{1,n\\}\\).</li> </ol> <p>In other words, a schedule is serial iff there is no interleaving.</p> <p>Note that given a set of tasks \\(t_1,...,t_n\\), there are in total \\(n!\\) serial schedules.</p>"},{"location":"notes/l9_transaction_concurrency/#state-and-instruction","title":"State and Instruction","text":"<p>Given two database states \\(s_1\\) and \\(s_2\\), and an instruction \\(c\\),  we say \\(s_1\\vdash c \\rightarrow s_2\\) to denote execution \\(c\\) in the database with in state \\(s_1\\) results in state \\(s_2\\). </p> <p>Inductively, we can define </p> <p>\\(s_1 \\vdash c_1,c_2,...,c_n \\rightarrow s_n\\) iff \\(s_1 \\vdash c_1 \\rightarrow s_2\\) and  \\(s_2 \\vdash c_2,...,c_n \\rightarrow s_n\\).</p>"},{"location":"notes/l9_transaction_concurrency/#serializable-schedule","title":"Serializable Schedule","text":"<p>Given tasks \\(t_1,...,t_n\\) and a schedule  \\(c_1,...,c_k\\), we say  \\(c_1,...,c_k\\) is serializable iff there exists a serial schedule of  \\(t_1,...,t_n\\), i.e. \\(c_1',...,c_k'\\) such that \\(s_1 \\vdash c_1,...,c_k \\rightarrow s_2\\) and \\(s_1 \\vdash c_1',...,c_k' \\rightarrow s_2'\\) and \\(s_2 = s_2'\\).</p> <p>In other words, a schedule is serializable if its effect is the same as a serial schedule after execution.</p> <p>For example, consider the following transaction</p> <pre><code>-- transaction 1\nbegin;\nselect @bal1 := bal from saving where id=1001;    -- c1\nupdate saving set bal = @bal1-100 where id=1001;  -- c2\ncommit;\n</code></pre> <pre><code>-- transaction 2\nbegin;\nselect @bal2 := bal from saving where id=1001;    -- c1'\nupdate saving set bal = @bal2*1.05 where id=1001; -- c2'\ncommit;\n</code></pre> <p>The schedules <code>c1,c2,c1',c2'</code> and <code>c1',c2',c1,c2</code> are serializable schedules, but <code>c1,c1',c2,c2'</code> is not a serializable schedule, we illustrate the state and instruction execution as follows</p> state before instruction state after 100 c1 100, bal1 = 100 100,bal1 = 100 c1' 100, bal1 = 100, bal2 = 100 100, bal1 = 100, bal2 = 100 c2 0, bal1 = 100, bal2 = 100 0, bal1 = 100, bal2 = 100 c2' 105, bal1 = 100, bal2 = 100 <p>At the end of execution, we found the balance of account <code>1001</code> is 105 though we withdraw 100 from this account in transaction1.</p>"},{"location":"notes/l9_transaction_concurrency/#serializability-check","title":"Serializability Check","text":"<p>Our dream approach is to be able to check whether the given schedule is serializable without executing it. However such a check is very expensive because</p> <ol> <li>there are \\(n!\\) possible serial schedules we need to verify the given one against.</li> <li>we need to model the set of possible operations and the value domains (without executing them).</li> </ol> <p>Our second best approach is to use a conservative approximation check, say \\(C(s)\\) where \\(C\\) is the approximation check and \\(s\\) is a schedule, such that \\(C(s)\\) yields true implies \\(s\\) is serializable. </p> <p>Note that we don't need to guarantee the inverse direction still holds (since \\(C\\) is a conservative approximation).</p>"},{"location":"notes/l9_transaction_concurrency/#conflict-serializability-check","title":"Conflict Serializability Check","text":"<p>One possible of consevative approximation is Conflict Serializability Check.</p>"},{"location":"notes/l9_transaction_concurrency/#update-of-terminologies","title":"Update of terminologies","text":"<p>From this point onwards, we sometimes refer</p> <ul> <li>an instruction \\(c\\) as an operator, which could be \\(R(A)\\) or \\(W(A)\\) where \\(R(\\cdot)\\) stands for the read operator, and \\(A\\) is the object that being read, \\(W(\\cdot)\\) denotes a write operator.</li> <li>a task \\(t\\) as an transaction \\(T\\).</li> <li>a state \\(s\\) is a mapping from objects, such as \\(A\\), \\(B\\), etc to values.</li> </ul>"},{"location":"notes/l9_transaction_concurrency/#operator-conflict","title":"Operator Conflict","text":"<p>Two instructions \\(c_1\\) and \\(c_2\\) are conflict iff</p> <ul> <li>\\(c_1\\) and \\(c_2\\) belong to different transactions and </li> <li>\\((c_1 = R(A) \\wedge c_2 = W(A))\\) or \\((c_1 = W(A) \\wedge c_2 = R(A))\\) or \\((c_1 = W(A) \\wedge c_2 = W(A))\\) for some common object \\(A\\) in the state.</li> </ul>"},{"location":"notes/l9_transaction_concurrency/#execution-order","title":"Execution Order","text":"<p>Given a schedule \\(X = c_1,...,c_n\\), we say \\(c_i \\prec c_j\\) iff  \\(1 \\leq i \\wedge  i &lt; j \\wedge j \\leq n\\)</p> <p>Intuitively, \\(c_i \\prec c_j\\) means \\(c_i\\) is executed before \\(c_j\\) in a schedule \\(X\\).</p>"},{"location":"notes/l9_transaction_concurrency/#conflict-equivalence","title":"Conflict Equivalence","text":"<p>Given two schedules \\(X = c_1,...,c_n\\) and \\(X' = c_1',...,c_n'\\) are conflict equivalent iff</p> <ol> <li>both schedules belong to the same set of transactions. </li> <li>for any pair of conflicting operators \\((c_i, c_j)\\) in \\(X\\), such that \\(c_i \\prec c_j\\), we find \\((c_k', c_l')\\) in \\(X'\\) such that \\(c_k' \\prec c_l'\\) and \\(c_i = c_k'\\) and \\(c_j = c_l'\\).</li> </ol> <p>In other words, the second condition in the above definition says that all conflicting operators in \\(X\\) have the same relative execution order (within the conflicting pair) in \\(X'\\).</p>"},{"location":"notes/l9_transaction_concurrency/#conflict-serializable","title":"Conflict Serializable","text":"<p>Given a set of tasks \\(t_1,...,t_n\\), a schedule \\(X\\) is conflict serializable iff it is conflict equivalent to some schedule \\(X'\\) of tasks \\(t_1,...,t_n\\) such that \\(X'\\) is serial.</p>"},{"location":"notes/l9_transaction_concurrency/#an-example","title":"An example","text":"<p>Consider the following example with two transactions </p> <ul> <li><code>T1: R(A), W(A), R(B), W(B)</code></li> <li><code>T2: R(A), W(A), R(B), W(B)</code></li> </ul> <p>Let's say </p> <ul> <li><code>T1:R(A), W(A)</code> is to increment <code>A</code>'s value by 10.</li> <li><code>T1:R(B), W(B)</code> is to increment <code>B</code>'s value by 10.</li> <li><code>T2:R(A), W(A)</code> is to double <code>A</code>'s value.</li> <li><code>T2:R(B), W(B)</code> is to double <code>B</code>'s value.</li> </ul> <p>Note that there are 2! = 2 possible serial schedules.</p> <ul> <li>Serial schedule 1</li> </ul> T1 T2 R(A) W(A) R(B) W(B) R(A) W(A) R(B) W(B) <p>assuming the initial state is <code>{A:10, B:10}</code>, the final state of the above is <code>{A:40, B:40}</code>.</p> <ul> <li>Serial schedule 2 </li> </ul> T1 T2 R(A) W(A) R(B) W(B) R(A) W(A) R(B) W(B) <p>assuming the initial state is <code>{A:10, B:10}</code>, the final state of the above is <code>{A:30, B:30}</code>.</p> <p>Now let's consider the following schedule (let's call it schedule 3)</p> T1 T2 R(A) W(A) R(A) W(A) R(B) W(B) R(B) W(B) <p>Note that the above schedule is not conflict serializable, because </p> <ol> <li>conflicting operator pair \\(T2:W(B) \\prec T1:R(B)\\) cannot be reordered to obtain serial schedule 1, and </li> <li>conflicting pair \\(T1:W(A) \\prec T2:R(A)\\) cannot be reordered to obtain serial schedule 2.</li> </ol> <p>If we were to execute the above non-conflict serializable schedule with the init state <code>{A:10, B:10}</code>, we get <code>{A:40, B:30}</code>.</p> <p>Recall that conflict serialiability is a conservative approximation of serialzability. Suppose we change the operation of <code>T2</code> </p> <ul> <li><code>T2:R(A), W(A)</code> is to increase <code>A</code>'s value by 20.</li> <li><code>T2:R(B), W(B)</code> is to increase <code>B</code>'s value by 20.</li> </ul> <p>Given the init state <code>{A:10, B:10}</code>,  serial schedule 1, serial schedule 2 and schedule 3 generate <code>{A:30, B:30}</code>, which means that after the change of the operation of <code>T2</code>, schedule 3 becomes serializable, but still not conflict serializable.</p>"},{"location":"notes/l9_transaction_concurrency/#conflict-serializable-check-algorithm","title":"Conflict Serializable Check Algorithm","text":"<p>In the above example, we illustrate the Conflict Serializable check via a table and manual checking. </p> <p>To implement the idea as an algorithm we could leverage on a directed graph, called precedence graph.</p> <p>The precedence graph is generated from a schedule \\(X\\) as follow.</p> <ol> <li>For each transaction \\(t\\), create a node \\(n_t\\).</li> <li>For each pair of transactions \\(t\\) and \\(t'\\), if there exists a conflicting pair of operators \\(t:c \\prec t':c'\\) in \\(X\\), we draw an directed arc from \\(n_t\\) to \\(n_{t'}\\).</li> </ol> <p>A schedule \\(X\\) is conflict serialzable if its precendence graph is acyclic.</p> <p>For instance, the precendence graph of the schedule 3 in the previous example is as follows</p> <pre><code>graph\nT1 --&gt; T2\nT1 --&gt; T2\nT2 --&gt; T1\nT2 --&gt; T1</code></pre>"},{"location":"notes/l9_transaction_concurrency/#two-phase-locking","title":"Two Phase Locking","text":"<p>Generate schedule in a bruteforce way then check for conflict serializablity is still impractical. DBMSes use Two Phase Locking (2PL) to generate conflict serializable schedule.</p>"},{"location":"notes/l9_transaction_concurrency/#strict-2pl","title":"Strict 2PL","text":"<p>Let's consider a strict version of two phase locking is called Strict 2PL. It is governed by the following two rules.</p> <ol> <li>When a transaction need to read an object \\(A\\), it needs to acquire a shared lock \\(S(A)\\); when it need to write an object \\(A\\), it needs to aquire an exclusive lock \\(X(A)\\). A shared lock can be shared by multiple reader transactions. An exclusive lock cannot be shared.</li> <li>All locks are only released \\(U(A)\\), when the transaction commit.</li> </ol> <p>The requests for lock \\(S(A)\\), \\(X(A)\\) and release of lock \\(U(A)\\) are inserted by the DBMSes as schedule being generated.</p> <p>Let's consider the earlier example with  with two transactions </p> <ul> <li><code>T1: R(A), W(A), R(B), W(B)</code></li> <li><code>T2: R(A), W(A), R(B), W(B)</code></li> </ul> <p>Suppose <code>T1</code> starts slightly earlier than <code>T2</code>. A schedule is generated with strict 2PL as follows</p> Timestep T1 T2 1 X(A) 2 R(A) 3 W(A) 4 X(B) 5 R(B) 6 W(B) 7 U(A), U(B), Commit 8 X(A) 9 R(A) 10 W(A) 11 X(B) 12 R(B) 13 W(B) 14 U(A), U(B), Commit <ol> <li>At timestep 1, <code>T1</code> acquires the exclusive lock on <code>A</code> since it needs to read and write to it. <code>T2</code> is blocked as it would also need to acquire an exclusive lock on <code>A</code> to start. </li> <li>At timestep 4, <code>T1</code> acquires the exclusive lock on <code>B</code>.</li> <li>At timestep 7, <code>T1</code> is commited, hence both locks are released.</li> <li>At timestep 8, <code>T2</code>  acquires lock on <code>A</code> and starts.  </li> </ol>"},{"location":"notes/l9_transaction_concurrency/#2pl","title":"2PL","text":"<p>Once issue with Strict 2PL is that a transaction holds on to the locks it acquires until it commits, this blocks off other possible concurrent execution opportunity, hence throughput might be affected.</p> <p>A variant of Strict 2PL, known as 2PL, is governed by the following two rules. (First rule is the same as Strict 2PL, the second rule is different).</p> <ol> <li>When a transaction needs to read an object \\(A\\), it needs to acquire a shared lock \\(S(A)\\); when it needs to write an object \\(A\\), it needs to aquire an exclusive lock \\(X(A)\\). A shared lock can be shared by multiple reader transactions. An exclusive lock cannot be shared.</li> <li>A transaction releases the lock right after usage. A transaction cannot acquire new lock once it releases some lock.</li> </ol> <p>The reason why this is called a two phase locking is that there is a phase of lock acquisition, the number of locks being acquired by a transaction grows until it starts to release some lock, then it will release all the locks gradually.</p> <p>Given the same example, a schedule is generated with 2PL as follows</p> Timestep T1 T2 1 X(A) 2 R(A) 3 W(A) 4 X(B) 5 U(A) 6 X(A) 7 R(A) 8 W(A) 9 R(B) 10 W(B) 11 U(B), Commit 12 X(B) 13 U(A) 14 R(B) 15 W(B) 16 U(B), Commit <ol> <li>At timestep 1, <code>T1</code> acquires an exclusive lock on <code>A</code> and starts, <code>T2</code> is blocked and waits.</li> <li>At timestep 4, <code>T1</code> is done with <code>A</code>, but needs to acquire an exclusive lock on <code>B</code> before releasing <code>A</code>.</li> <li>At timestep 6, <code>T2</code> acuiqres an exclusive lock on <code>A</code> and starts and runs until timestep 8, it needs to acquire an exclusive lock on <code>B</code> but it is blocked.</li> <li>At timesteps 9-10, <code>T1</code> operates on <code>B</code>.</li> <li>At timestep 11, <code>T1</code> releases lock on <code>B</code> and is commited.</li> <li>At timestep 12, <code>T2</code> is unblocked and acquires the exclusive lock on <code>B</code>, it then releases lock on <code>A</code> and continues with the rest of operations.</li> </ol> <p>Note that schedules generated by both Strict 2PL and 2PL are conflict serializable. (Proof omitted.)</p>"},{"location":"notes/l9_transaction_concurrency/#strict-2pl-vs-2pl","title":"Strict 2PL vs 2PL","text":"<p>2PL in general yields a better throughput compared to Strict 2PL by allowing some transaction to start early. Such gain comes with a price. 2PL might produce schedules that are harder to recover due to cascading abort.</p> <p>Consider the following schedule generated by 2PL, we keep the begin and commit explicit.</p> Timestep T1 T2 1 begin 2 X(A) 3 A=10 4 X(B) 5 U(A) 6 begin 7 X(A) 8 A=A+1 9 U(A) 10 commit <p>Assume the initial state is <code>A=0</code>, executing the above schedule yields the following Undo/Redo Log</p> <pre><code>1.  &lt;T1 begin&gt;\n2.  &lt;T1 update A 0 10&gt;\n3.  &lt;T2 begin&gt;\n4.  &lt;T2 update A 10 11&gt;\n5.  &lt;T2 commit&gt;\n</code></pre> <p>After timestep 11, the database crashes.</p> <p>During the recovery phase, we start with undo phase, we find that T1 need to be undone and aborted. However, T2's effect of <code>A=A+1</code> is based on the assumption that <code>A=10</code> in T1 was successful. As a result, T2 has be to aborted and undone too. This is known as cascading abort.</p>"},{"location":"notes/l9_transaction_concurrency/#conservative-2pl","title":"Conservative 2PL","text":"<p>Another variant of Strict 2PL is called Conservative 2PL. It is same as Strick 2PL except that a transaction acquires all the locks it needs at the very begining. </p> <p>For instance, given the previous example, a schedule is generated with conservative 2PL as follows</p> Timestep T1 T2 1 X(A),X(B) 2 R(A) 3 W(A) 4 R(B) 5 W(B) 6 U(A), U(B), Commit 7 X(A),X(B) 8 R(A) 9 W(A) 10 X(B) 11 R(B) 12 W(B) 13 U(A), U(B), Commit <p>Note that all the locks are acquired by a transaction in an atomic operation, i.e. all or none. This is a way to prevent deadlock. </p>"},{"location":"notes/l9_transaction_concurrency/#deadlock","title":"Deadlock","text":"<p>Let's consider how deadlock can be caused by 2PL (and Strict 2PL).</p> <p>Consider the following transactions</p> <ul> <li><code>T1: R(A), W(A), R(B), W(B)</code></li> <li><code>T2: R(B), W(B), R(A), W(A)</code></li> </ul> <p>and a schedule</p> Timestep T1 T2 1 X(A) 2 R(A) 3 W(A) 4 X(B) 5 R(B) 6 W(B) 7 try X(B) but blocked try X(A) but blocked <ol> <li>At timestep 1, <code>T1</code> acquires an exclusive lock on <code>A</code> and starts</li> <li>At timestep 4, <code>T2</code> acquires an exclusive lock on <code>B</code> and starts</li> <li>AT timestep 7, <code>T1</code> tries to get a lock on <code>B</code> but it gets blocked. <code>T2</code> tries to get a lock on <code>A</code> but it is blocked too. A deadlock arises.</li> </ol>"},{"location":"notes/l9_transaction_concurrency/#deadlock-detection","title":"Deadlock detection","text":"<p>A way to detect deadlock is that when transactions waiting for locks, the lock manager (part of DBMS) generate a wait-for graph. </p> <p>In a wait for graph, </p> <ul> <li>each active transaction \\(t\\) is a node \\(n_t\\).</li> <li>an arc going from \\(n_t\\) to \\(n_t'\\) if transaction \\(t\\) is waiting for a lock which is being held by \\(t'\\).</li> </ul> <p>For insance, the wait-for graph from the previous example is </p> <pre><code>graph\nT1 --&gt; T2\nT2 --&gt; T1</code></pre>"},{"location":"notes/l9_transaction_concurrency/#deadlock-prevention","title":"Deadlock prevention","text":"<p>To prevent deadlock, there are several approaches</p> <ol> <li>Use conservative 2PL, but throughput will be affected.</li> <li>Set priorities. Transactions with higher priorty get the lock. Transaction wth lower prioerty get aborted and retry.</li> </ol>"},{"location":"notes/l9_transaction_crash_recover/","title":"50.043 Transactions and Crash Recovery","text":""},{"location":"notes/l9_transaction_crash_recover/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this unit, you should be able to</p> <ol> <li>describe the ACID properties of database system.</li> <li>describe how atomicity is guaranteed by using transaction and log.</li> <li>explain write ahead log with different policies.</li> <li>explain recovery process with undo, redo, undo/redo logging.</li> <li>explain quiescent and nonquiescent checkpoints.</li> </ol>"},{"location":"notes/l9_transaction_crash_recover/#acid-properties","title":"ACID properties","text":"<p>To ensure data correctness, modern DBMSes often ensure certain properties.</p>"},{"location":"notes/l9_transaction_crash_recover/#atomicity","title":"Atomicity","text":"<p>The atomicity property allows users of DBMSes to define a sequence of operations to be performed as none or all, there is no in-between.  For example, consider the following sequence of database operations represent a balance transfer of $100 from account A to account B.</p> <ol> <li>debit $100 from account A</li> <li>credit $100 into account B</li> </ol> <p>These two operations must be performed altogether but not partially. </p>"},{"location":"notes/l9_transaction_crash_recover/#consistency","title":"Consistency","text":"<p>The consistency property ensures the data integrity constraints are not violated. This property has two levels.</p>"},{"location":"notes/l9_transaction_crash_recover/#data-consistency","title":"Data consistency","text":"<p>The data consistency refers to the overall constraint (aka business logic) needs to be maintained by the database. DBMS ensures data consistency if the user's operations (in terms of transactions) are consistent.</p>"},{"location":"notes/l9_transaction_crash_recover/#operation-consistency","title":"Operation consistency","text":"<p>This refers to database operations issued by the users to the database as a single transaction must not violate the business constraint, e.g. the total sum of balance of all accounts in a bank must remain unchanged, and the user operations given in earlier example is  of consistent.</p> <p>Esnuring operation consistency has to be user's responsibility.</p>"},{"location":"notes/l9_transaction_crash_recover/#isolation","title":"Isolation","text":"<p>The isoliation property ensures that multiple concurrent transactions must not be dependent on one another. In other words, if the DBMS executing these concurrent transactions by interleaving, the result be the same as executing them in a sequential order.</p> <p>Note that isolation does not entail determinism.</p>"},{"location":"notes/l9_transaction_crash_recover/#durability","title":"Durability","text":"<p>The durability property ensures the effect of transaction operations will stay permanently upon commit.</p>"},{"location":"notes/l9_transaction_crash_recover/#transaction-and-acid","title":"Transaction and ACID","text":"<p>Transaction is one of the popular techniques in ensuring ACID properties. </p> <p>In SQL, one can enclose the operations between <code>BEGIN</code> and <code>COMMIT</code></p> <pre><code>begin;\nselect @bal := bal from saving where id=1001;\nupdate saving set bal = @bal-100 where id=1001;\nselect @bal := bal from saving where id=2001;\nupdate saving set bal = @bal+100 where id=2001;\ncommit;\n</code></pre> <p>In the above transaction, we transfer $100 from account with id <code>1001</code> to another account with id <code>2001</code>. The debit from account <code>1001</code> and the credit to account <code>2001</code> will take effect altogether or none.</p> <p>In the following section, we first study how to apply transaction with logs to ensure atomicity. Then we consider how to apply transaction to ensure isolation.</p> <p>Durability follows from atomicty and crash recovery process if page flush operation is atomic (as an assumption we take).</p> <p>Consistency is ensured if transactions operations are consistent.</p>"},{"location":"notes/l9_transaction_crash_recover/#transaction-and-crash-recovery","title":"Transaction and Crash recovery","text":"<p>Let's consider the running example of of debitting account <code>1001</code> and crediting account <code>2001</code>.</p> <p>There are a few possible program points where a crash could occur</p> <ol> <li>before the first update statement </li> <li>after the first update statement and before the second update statement</li> <li>after the second update statement before the commit</li> <li>after the commit</li> </ol>"},{"location":"notes/l9_transaction_crash_recover/#force-and-no-stealing","title":"Force and no stealing","text":"<p>One way to simplify the crash recovery is to assume that </p> <ul> <li>A dirty page in the buffer pool is written to disk when the transaction is committed. This is known as the Force policy</li> <li>A page in the buffer pool belonging to an uncommitted transaction will not be evicted. This is known is the No-stealing policy</li> </ul> <p>With force and no stealing policy in-place, there is little work to do for recovery, as a committed transaction must have been written to disk. Uncommit transactions have not be written to disk. Note we assume that the commit and page flushing are atomically performed altogether. </p> <p>However such a policy is not very practical. Forcing increases unncessary page I/Os, e.g. when multiple records from the same page get updated in consecutive transactions. No-stealing policy disallows concurrent transaction activities in the event the transaction involves larger set of pages.</p>"},{"location":"notes/l9_transaction_crash_recover/#stealing-and-undo-logging","title":"Stealing and undo logging","text":"<p>Let's relax our policy to allow page stealing (but forcing is still enforced).  Now the issue is that some of the pages in the buffer pool belonging to an uncommitted transaction could have been written to the disk due to stealing before the commit. When the crash occurs before the commit, we need to undo the page write. To enable recovery, the system maintains a write ahead log with undo information. Each entry in this log has the following fields</p> <ul> <li>transaction_id</li> <li>entry_type</li> <li>file_id</li> <li>page_id</li> <li>offset</li> <li>old_value</li> </ul> <p>where <code>entry_type</code> could be <code>begin</code>, <code>commit</code>, <code>update</code> and <code>abort</code>. For now we assume <code>abort</code> is issued only by the DBMS but not the user for simplicity. In case of <code>begin</code>, <code>abort</code>, and <code>commit</code>, the <code>file id</code>, <code>page id</code>, <code>offset</code> and <code>old_value</code> fields are null.</p> <p>The undo logging is governed by the following two rules. </p> <ol> <li>In a transaction <code>T</code>, before writing the updated page <code>X</code> containing changes <code>V</code> to disk, we must append <code>&lt;T, update, X.file_id, X.page_id, V.offset, V.old_value&gt;</code> to the log. (<code>V.offset</code> denotes where <code>V</code> is located in the page <code>X</code>.)</li> <li>When <code>T</code> is being committed, to make sure all log entries are written to disk, all updated pages are written to disk. Then append <code>&lt;T, commit&gt;</code> to the log on the disk.</li> </ol> <p>For instance, from our running example, let's say the transaction id is <code>t1</code>, the balance of account <code>1001</code> was <code>2000</code> in page <code>p1</code> with offset <code>0</code>, and the account <code>2001</code> balance was <code>500</code> in page <code>p2</code> with offset <code>40</code>. Both pages belong to file <code>f0</code>. When the transaction committing, we should have the following in the log</p> <pre><code>&lt;t1 begin&gt;\n&lt;t1 update f0 p1 0 2000&gt;\n&lt;t1 update f0 p2 40 500&gt;\n&lt;t1 commit&gt;\n</code></pre> <p>Now let's imagine the crash occurs after the second update statement, but we are unsure whether all the dirty pages have been written to disk. In that scenario, the recovery manager scans the log backwards (from the tail of the log to the start of the log) to look for transactions that have a <code>begin</code> but without <code>commit</code> (and without <code>abort</code>) and it finds the following entries at the tail.</p> <pre><code>&lt;t1 begin&gt;\n&lt;t1 update f0 p1 0 2000&gt;\n&lt;t1 update f0 p2 40 500&gt;\n</code></pre> <p>With the above information the recovery manager can (conservatively) restore pages <code>p1</code> and <code>p2</code> back to the origin state before the transaction <code>t1</code> started. Finally it marks the transaction has been undone correctly with <code>&lt;t1, abort&gt;</code> log entry, so that the future recovery routine should not bother about this \"chunk of history\".</p>"},{"location":"notes/l9_transaction_crash_recover/#no-force-and-redo-logging","title":"No force and redo logging","text":"<p>If we relax the policy by lifting force (but no stealing is enforced), we need another kind of logging. </p> <p>Now the issue we face is that all the dirty pages in the buffer pool might have not been flushed off the disk even after the transaction has been committed. When the crash occurs after the transaction commit, we need to find a way to check and \"redo\" these page writes during the crash recovery phase.</p> <p>To enable recovery, the system maintains a write ahead log with redo information. Each entry in this log has the following fields</p> <ul> <li>transaction_id</li> <li>entry_type</li> <li>file_id</li> <li>page_id</li> <li>offset</li> <li>new_value</li> </ul> <p>the fields are almost the same as the one we saw in undo logging except for <code>new_value</code>, in which it captures the new value that the update operation is trying to write.</p> <p>The redo logging is governed by the following rules</p> <ol> <li>In a transaction <code>T</code>, before writing the updated page <code>X</code> containing changes <code>V</code> to disk, we must have appended <code>&lt;T, update, X.file_id, X.page_id, V.offset, V.new_value&gt;</code> to the log and appended <code>&lt;T, commit&gt;</code> to the log. (i.e. update and commit to log before flushing).</li> <li>When <code>T</code> is being committed, to make sure all log entries are written to disk. Then append <code>&lt;T, commit&gt;</code> to the log on the disk.</li> </ol> <p>Next, let's reuse the same running example and imagine the crash occurs after the commit statement, but we are unsure whether all the dirty pages have been flushed to disk. In that scenario, the recovery manager scans the log from the very begining of the log to the end, for every transaction with <code>begin</code> and <code>commit</code>, apply the redo operation.</p> <p>For instance, <pre><code>...\n&lt;t1 begin&gt;\n&lt;t1 update f0 p1 0 2100&gt;\n&lt;t1 update f0 p2 40 600&gt;\n&lt;t1 commit&gt;\n</code></pre></p> <p>given the above entry, the recovery algorithm re-applies the updates though they could have been written to disk. It needs start from the begining, i.e. it has to look at all log entries of committed transaction including those older than <code>t1</code>. </p>"},{"location":"notes/l9_transaction_crash_recover/#a-quick-summary-so-far-undo-vs-redo","title":"A Quick Summary so far - Undo vs Redo","text":"<p>Let's contrast Undo logging with Redo logging.</p> <p>Undo logging assumes force and stealing. Dirty pages must be flushed before committing the transaction, hence the commit is slower (more work to do). On the other hand, the recovery process is easier, as we only need to look for uncommitted transaction logs by scanning backward from the tail of the log till the start of the log. Very often we only need to small fraction of the log entries (by heuristics we often find very small numbers of uncommited transaction near the tail).</p> <p>Redo logging assumes no-force and no-stealing. Dirty pages live in the buffer pool beyond transaction committing point, however, pages belonging to an active transaction must not be evicted (written to disk). Hence the commit is faster (no need to make sure dirty pages are flushed). On the other hand, the recovery process overhaul all the transaction entries in the log.</p>"},{"location":"notes/l9_transaction_crash_recover/#undoredo-logging","title":"Undo/Redo Logging","text":"<p>Let's further relax the policy to allow no-force and stealing. To support this liberal policy, we combine both Undo and Redo into a single logging mechannism. </p> <p>Each entry of the Undo/Redo logging has the following fields</p> <ul> <li>transaction_id</li> <li>entry_type</li> <li>file_id</li> <li>page_id</li> <li>offset</li> <li>old_value</li> <li>new_value</li> </ul> <p>The Undo/Redo logging is governed by the following rules.</p> <ol> <li>In a transaction <code>T</code>, before writing the updated page <code>X</code> containing changes <code>V</code> to disk, we must have appended <code>&lt;T, update, X.file_id, X.page_id, V.offset, V.old_value, V.new_value&gt;</code> to the log.</li> <li>When <code>T</code> is being committed, to make sure all log entries are written to disk. Then append <code>&lt;T, commit&gt;</code> to the log on the disk.</li> </ol> <p>The recovery is carried out in two phases.</p>"},{"location":"notes/l9_transaction_crash_recover/#phase-1-backward-pass","title":"Phase 1 - backward pass","text":"<p>Since stealing is allowed, in the first pass, like undo logging, we scan the log backwards from the tail, to search for open transactions (i.e. transactions without commit or abort), and perform the undo steps.</p>"},{"location":"notes/l9_transaction_crash_recover/#phase-2-forward-pass","title":"Phase 2 - forward pass","text":"<p>Since no force is in placed, during the second pass, like redo logging, we scan the log forward from the begining. For each committed transaction, we redo the update.</p>"},{"location":"notes/l9_transaction_crash_recover/#checkpoint","title":"Checkpoint","text":"<p>To reduce the recovery overhead (especially the redo step), we could insert checkpoints into the log.</p>"},{"location":"notes/l9_transaction_crash_recover/#quiescent-checkpoint","title":"Quiescent checkpoint","text":"<p>One naive approach is to </p> <ol> <li>pause the system by preventing new transactions to be initiated</li> <li>wait for the active transactions to be committed and flushed.</li> <li>insert a <code>checkpoint</code> entry to the log, (<code>checkpoint</code> is a special log entry).</li> <li>unpause the system.</li> </ol> <p>During the undo recovery phase, we scan the log backward upto the checkpoint. During the redo recovery phase, we scan the log forward starting from the checkpoint.</p> <p>For instance, consider the tail fragment of some log (simplified). For the ease of referencing, we added line number to each entry.</p> <pre><code>1. ...\n2.  &lt;t0 commit&gt;\n3.  &lt;t1 begin&gt;\n4.  &lt;t1 update f1 p1 0 10 20&gt;\n5.  &lt;t2 begin&gt;\n6.  &lt;t2 update f2 p3 0 200 300&gt;\n7.  &lt;start_checkpoint t1,t2&gt; # sys pauses, no more new transaction\n8.  &lt;t1 commit&gt;\n9.  &lt;t2 commit&gt;\n10. &lt;end_checkpoint t1,t2&gt;  # sys resumes, new transaction can be added\n11. &lt;t3 begin&gt;\n12. &lt;t3 update f3 p2 0 30 40&gt;\n</code></pre> <p>For undo, we scan the log backwards until line 10, we undo the update at line 12. For redo, we start from line 10 and scan forward, but we find nothing to re-do because there is nothing to re-do.</p>"},{"location":"notes/l9_transaction_crash_recover/#nonquiescent-checkpoint","title":"Nonquiescent checkpoint","text":"<p>A major disadvantage of quiescent checkpoing is during the pause, no new transactions is initiated, it hurts the overall performance. </p> <p>A nonquiescent checkpoint overcomes this drawback by capturing the uncommitted active transactions when the checkpoint is started.</p>"},{"location":"notes/l9_transaction_crash_recover/#nonquiescent-checkpoint-with-undo-logging-with-stealing-and-force","title":"Nonquiescent checkpoint with undo-logging (with stealing and force)","text":"<p>The main idea is to flush all uncommitted tranactions' dirty pages during the checkpoint and commit them.</p> <ol> <li>find all the active (and uncommitted) transactions ids, <code>T1</code>, ..., <code>Tn</code>.</li> <li>insert a <code>&lt;start_checkpoint T1,...,Tn&gt;</code> entry to the log.</li> <li>when all the dirty pages belonging to <code>T1,...,Tn</code> are flushed and committed, insert <code>&lt;end_checkpoint T1,...,Tn&gt;</code> entry to the log.</li> </ol> <p>During the undo recovery phase, we start from the last completed checkpoint's  <code>start_checkpoint</code> entry and scan for uncommitted transactions in the log and undo the page write if there is any. This is because during the checkpoint, there might be new transaction initiated. Note that that undo operations are applied backwards starting from the tail.</p> <p>For instance, consider the tail fragment of some log (simplified).</p> <pre><code>1. ...\n2.  &lt;t0 commit&gt;\n3.  &lt;t1 begin&gt;\n4.  &lt;t1 update f1 p1 0 10&gt;\n5.  &lt;t2 begin&gt;\n6.  &lt;t2 update f2 p3 0 200&gt;\n7.  &lt;start_checkpoint t1,t2&gt;\n8.  &lt;t3 begin&gt;\n9.  &lt;t1 commit&gt;\n10. &lt;t2 commit&gt;\n11. &lt;t3 update f3 p2 0 30&gt;\n12. &lt;end_checkpoint t1,t2&gt;\n</code></pre> <p>During the recovery, we start from line 7 <code>&lt;start_checkpoint t1,t2&gt;</code> and look for uncommitted transaction, which is <code>t3</code> in this case, and undo the update at line 11.</p>"},{"location":"notes/l9_transaction_crash_recover/#nonquiescent-checkpoint-with-redo-logging-with-no-force-and-no-stealing","title":"Nonquiescent checkpoint with redo-logging (with no force and no stealing)","text":"<p>The main idea is to flush all committed transactions (the dirty pages) during the check point.</p> <ol> <li>find all the active (and uncommitted) transactions ids, <code>T1</code>, ..., <code>Tn</code>.</li> <li>insert a <code>&lt;start_checkpoint T1,...,Tn&gt;</code> entry to the log.</li> <li>flush any dirty pages belonging to some committed transactions (committed before the start of the check point.) </li> <li>insert a <code>&lt;end_checkpoint T1,...,Tn&gt;</code>.</li> </ol> <p>During the redo recovery phase, we start from the last completed checkpoint's <code>start_checkpoint</code> entry and search for transactions being committed  after this point, and redo these transactions. Note that some of these transactions (to be redone) could have been started before the starting of the check point (but still active during the check point).</p> <p>For instance, consider the tail fragment of some log (simplified).</p> <pre><code>1.  ...\n2.  &lt;t0 commit&gt;\n3.  &lt;t1 begin&gt;\n4.  &lt;t1 update f1 p1 0 20&gt;\n5.  &lt;t2 begin&gt;\n6.  &lt;t2 update f2 p3 0 300&gt;\n7.  &lt;start_checkpoint t1,t2&gt; \n8.  &lt;t3 begin&gt;\n9.  &lt;t1 commit&gt;\n10. &lt;t3 update f3 p2 0 40&gt;\n11. &lt;end_checkpoint t1,t2&gt; # dirty pages belong to t0 have been flushed\n12. &lt;t2 commit&gt;\n</code></pre> <p>We start from line 7 <code>&lt;start_checkpoint t1,t2&gt;</code> and search for committed transactions, i.e. <code>t1</code> and <code>t2</code>, we need to re-do the updates at lines 4 and 6.</p>"},{"location":"notes/l9_transaction_crash_recover/#nonquiescent-checkpoint-with-undoredo-logging-with-no-force-and-stealing","title":"Nonquiescent checkpoint with undo/redo-logging (with no force and stealing)","text":"<p>The most complicated checkpoint so far. The main idea is to flush all dirty pages being updated before the check point start.</p> <ol> <li>find all the active (and uncommitted) transactions ids, <code>T1</code>, ..., <code>Tn</code>.</li> <li>insert a <code>&lt;start_checkpoint T1,...,Tn&gt;</code> entry to the log.</li> <li>flush any dirty pages belonging to some update entries made before the start of the check point.</li> <li>insert a <code>&lt;end_checkpoint T1,...,Tn&gt;</code>.</li> </ol> <p>During the undo recovery phase, we start from the last completed checkpoint's <code>start_checkpoint</code> entry and search for uncommitted transactions and undo the update, some of these transaction could have been started before the <code>start_checkpoint</code>. During the redo recovery phase, we start from the last completed checkpoint's <code>start_checkpoint</code> entry and search for transactions being committed  after this point, and redo these transactions (the same as the redo-logging case).</p> <p>For instance, consider the tail fragment of some log (simplified).</p> <p><pre><code>1.  ...\n2.  &lt;t0 commit&gt;\n3.  &lt;t1 begin&gt;\n4.  &lt;t1 update f1 p1 0 10 20&gt;\n5.  &lt;t2 begin&gt;\n6.  &lt;t2 update f2 p3 0 200 300&gt;\n7.  &lt;start_checkpoint t1,t2&gt; \n8.  &lt;t3 begin&gt;\n9.  &lt;t1 update f1 p10 0 5 7&gt;\n10. &lt;t1 commit&gt;\n11. &lt;t3 update f3 p2 0 30 40&gt;\n12. &lt;end_checkpoint t1,t2&gt; # p1, p3 and other dirty pages belong to t0 have been flushed\n13. &lt;t2 commit&gt;\n</code></pre> For undo-phase, we start from line 7 <code>&lt;start_checkpoint t1,t2&gt;</code> and search for uncommitted transactions, i.e. <code>t3</code>, we undo the update at line 11.</p> <p>For redo-phase, we start from line 7 <code>&lt;start_checkpoint t1,t2&gt;</code> and search for committed transactions, i.e. <code>t1</code> and <code>t2</code>, we need to re-do the updates at lines 4, 6 and 9.</p>"}]}